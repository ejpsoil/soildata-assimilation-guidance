[
  {
    "objectID": "etl.html",
    "href": "etl.html",
    "title": "Data Standardisation (vector data)",
    "section": "",
    "text": "Data Standardisation is an aspect of step 4) data organization in the soil information workflow.\nData standardisation is the process of aligning one or more datasets to a common (standardised) data model. For the soil domain, ISO 28258:2013 Soil quality — Digital exchange of soil-related data is a standardised conceptual model which has been developed by the soil community to facilitate the capture of soil plot and profile data. The core of the model is the Observations and Measurements model of the Open Geospatial Community. The ISO28258 model has proven to be generic to capture a lot of soil data use cases, and facilitates interoperability with partners.\n\n\n\nObservations and Measurements\n\n\nThese days various encodings of the ISO28258 conceptual model are available - A xml/xsd oriented model - A relational model (PostGreSQL) - A semantic web ontology, called GLOSIS Web Ontology\nDuring implantation of ISO28258 limitations of the model are likely to be discovered related to the local Use Cases. These limitations can usually be bypassed by extending the model. In recent years some extensions to ISO28258 have been published, which could be relevant to the your cases. Examples are GLOSIS domain model and the INSPIRE Soil data model. Annex D of TG Soil has a specific example on extending the model for a soil contamination use case. Options for model extensions vary per technology. Wetransform has a dedicated section on model extension on their website, based on an R&D project from 2016.\nAn important activity related to Standardisation is the adoption of common code lists and extending those lists to capture regional conventions. The role of code lists is explained in a dedicated codelists section.\nIf you missed the initial EJP Training on Soil data good practices, you can still have a look at a presentation about vector data standardisation.\nThis document lists various implementation options for data Standardisation.\n\n\nThe good practice on GeoPackage describes a relational database format to share standardised data. GeoPackage is a standardised format for storing relational data by the Open Geospatial Consorium, building on the SQLite database specification. Becuase many soil data is stored in the form of relational databases, the transformation to GeoPackage is relatively easy. The transformation process could for example be triggered by a series of database queries within a GIS Desktop client such as QGIS, in Python scripts or an ETL tool such as Hale Studio or FME.\nPro’s and Con’s:\n\nThe GeoPackage format is easy to consume by average users.\nTo capture the hierarchical structure of the INSPIRE datamodels, a lot of tables are needed, resulting in a complex data-model.\nThe good practice is recent, so not a lot of community experience is available yet\nUsers download a full dataset, no (filering) api’s are defined for GeoPackages yet\n\n\n\n\nCookbook\nSoftware\nDescription\n\n\n\n\nINSPIRE in a relational database\nGeopackage\nStandardise soil data using GeoPackage\n\n\n\n\n\n\nTools like Hale Studio and FME are typically used to configure a conversion from data in a relational data model to data in a GML based INSPIRE model. The output is a big GML file which can be published using a WFS server or Atom download service. Below table links to detailed pages on various relevant technologies.\n\n\n\nCookbook\nSoftware\nDescription\n\n\n\n\nHale Studio\nHale Studio\nHumboldt Alignment Editor Studio is a Desktop tool to author ‘data alignments’.\n\n\n-\nFME\nFeature Manipulation Engine is a visually oriented data integration platform\n\n\n\nYou may not have considered before, but consuming a rich GML is not straight forward in common GIS clients like ArcGIS or QGIS. To consume a rich GML you need software which is able to traverse xml hierarchies and links. Tools like Hale Studio can also be used to read rich GML and convert it back to a relational database. Unfortunately you can not automatically reverse a ETL-configuration. But you can set up a new ETL configuration to read and transform the rich GML. A recipe is available which imports INSPIRE Soil GML from the city of Berlin and converts it to a relational database.\n\n\n\nAlternative ETL procedures are based on Semantic Web technology. The INSPIRE community is exploring various semantic web options, but no good practice has been adopted yet.\nSee also the presenation about semantic web (GLOSIS) from the 2022 EJP Soil training.\n\n\n\nProduct\nSoftware\nDescription\n\n\n\n\nSemantic Standardisation using TARQL\nTARQL\ncommand-line tool for converting CSV files to RDF using SPARQL 1.1\n\n\nSemantic mapping using YARRRML\nRML mapper\nHuman readable RDF mappings for RML.io\n\n\nPublish data through semantic web\nYed Coby BlazeGraph\nCookbook by INRAE on publishing soild data as RDF\n\n\n\ntarql"
  },
  {
    "objectID": "data-sharing.html",
    "href": "data-sharing.html",
    "title": "Soil data guidance",
    "section": "",
    "text": "This article describes the data and information sharing step in the Soil Information Workflow and references various cookbook recipes related to this step in the workflow.\nIn this step the data curator manages processes that maintain discovery, view and download services on the web. The main activity is probably the creation of new services, as part of that activity it should be assessed if the services comply with the relevant regulation. Another important activity is the monitoring on Quality of service.\n\n\n\nMetadata and Discovery\nView services\nDownload services\nQuality of service"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction",
    "section": "",
    "text": "This WIKI is a collaborative effort to collect and describe hands-on good practices on data assimilation and dissemination in the Soil domain, with a focus on Europe. The INSPIRE directive has been and is an important effort for standardisation in the environmental data domain, therefore this WIKI has a lot of links to INSPIRE sources. Because INSPIRE adopts industry standards, this WIKI does reference common standards from ISO, Open Geospatial Consortium, Global Soils Partnership, IANA and W3C, giving it a global relevance.\nThe term data assimilation has been chosen by the autors as an alternative to the terms harmonisation and standardisation, which already have a specific meaning in the soil domain:\n\nstandardisation; aligning soil data to a common model, using common codelists.\nharmonisation; transforming results from observations and measurements to values as if all results for a property are measured using the same procedure, by applying so called Pedotransfer Functions (PTF).\n\nThe process of assimilation also aims to capture additional aspects, such as finding, downloading and using the data. Most of these aspects are also well described as part of the FAIR principles.\n\nFindable, provide relevant metadata via online portals\nAccessible, make the data available as download or API\nInteroperable, adopt common data models and code lists\nReusable, add documentation and provide tools to reuse the data\n\n\n\nToner et al, 2022 identified 6 steps in a typical soil information workflow from a data producer perspective and a separate category for the data user perspective. These steps form a relevant categorisation of the articles in this wiki. Much of the articles apply to the categories:\n\n4) Data organisation\n6) data and info sharing\n7) Soil Information User Consideration\n\nWe’ve labeled each of the articles as to which step in the workflow they apply.\n\n\n\nSoil information workflow\n\n\nThis wiki lists a series of options for publishing data according to the Technical Guidelines and/or the Good Practices dedicated to use cases from the Soil domain. For each option recipes on various technologies are provided. The practices are categorized at 3 levels:\n\nMinimal, based on a minimal effort\nTraditional, following initial technical guidelines\nExperimental, following recent and upcoming good practices\n\nThe practices cover 7 topics.\n\nIdentification and namespaces\nData harmonization\nCode lists\nMetadata and Discovery\nView services\nDownload services\nQuality of service\n\n!!! info\nDisclaimer: References to products and approaches are examples. We do not aim to provide a complete listing, nor endorse a specific technology or service provider. Please consult any alternative software provider to what extent INSPIRE is supported in their products. In that scenario consider to contribute your experiences to this WIKI. \n\n\n\nWhile reading the vast amount of recipes in this cookbook you may realize; if there are so many options, how are the differences between the implementations bridged. - Some implementation options differ in technology, but generate a similar result - Some options actually need some bridging before data can be combined, but the number of bridges is limited (and could also be bridged by intermediaries) - All options share some basic principles, such as identifier persistence and adoption of common codelists, which make any option beneficial\nWhen selecting one of the available options, consider the following aspects:\n\nThe minimal implementation will have limitations for end users (for example having to download the full dataset, if they are only interested in a small section). On the other hand, minimal implementations tend to be less complex in setup which makes understanding the implementation easier.\nThe traditional implementations have the most active users, so dedicated documentation and tooling is available with high Technical Readiness Level (TRL). However, some technologies are based on conventions of almost 20 years ago. These conventions are outdated, compared to current IT practices.\nAn experimental approach brings the risk of incomplete documentation and tools. Also there is less evidence on usability. But it does give opportunity to use current technologies and engage with the community to design the next iteration of data sharing.\n\nBefore selecting an option evaluate the following aspects in your organization.\n\nWhat are current IT tools and conventions used in the organization to understand which of the approaches fits best with the current knowledge and experience\nCombine an implementation of INSPIRE with business cases that generate direct benefit for your organization or partners. For example, adoption of the open data directive, better documentation and reporting of service levels, improved archival of data, discoverability on search engines.\nAssess the projected audience. Verify that the complexity and nature of the implementation matches with the expectations and capabilities of that audience.\n\nWe recommend to start with a minimal implementation and validate it with the provided compliance test tooling. From there, extend the implementation while continuing the tests with each iteration. With such an you are able to focus on the important aspects and prevent caveats early on in the process."
  },
  {
    "objectID": "metadata.html",
    "href": "metadata.html",
    "title": "Overview Metadata & discovery",
    "section": "",
    "text": "Discovery of available data is important for potential users to be aware what data is available, evaluate if the data is relevant for them and how they can fetch it, or who to contact for more details. Essentially, the initial goal of any Spatial Data Infrastructure (SDI) is to describe its content. Metadata of datasets and services is described in documents, which are made accessible via a discovery service as records in a catalogue.\nCapturing metadata prior and during data collection is an aspect of step 4) data organization in the soil information workflow, publishing the metadata as part of the data dissemination is an aspect of step 6) data and info sharing. Evaluating the findability of datasets and assessing broken links on existing metadata is an aspect of step 7) Soil Information User Consideration.\nIn the soil domain we generally have 2 types of datasets, actual soil observations (calcium content in a horizon of a soil profile at a certain date or a soil profile field classification) and derived grids or polygon maps which represent parameter or soil type distribution for an area. For the second type of datasets describing the lineage (history) of the data is very important. Typically, you would describe the dataset with point observations in 1 document and link another document, describing the derived dataset, and link it as a parent-child relation. The document describing the derived dataset will contain ‘processing-steps’ describing the model that was used to calculate or derive the parameter or soil type distribution (D6.1 ch 5). This aspect is important for the usage of the derived dataset, to be able to evaluate if the estimate is valid for the envisioned use.\nIf you missed the initial EJP Training on Soil data good practices, you can still have a look at the presentation about metadata and discovery.\nThis page lists various implementation options both for creating metadata, as well as setting up a discovery service.\n\n\nIn a minimal implementation you can describe your dataset as well as your services in a single metadata document. This ‘good practice’ is described in https://github.com/INSPIRE-MIF/gp-data-service-linking-simplification. Basic metadata editors exist, of which the most basic is Notepad++. In the Python domain exists the pygeometa and OWSLib projects, which offer capabilities to generate ISO19139 metadata from other formats.\nThese metadata documents can be placed in a Web Accessible Folder. Products exist which are able to ingest documents from such a folder and expose it as a CSW discovery service. Such an ingest point could be installed at a national level, to facilitate the European INSPIRE GeoPortal (which currently only supports ingests via CSW).\n\n\n\nCookbook\nSoftware\nDescription\n\n\n\n\nA Pythonic metadata workflow\npygeometa\nA minimalistic approach to data discovery\n\n\nData in zenodo\nzenodo\nZenodo is a data repository by CERN/Horizon2020, including rich metadata options\n\n\n\n\n\n\nThe TG metadata (Technical Guidelines metadata) defines 2 types of metadata; documents which describe a dataset which are linked to documents which describe the service via which the datasets are published.\nThe TG discovery describes how the metadata documents need to be published as a CSW discovery service. The table below lists some products which can be used to set up such a service. Mind that the TG extends the OGC CSW specification with some specific INSPIRE elements, for identification and multilingualism.\n\n\n\nCookbook\nSoftware\nDescription\n\n\n\n\nGeoNetwork\nGeoNetwork\nA java based open source catalogue application widely used by member states for INSPIRE discovery. Provides a public portal application. Supports CSW, metadata authoring, validation and harvesting.\n\n\npycsw\npycsw\nA Python based open source CSW server. Supports CSW, OGC API Records. Used in portal software such as CKANSpatial and GeoNode.\n\n\nGeoportal server\nArcGIS Geoportal\nA java based open source CSW implementation for the ArcGIS platform. A CSW client for ArcGIS desktop is included. Note that this package is not the same as ArcGIS Portal.\n\n\nHale Connect\nHale Connect\nA metadata authoring and CSW interface is provided as part of the HALE Connect SAAS offering.\n\n\n\n\n\n\nA good practice exists related to Geo-DCAT-ap. It explains how to publish metadata using the Geo-DCAT-ap vocabulary as an additional metadata format. At present the use of ISO19139 is required by all guidelines. However, it is expected that it will soon be possible to offer metadata in a DCAT only. DCAT facilitates records to be discovered via google dataset search (and other search engines and semantic web platforms).\nCurrently no ‘good practice’ exists to offer discovery services in alternative protocols then CSW. A good practice to adopt OGC API Records is being prepared. OpenSearch, OData and SPARQL could be alternative discovery service protocols.\n\n\n\nCookbook\nSoftware\nDescription\n\n\n\n\ndcat\n-\nA dcat approach to dataset discovery"
  },
  {
    "objectID": "identification.html",
    "href": "identification.html",
    "title": "Identification, namespaces and URI strategy",
    "section": "",
    "text": "An important aspect of publication of data on the web is universal identification of objects within the European INSPIRE infrastructure. Identifiers are constant, unique and authoritative.\nResource identificiation is an aspect of step 4) data organization in the soil information workflow.\nAny identifier is typically combined with a namespace for that identifier, or both aspects are combined into a single Universal Resource Identifier (URI) for the object. Namespaces need to be authoritative but should not be sensible to change. For example, a project name is not a good namespace, because the project is bound to end after a certain period. Examples of good namespaces are: w3id.org, doi.org, data.gouv.fr.\nSome countries have a registry of national namespaces (Netherlands, Germany). Select a namespace from that registry, or consider to add your namespace to such a registry.\n\n\nIn a minimal implementation you can concatenate the database identifier, the featuretype and a namespace to create the INSPIRE identification for the object. For example:\nhttps://data.gouv.fr/inrae/collections/{featuretype}/items/{id}\nTo facilitate users to use the INSPIRE identification to open the object in a web browser, you can set up a routing mechanism to forward the request to the location where the catalogue or feature server is located.\nAn example of such a routing rule in Apache webserver:\nRewriteRule\n  \"https://data.gouv.fr/inrae/collections/(.\\*)/items/(.\\*)$\"\n  \"http://data.recover.inrae.fr:8081/geoserver/vulter/wfs?typeNames=$1&featureID=$2&request=GetFeature\"\n\n\n\nThe Technical Guidelines have a long section on identification within INSPIRE, including dates indicating the validity of a feature. An interesting blog about the use of Namespaces and Identifiers is written by Thorsten Reitz at https://www.wetransform.to/news/2018/02/12/best-practices-for-inspire-ids/.\n\n\n\nThese days the INSPIRE community recommends the use of URI’s to identify things. This aspect is described in https://inspire.ec.europa.eu/implementation-identifiers-using-uris-inspire-%E2%80%93-frequently-asked-questions/59309.\nThe aspect of identification is one of the major benefits of the upcoming OGC API’s over traditional WMS, WFS, WCS. By design any feature, coverage, record or tile in OGC API has a unique URI, including content negotiation to be able to request the object in one of the available encodings (xml, json, html, Geopackage)\n\n\n\nIn many cases catalogue records and service definitions are populated manually in separate locations. Verify that at each location the identification and namespace of links between metadata and services are correct. Initially JRC did not have testing procedures to test these linkages. In practice a lot of these links where not correct, causing users not to be able to download a dataset from a search result in the national and INSPIRE GeoPortal.\nJRC provides the resource linkage checker to evaluate linkage of resources.\n\n\n\nSee also the article about uri strategy"
  },
  {
    "objectID": "download.html",
    "href": "download.html",
    "title": "Overview Download Services",
    "section": "",
    "text": "Overview Download Services\nDownload services facilitate the download of vector, grid or sensor data.\nIf you missed the initial EJP Training on Soil data good practices, you can still have a look at a presentation about download services as Coverage and SensorThings or a presentation on Atom/WMS/WFS.\nSetting up download services is an aspect of step 6) data and info sharing in the soil information workflow.\nThis page lists some implementation options for providing INSPIRE Download Services.\n\nMinimal\nThe INSPIRE Atom Service provides a minimal download service implementation on a series of downloadable files placed in a web accessible folder. For every file a ‘dataset feed’ document can be generated and linked to a service feed describing the ‘service’. A metadata record points to the service feed to complete the implementation.\nAlternatively, products like GeoNetwork and Hale Connect (Annex 1) can provide an Atom interface on top of a set of registered datasets. The TG download services also provides some PHP scripts which create an Atom OpenSearch interface for a series of files.\n\n\n\nCookbook\nSoftware\nDescription\n\n\n\n\nWebdav\nWsgidav\nSetting up atom service using webdav\n\n\nZenodo\nzenodo\nData publication on Zenodo\n\n\n\n\n\nTraditional\nWeb Feature Service (WFS) and Web Coverage Service (WCS) are commonly used to download Featurecollections (vector) and Coverages (grids). Consider that most of the INSPIRE themes (including soil) require publication of hierarchical (app-schema) features, this aspect is not supported by many WFS server implementations. Some tools with this capability are listed in the table below. For WCS a good practice is available to facilitate implementation.\n\n\n\nCookbook\nSoftware\nDescription\n\n\n\n\nGeoServer\nGeoServer\nThe app-schema plugin extends the WFS implementation with support for hierarchical features. On the fly transformation is managed from a configuration file. Marcus Sen (Onegeology) create a cookbook for this approach.\n\n\nGet started with Hale Connect\nHale Connect\nOptimized for performance, stores pregeneralised xml fragments in combination with an elastic search index for filtering\n\n\ndeegree\ndeegree\nA java based spatial application server\n\n\nCoverages with rasdaman\nRasdaman\nA Web Coverage Service implementation\n\n\nSensorThings API via FROST server\nFROST server\nSoil observation data as sensor stream\n\n\n\nConsider that a product advertising WFS support does not automatically qualify for INSPIRE, the product has to support hierarchical GML.\n\n\nExperimental\nA good practice document has been adopted on the use of OGC API Features as download service. With its 20 years of history WFS and GML are out of synch with current IT practices. OGC API is a new direction of standards within OGC adopting some of the latest IT conventions, such as Open API, Rest services, JSON encodings, content negotiation, etc. The use of OGC API will increase in coming years while OGC adopts more standards. Various products exist implementing the final and/or draft specifications.\nAs described in the harmonization paragraph, Sensor Observation Service and SensorThings API offer an alternative download option for themes including much observation data, such as Soil.\nGraphQL and SPARQL are powerfull query api’s to request data over the web. Both have not gone through the proces of Good practice adoption. But they are serious candidates to provide a modern INSPIRE download service.\n\n\n\nCookbook\nSoftware\nDescription\n\n\n\n\nOGC API Features via pygeoapi\npygeoapi\nA python based open source implementation of OGC APIs including OGC API Features. Configuration is managed from a configuration file.\n\n\nProxy a WFS as OGC API Features using LDProxy\nldproxy\nA java based open source implementation of OGC APIs. Originally developed by Interactive Instruments as an easy way to consume API (proxy) on top of existing WFS. These experiments were a main driver for OGC in the direction of OGC API. Configuration is managed from a web interface.\n\n\nSoil data via GraphQL\nPostgraphile\nSoil data from a postgres database via graphQL"
  },
  {
    "objectID": "data-organisation.html",
    "href": "data-organisation.html",
    "title": "Soil data guidance",
    "section": "",
    "text": "This article describes the data organisation step in the Soil Information Workflow and references various cookbook recipes related to this step in the workflow.\nIn this step a data curator prepares the organisation for incoming soil observations from field studies and laboratory work. Data from external organisations is prepared and made available to augment and/or validate local data. Relevant measures are put in place to authorise relevant staff, prevent data loss and data redundancy and assess privacy concerns. Concrete deliverables of this phase are: - data model materialised in a physical database - implemented procedures to prevent data loss - access grants to relevant staff - metadata document in which these aspects are described - procedures to evaluate quality of the data and infrastructure\nNote that in this phase no effort is made to publish data to remote partners or the general public, but it does make sense to capture on the metadata if any access constraints apply which may prevent publication to a wider audience in a later stage.\n\n\nWithin the soil domain (and wider earth sciences) typically 4 types of data are identified:\n\n‘As-is’ data, legacy datasets in their original form or field and analytical data as it is received from field surveyors and laboratories, including reporting about the applied procedures.\nSoil analytical data and field descriptions of a) soil profiles and b) soil plots, standardised to a common model (iso28258). The data includes the lineage, the definition of the soil in space and time (xyzt), the attribute concerned, the procedure used and the unit of expression. Domain values are selected from common code lists.\nA dataset in which the soil parameter values are harmonised as if they were analysed using a single procedure. The metadata of individual observations can be left out, resulting in a abbreviated dataset, typically presented as multiple parameter values (texture fractions, pH, Ca content, Vegetation, Groundwater level) of a feature of interest (horizon, profile, plot defined by xyzt) in a flat table.\nPredicted distribution of soil properties or soil classes, either as grid cells through a statistical model or as vector polygons drawn with expert knowledge in the field.\n\n\n\n\nAnother aspect to consider as part of data standardization is the adoption of common code lists that support interoperability of data. Datasets which use for example different texture classes are hard to compare. Within the soil community there are a number of common code lists available.\n\nWorld Reference Base (WRB), available in digital form via Agrovoc.\nFAO Guidelines of Soil Description (these practices are integrated in the latest version of WRB), digitised as part of the GLOSIS Web Ontology effort.\nThe GLOSIS Web Ontology includes code lists of common soil properties and analysis procedures. This list was originally collected in A compilation of georeferenced and standardised legacy soil profile data for Sub-Saharan Africa (Africa Soil Profiles Database; Leenaars J.G.B et al 2014). And later extended by the soil community.\n\nIn some cases it is relevant to define code lists at a country or even regional level. A regional code list repository supports practitioners in standardizing their data early in the data lifecycle. Extending a codelist is then a relevant recipe.\n\n\n\nMetadata documents are preferably based on a common ontology such as DCAT, DataCite or iso19115. Roughly 2 approaches, with various implementation options, exist to maintain metadata within an organisation:\n\nMetadata is included in the data file or is stored as a sidecar file next to the actual data file. At intervals a crawler imports metadata to make the data findable from a central location.\nMetadata is captured in a separate system and linkage is maintained between the actual dataset and its metadata.\n\nThe second option is relatively easy to adopt, however it comes with a potential big effort to keep the metadata system in sync within the organisation. In practice organisations tend to implement a combination of both options.\nThis step includes the organisation of data arriving from external sources and the standardisation of that data with local data (transform to a common model). As soon as a user imports a remote dataset for use in a project, it is useful to include a metadata record about that dataset in the local repository, so colleagues understand where/when the data was imported. If the dataset is available online, it is better to link to the remote dataset, so users would always use the latest version.\n\n\n\n\na URI strategy\nA Pythonic metadata workflow\nA discovery service in GeoNetwork includes a section on metadata authoring in GeoNetwork\nMetadata and View Service with GeoCat Bridge, GeoNetwork and GeoServer includes a section on metadata management in QGIS\nDCAT\n\nConsider that if a project uses a dedicated data model, transforming external data from a common model to the dedicated project model is relevant. The recipe on data harmonisation to a locally used data model is relevant (See also the general Data standardisation recipe)."
  },
  {
    "objectID": "masterclass/index.html",
    "href": "masterclass/index.html",
    "title": "Soil data guidance",
    "section": "",
    "text": "At 24-26 january 2023 Wageningen Environmental Research and ISRIC - World Soil Information has organised a masterclass on Soil Data Assimilation. This masterclass was a follow up of the Training on INSPIRE Good Practices around Soil Data held in april 2022.\n\n\n\n\n\n\n\n\nBuilding Gaia; Meeting room 2\nVideo\n\n\n\n\n10.00\nIntroduction to data assimilation (harmonization; extension; codelists) - Paul van Genuchten, Jandirk Bulens\nvideo\n\n\n11:00\nINSPIRE Tools - Paul van Genuchten\nvideo\n\n\n12:00\nSkosmos (code lists) - Paul van Genuchten\nvideo\n\n\n14:30\nSoil model in Geopackage - Stefania Morrone\nvideo\n\n\n\n\n\n\n\n\n\n\nLumen 2 meeting room\nVideo\n\n\n\n\n9.00\nIntroduction to View and download services\nvideo\n\n\n10.00\nSensorthings API - Kathi Schleidt, Hylke van der Schaaf\nvideo\n\n\n13:45\nWCS - Kathi Schleidt, Peter Baumann\nvideo\n\n\n14:30\ngeopython (pygeoapi, pygeometa, pycsw) - Tom Kralidis, Luis de Sousa\nvideo\n\n\n\n\n\n\n\n\n\n\nGaia 2 meeting room\nVideo\n\n\n\n\n9.00\nIntroduction to Metadata and discovery\nvido\n\n\n10:00\ndeegree webservices intro and Q&A - Torsten Friebe\nvideo\n\n\n10:30\nMapServer intro and Q&A - Seth G\nvideo\n\n\n11:00\nExtending Codelists - Luis de Sousa\nvideo\n\n\n14.00\nHale Studio - Kate Lyndegaard"
  },
  {
    "objectID": "masterclass/edition-2022.html",
    "href": "masterclass/edition-2022.html",
    "title": "Soil data guidance",
    "section": "",
    "text": "Training on INSPIRE Good Practices around Soil Data\nIn april 2022 WENR and ISRIC organised a first iteration of a training series on Good Practices around Soil Data. The presentations of that training are still available and provide a good introduction to the edition 2023 masterclass.\n\n\n\nVisit to Soil Museum\n\n\n\n\n\nVideo\nSpeaker\n\n\n\n\nThe reasoning behind INSPIRE why do we need a directive?\nJoeri Robbrecht, European Commission\n\n\nWhy do we need to understand INSPIRE and share data?\nJandrik Bulens, WENR\n\n\nExperiences of Implementing SOIL in INSPIRE.\nMaria Fantappiè (CREA), Florian Hoedt (Thünen) and Dries Luts (Department of the Environment and Spatial Development, Flemish Government)\n\n\nINSPIRE Conceptual Framework\nLuis de Sousa, ISRIC\n\n\nData discovery\nPaul van Genuchten, ISRIC\n\n\nInteroperability; O&M, Sensorthings API, Web Coverage Services\nKathi Schleidt, Datacove\n\n\nExtending INSPIRE for the Air Quality directive\nKathi Schleidt, Datacove\n\n\nINSPIRE Soil: an overview and relations with other standards, the conceptual model of the soil theme as a common base\nKathi Schleidt, Datacove\n\n\nHarmonize, map, transform: what does it mean?\nPaul van Genuchten, ISRIC\n\n\nCode lists in INSPIRE\nPaul van Genuchten, ISRIC\n\n\nImplementation, operation, reporting. How do you keep track on progress\nPaul van Genuchten, ISRIC\n\n\nTechnical aspects of view (WMS)-, download (WFS, Atom) services and data harmonization\nPaul van Genuchten, ISRIC\n\n\nAdapting to evolved developments specifically WCS and SensorThings\nKathi Schleidt, Datacove\n\n\nZooming in on INSPIRE and GloSIS mapping. What about tools and software to be used\nLuis de Sousa, ISRIC\n\n\nEmerging data exchange technologies: OGC API; RDF/SPARQL, Gaia-x. Why, what and how?\nPaul van Genuchten, ISRIC\n\n\n\nFor a full report see also the EJP website."
  },
  {
    "objectID": "data-collection.html",
    "href": "data-collection.html",
    "title": "Soil data guidance",
    "section": "",
    "text": "Data Collection\nThis article describes the data collection step in the Soil Information Workflow and references various cookbook recipes related to this step in the workflow.\nData collection is done in the field and laboratory. During this process, data collection tools are used. Proper selection and preparation of data collection tooling impacts how data is passed to the next steps of the data workflow. Verify on the collection tooling:\n\nRelevant aspects to the observation are captured (staff, date, sample-id, depth, location, procedure, …)\nUse of common code lists (definition of texture classes, colours, units of measure)"
  },
  {
    "objectID": "codelists.html",
    "href": "codelists.html",
    "title": "Overview Code Lists",
    "section": "",
    "text": "Adoption of common code lists is an important aspect of data harmonization. For INSPIRE the INSPIRE registry is the source of common code lists. Other common codelists relevant to the soil domain are available in FAO Agrovoc, GEMET, OGC definition server, ISO TC211, and GLosis. At a national level some countries have implemented a national repository for common national codelists which may be relevant (either as a source or as a target, to publish an extended list).\nAdoption of common code lists is an aspect of step 4) data organization in the soil information workflow, although it could also impact step 1) data collection.\nThe adoption of code lists has three aspects:\n\nInventarisation of the code lists used in the source model\nEvaluation of the differences between local and common code lists\n\nSome code lists are a full match\nSome code lists need to be extended, or values mapped\nSome local code lists do not (yet) have a common code list available\n\nIn cases where the common code list cannot (fully) be adopted, the code list needs to be published in a local repository\n\nAdoption of a dedicated codelist is relevant for example for Soil classification. Many of the national soil classification systems have much more detail than the World Reference Base, as suggested to be used by the TG Soil.\n\n\n\n\n\n\nPlease note that the harmonization meant here is harmonization of the description of the data, for example describing a soil observation of pH KCl with dilution 1:10 in the same way across Europe. The harmonization of the data itself, for example transforming pH KCl values to pH H2O values, is a separate step and not described in this wiki. More information on that harmonization can be found in D6.1 chapter 3.5 page 122.\n\n\n\n\n\nThe soil theme has a large number of code lists, ranging from soil type to ranges of grain size. Many code lists originate from the FAO soil classification and are published in the INSPIRE registry.\nIf you missed the 2022 EJP Training on Soil data good practices, you can still have a look at a presentation about codelists.\nImplementation options for managing and publishing a code list:\n\n\nThe most basic form of publishing an alternative or extended code list is to place a code list file on a web location and reference values in it as https://example.org/codelist.xml#concept (see for example http://schemas.opengis.net/iso/19139/20070417/resources/Codelist/gmxCodelists.xml)\n\n\n\nCookbook\nSoftware\nDescription\n\n\n\n\nCode list as iso19135\n\nPublish an XML file on a web location\n\n\n\n\n\nExtended code lists can be published in a local or national instance of the Re3gistry software. This open source project is hosted by JRC to facilitate the INSPIRE registry.\n\n\n\nCookbook\nSoftware\nDescription\n\n\n\n\nCode list in Re3gistry\nRe3gistry\nPublish a codelist in Re3gistry\n\n\n\n\n\n\nA standard for the definition of code lists is Simple Knowledge Organization System (SKOS). Any SPARQL endpoint can be used to publish a code list based on SKOS. Software exists which facilitates the consumption of SKOS data from a SPARQL endpoint in a human friendly way. An example is Skosmos.\nA powerfull aspect of SKOS is that you can link from a concept to existing concepts in other codelists using link relations such as: sameAs, Broader, Narrower.\n\n\n\nCookbook\nSoftware\nDescription\n\n\n\n\nExtend a codelist\n\nHow to extend an INSPIRE codelist\n\n\nPublish a SKOS codelist\nVirtuoso Skosmos\nPublish a code list in semantic web"
  },
  {
    "objectID": "view.html",
    "href": "view.html",
    "title": "Overview INSPIRE View Services",
    "section": "",
    "text": "Overview INSPIRE View Services\nThe TG View services prescribes the adoption of view services, which offer the capability of visualization of spatial data, possibly in a portal, GIS software or webpage. The service provides a quick view on the data, without the need to transfer the data itself to the client. The TG Soil prescribes that for each measured soil parameter a view service layer is made available online. Layers can relate to actual site observations (soil profiles) as well as parameter distribution grids or vector maps.\nSetting up view services is an aspect of step 6) data and info sharing in the soil information workflow.\nThis page lists some implementation options for providing INSPIRE View Services.\n\nMinimal\nIn a minimal implementation the Web Map Tiling Service (WMTS) standard is used to provide view services. Tile services have little risk with respect to the Quality of Service requirements. The alternative option, Web Map Service (WMS), is quite prone to exceed the performance limits in cases when it has to ‘draw’ a lot of data at once.\nTile services are however not optimal for dynamic data and may require a large (tile) storage. Also consider that the adoption of WMTS is less wide spread then WMS in GIS clients.\nThe getFeatureInfo (gfi) operation is not mandatory for INSPIRE (however useful for end users). Without getFeatureInfo, data used as a source for the view service can be minimal (geometry only).\n\n\n\nCookbook\nSoftware\nDescription\n\n\n\n\nMapServer MapServer\nC based FastCGI WMS/WFS/WCS server implementation configured using ‘mapfiles’\n\n\n\nBridge & GeoServer\nBridge GeoServer\nThe recipe describes how to publish view services from QGIS ArcMAP using GeoCat Bridge\n\n\nQGis server\nQGIS\nQgis as a server\n\n\n\n\n\nTraditional\nMost current view services are based on the Web Map Service (WMS) standard. These services are usually easy to set up on top of an existing traditional Web Feature Service or Web Coverage service implementation.\nExamples are in the download services section.\n\n\nExperimental\nOGC API Tiles and OGC API Maps are upcoming standards for map visualization. The Open Geospatial Consortium (OGC) is preparing the final standardization documents, however initial implementations are available in. There is no good practice document for adoption of OGC API Tiles or Maps in preparation yet within INSPIRE.\n\n\n\nCookbook\nSoftware\nDescription\n\n\n\n\n-\nGeoServer\nOGC API Tiles is available via the OGC API community plugin\n\n\n-\npygeoapi\nPython package which exposes a cache of tiles as OGC API Tiles\n\n\n-\nLDProxy\nJava based opensource OGC API implementation\n\n\n\nWithin the sector there is a shift to the use of Vector tiles for vector map visualization. Vector tiles usually require less bandwidth and provide a sharper view on the data, especially on mobile devices with high resolution. INSPIRE does not provide Guidance on the use of vector tiles yet. OGC API Tiles and the MapBox Vector tiles specification are common API’s used to publish vector tiles."
  },
  {
    "objectID": "consumer.html",
    "href": "consumer.html",
    "title": "Soil data guidance",
    "section": "",
    "text": "In each of the articles we aim to describe the producer and consumer aspect of a topic. However in many cases we mainly describe the producer aspect. On this page we provide some specific consumer guidance and reference a number of articles which have explicit sections on the consumer aspect of discovering, accessing and assimilating Soil Data.\nConsuming and assimilating soil data has various challenges.\n\nDiscovery of data; not much data is advertised and on various platforms. In many cases the search engine is the only option to find something. In other cases there’s too much similar results, which prevent you from easily finding the gems between the others.\nSoil data is quite complex by nature; samples representing a soil horizon at a location, being analysed in some laboratory. These observations are used to derive a region sharing a common value, being vizualised on a map. Capturing this complexity in a standardised model has been quite a challenge. It resulted in iso28258 and derived models such as GLOSIS and the INSPIRE model. The process of adopting the standard requires expert skills, consider that working with this type of models is challenging, especcially in generic GIS tools such as QGIS/ArcGIS. It resulted in the fact that many organisations haven’t fully adopted these models and provide the data in a local model or that the harmonised data is a subset of the original dataset. As a consumer it is hard to assess the level of implementation of the standard.\nINSPIRE identified WFS as a relevant exchange mechanism for rich data such as soil data. However the industry has not moved with those efforts and hardly any tooling exists which is able to use WFS to explore the richness of WFS’s providing rich (soil) data.\nAs part of the harmonisation it is important to adopt common code lists, or, for unique concepts, extend existing code lists in a proper way. Many implementations have failed to follow this principle, resulting in a multitude of code list references which are redundant or under documented. In many cases a consumer will need to harmonize code list values as part of using the data.\n\nBelow some guidance on how to face some of these challenges:\n\n\nSome guidance on locating soil data sources.\n\nDatasets of soil institutes of EU memberstates are typically found on the INSPIRE GeoPortal, filter by theme ‘soil’.\nESDAC hosts a series of pan European datasets, including the Lucas database.\nDatasets of academic projects can best be located at OpenAire Explore\nISRIC - World soil information hosts a discovery service including some 200 datasets at https://data.isric.org, and a separate collection of external soil resources\nGlobal Soil Partnership provides a number of global soil maps\nFAO provides a number of legecay soil maps and databases in the Soils portal\nGoogle provides a dedicated dataset search option, operating on structured data of websites\n\n\n\n\nINSPIRE provides guidance on how to indicate in metadata the level of harmonisation of the service. Unfortunately this feature is not broadly adopted yet, many data providers provide non harmonised data without indicating it as such.\nIn case data is provided using the INSPIRE Soil Model you can use Hale Studio to import gml to an alternative datamodel.\nBecause INSPIRE Soil is based on the Observations and Measurements model, it is possible to publish soil data using a SOS or SensorThings API. At this moment there are no known implementations of SOS or STA to provide Soil data. I’m not aware of procedures to download full datasets from a sensor service and transform them, but the use case may not be relevant to sensor data. A multitude of sensor clients is available to interact directly with Sensor Services, providing the options to filter the results and extract only the relevant data for a use case.\n\n\n\nBecause a single INSPIRE Soil dataset consists of multiple featuretypes (plots, profiles, horizons, observations, …) requesting the full dataset is not possible using a single WFS GetFeature request. Instead a client should consult the service capabilities to evaluate which featuretypes are available and iterate over each of them.\nINSPIRE mandates the availability of a stored query on any WFS which is able to download the full dataset in a single request. This is by far the easiest option to download a dataset. However consider that this option is not provided by some providers.\nThe GDAL utility supports GMLAS (GML Application Schema) and is able to import gml from WFS and store it in for example a database. See the GDAL recipe for an example. Hale studio is also able to import data from a WFS.\n!!! note\nIf you open a rich INSPIRE WFS service in QGIS, you will get unexpected results. In some cases QGIS will be able to extract a geometry and display the layer, but in other cases it will open each featuretype as a table (without geometry). Links between layers and tables are not imported. A [GMLAS plugin](https://plugins.qgis.org/plugins/gml_application_schema_toolbox/) (based on the GDAL GMLAS functionality) has been developed in 2016, but adoption has been low and the project seems currently unmaintained.\n\n\n\nCommon codelists are stored on the INSPIRE registry, or common registries such as Agrovoc or Gemet. If a value needs to be registered which is not in those registries, organisations have the option to publish an extended version of the code list locally. Code list values are typically provided as a URI referencing the concept in a registry.\nThe soil model has various codelists. Important codelists are the (WRB) soil clasification, the observed soil property and the methods used in the laboratory. It is important to understand if the method used impacts the measured value. For example there are various methods to measure pH, which each give a specific value. Pedo transfer functions exist to recalculate the value based on the method used. Prior to applying a pedo transfer function, you have to map the codelist from the remote dataset to the code list used locally. Some cases can exist:\n\nBoth datasets reference the same value from a common code list\nThe dataset references a remote concept, but it matches with a concept used locally\nThe dataset references a remote concept, but no local representation exists for the concept\nNo information exists on the concept used\n\nOption 1 is optimal, option 2 requires a mapping table (a suggestion can be made to include the concept on the inspire registry), option 3 and 4 are problematic, these cases should be decided on case by case. Hale studio includes an option to provide mapping tables to map codelist values."
  },
  {
    "objectID": "cookbook/re3gistry.html",
    "href": "cookbook/re3gistry.html",
    "title": "Soil data guidance",
    "section": "",
    "text": "Status: contribution needed\nRe3gistry is a tool to manage and share reference codes. It provides a central access point that allows labels and descriptions for reference codes to be easily looked up by humans or retrieved by machines. It supports organisations in managing and updating reference codes consistently in a well governed manner so that all versions of a code remain traceable and adequately documented over time.\nThe Re3gistry development started under the ISA Are3na action. It continues under the ISA2 ELISE action. The main user of the software is the INSPIRE Registry. Although the software has been implemented in various regional instances.\nWhen you need to manage and share reference codes within and between organisations, the Re3gistry solution can support you. These reference codes uniquely define sets of permissible values for a data field or provide context for the exchanged data. Examples of reference codes are simple enumerations and flat lists to complex controlled vocabularies, taxonomies, thesauri.\n\n\nTools such as GeoNetwork and Hale studio are able to import code lists from Re3gistry software. Which are then used within the software to populate pull down menus.\n\n\n\nThe background of Re3gistry is in the geospatial domain (iso19135). Hoewever there are efforts to integrate with the semantic web. Each codelist can for example be exported as a SKOS RDF, for example in an rdf/xml format\n\n\n\n\nCommunity at GitHub\nINSPIRE registry\nSolution at ISA2\nUser manual"
  },
  {
    "objectID": "cookbook/awstats.html",
    "href": "cookbook/awstats.html",
    "title": "Soil data guidance",
    "section": "",
    "text": "Status: in progress\nAWStat is a utility to report on service usage. These days there are advanced tools such as Splunk, Elastic Search, Grafana which can report on service usage. These tools are recommended to use if your orchanisation provides it. However if such a tool is not provided AWStat could be an interesting alternative. It is a small utility with some nice vizualisation options to present aggregated usage statistics in a dashboard.\nMost of these tools, including AWStats use the Access logs of the webserver to extract and aggregate usefull information. Consider that these applications need a carefull privacy strategy, because ip-adresses from logs can be used to identify users.\n\n\nNavigate to an empty folder, place a sample log file in the folder.\n192.168.2.20 - - [28/Jul/2012:10:27:10 -0300] \"GET /cgi-bin/try/ HTTP/1.0\" 200 3395\n83.149.9.216 - - [04/Jan/2015:05:13:42 +0000] \"GET /presentations/logstash-monitorama-2013/images/kibana-search.png HTTP/1.1\" 200 203023 \"http://semicomplete.com/presentations/logstash-monitorama-2013/\" \"Mozilla/5.0 (Macintosh; Intel\"\nMac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36\"\n127.0.0.1 - - [28/Jul/2011:10:22:04 -0300] \"GET / HTTP/1.0\" 200 2216\n211.0.23.16 - - [28/Jul/2016:10:27:32 -0300] \"GET /hidden/ HTTP/1.0\" 404 7218\n211.168.17.20 - - [28/Jul/2021:10:27:10 -0300] \"GET /cgi-bin/try/ HTTP/1.0\" 200 3395\n18.12.120.17 - - [28/Jul/2022:10:22:04 -0300] \"GET / HTTP/1.0\" 200 2216\n163.22.12.13 - - [28/Jul/2018:10:27:32 -0300] \"GET /hidden/ HTTP/1.0\" 404 7218\nStart a container\ndocker run -d --restart always --publish 3000:80 --name awstats --volume $(PWD):/var/local/log:ro pabra/awstats\nParse the logs:\ndocker exec awstats awstats_updateall.pl now\nView the dashboard:\nhttp://localhost:3000\n\n\n\n\nWebsite\nDocker\nConfiguration"
  },
  {
    "objectID": "cookbook/inspire-geoportal.html",
    "href": "cookbook/inspire-geoportal.html",
    "title": "Soil data guidance",
    "section": "",
    "text": "Status: in progress\nThe INSPIRE Geoportal is the central European access point to the data provided by EU Member States and several EFTA countries under the INSPIRE Directive.\n\nWebsite: https://inspire-geoportal.ec.europa.eu/\nGitHub: https://github.com/INSPIRE-MIF/helpdesk-geoportal\n\n\n\nHarvest is the process of copying metadata from a remote source (catalogue) to the european GeoPortal.\nFor each memberstate a contact point is assigned, which is responsible for registering the national endpoint(s) to be harvested by the geoportal. The national contact point also triggers the harvest and validates the result before publishing it. At the harvest status page. In case you’re interested to have a resource harvested into the INSPIRE GeoPortal you best contact the national contact point to understand the INSPIRE practices in your country. A list of national contact points is available at https://inspire.ec.europa.eu/contact-points/57734. A quick look into the contents of the portal and the harvest status page indicates that countries have quite varying practices on accepting datasets in the geoportal.\n\n\n\nThe TG Metadata indicates that INSPIRE datasets and services are described using ISO19139:2007. Discussion is ongoing if the allowed encodings should be extended to include for example DCAT (used by European data portal) or DataCite (used by Zenodo, dataverse, etc).\n\n\n\nVarious tools exist to support describing datasets using iso19139:2007.\n\nGeoNetwork is a webbased application presenting forms and tools to assist in describing datasets.\nArcMap and QGIS include an embedded metadata editor, which is able to export to iso19139:2007\npygeometa is a python library which exports ISO19139:2007 (and other metadata encodings) from a YML encoded format called metadata control file (mcf)\n\n\n\n\n\n\n\nOperational links are an important aspect in metadata. It determines for example if a user is able to view or download a file after having discovered its metadata. The INSPIRE Geoportal applies some link checks while harvesting the metadata and adds the link check result as tags to the metadata. To optimize your links beforehand the GeoPortal offers the option to run the link checker on an arbitrary metadata record. The Link checker is available at https://inspire-geoportal.ec.europa.eu/linkagechecker.html.\n\n\n\nmetadata harvested from a national catalogue is usually available in a local language only. To offer users a better user experience, the INSPIRE geoportal translates some key elements of the metadata to english. An automated service is used, which in some cases gives a unexpected translation result. For this reason we encourage you to use the multilingual options of ISO19139:2007 to provide the metadata at least in a local language and english."
  },
  {
    "objectID": "cookbook/dcat.html",
    "href": "cookbook/dcat.html",
    "title": "Soil data guidance",
    "section": "",
    "text": "DCAT is an ontology from W3C to describe datasets in the semantic web. It is supported by software such as CKAN, GeoNetwork. The geodcat-ap group has developed a specialisation of DCAT v1 to describe spatial datasets. Many of the conventions of geodcat-ap have been introduced in follow-up editions of DCAT (v2, v3).\n\nStandard: https://www.w3.org/ns/dcat\nGeoDCAT-AP: https://semiceu.github.io/GeoDCAT-AP\nGood Paractice: https://inspire.ec.europa.eu/good-practice/geodcat-ap\n\n\n\nDCAT makes use of other ontologies, following the best practices of the Semantic Web. They are all relevant to produce consistent and usable meta-data for the web.\n\nvCard: For the description of People and Organisations according to the specification issued by the Internet Engineering Task Force (IETF) (RFC6350). Also considers addresses, communication means and inter-personal relations.\nDublin Core: A small ontology implementing the fifteen element ISO 15836-1:2017 standard for documentation meta-data. The ontology expands on the original Dublin Core with additional terms meant to facilitate meta-data creation and publication with RDF.\nPROV-O: An OWL translation of the Prov Data Model specified by the W3C. Provides a set of classes, properties, and restrictions that can be used to represent and interchange provenance information.\n\n\n\n\nThe query below provides an example of how to interact with a knowledge graph of metadata making use of the DCAT ontology. It returns a list of datasets tagged with keywords containing the string “soil”. The data property dcat:keyword was originally meant exclusively for instances of the dcat:Dataset class, but since version 2 of the ontology it can be used with any class.\n\nFollow the virtuoso/skosmos recipe up to step On the linked data tab, select Quad store upload. Instead of adding a remote url you unzip and upload a rdf snapshot of the dutch spatial catalogue.\nNavigate to http://localhost:8890/sparql/ and run the query below.\n\nPREFIX dcat: <http://www.w3.org/ns/dcat#>\nPREFIX dct: <http://purl.org/dc/terms/>\n\nSELECT ?dataset, ?title\nWHERE {\n    ?dataset a dcat:Dataset ;\n             dct:title ?title ;\n             dcat:keyword ?keyword .\n    FILTER CONTAINS(?keyword, \"EIGEN\") .\n}\nThe CONTAINS function in the query above is used to partially match the string. For an exact match the equals operator can be used instead (=). To match more than one keyword, successive FILTER clauses can be concatenated with the or operator (||).\nIf at some point your database is corrupt, you can remove all triples by running:\nDELETE FROM DB.DBA.RDF_QUAD ;\nColin Maundry provides some dcat sample sparql queries. Notice that you replace the graph url http://www.data.maudry.com/uk in the queries with yours."
  },
  {
    "objectID": "cookbook/ldproxy.html",
    "href": "cookbook/ldproxy.html",
    "title": "Soil data guidance",
    "section": "",
    "text": "Status: ready\nldproxy is an open source product by interactive instruments. The team has an important role in the development of the suite of new OGC API’s and the implementation of ldproxy is an important aspect of that process.\n\n\n\nRun the image\ndocker run -p7080:7080 -v${PWD}:/ldproxy/data iide/ldproxy\nNavigate to https://localhost:7080/manager/\nLogin as usr:admin pwd:admin (set new password)\nYou arrive in the ‘services’ page, create a new service with plus button top right\nWe’re setting up LDProxy to act as a proxy to provide OGC API Features over an existing WFS. Select type WFS\nProvide a name and a WFS url (for example https://maps.isric.org/mapserv?map=/map/wosis_latest.map&request=getcapabilities&service=wfs)\nClick ADD. You return to the list of services, select the one you’ve just created and click on the home button top right to open it.\nClick Access the data, select a collection to visualise the items of the collection.\n\nWith a tunnel, you can test the local service using inspire OGC API Features validator.\n\n\n\n\nGithub\nDocker\nDocumentation\nReport testbed Spatial data on the web"
  },
  {
    "objectID": "cookbook/geoportal-server.html",
    "href": "cookbook/geoportal-server.html",
    "title": "Soil data guidance",
    "section": "",
    "text": "Esri Geoportal server\nStatus: contribution required\nEsri Geoportal Server provides seamless communication with data services that use a wide range of communication protocols, and also supports searching, publishing, and managing standards-based resources.\nWebsite: https://enterprise.arcgis.com/en/inspire/10.8/get-started/introduction-to-geoportals.htm GitHub: https://github.com/Esri/geoportal-server-catalog/blob/master/docker/gpt_stack/geoportal/Dockerfile"
  },
  {
    "objectID": "cookbook/geohealthcheck.html",
    "href": "cookbook/geohealthcheck.html",
    "title": "Soil data guidance",
    "section": "",
    "text": "Status: ready\nA tool to monitor availability of spatial services. The tool will query a list of configured spatial services at intervals and report on availability using charts. The tool can also send out notifications in case of disruptions.\nThe tool can be compared to (and is often combined with) generic web availability tools such as Zabbix, Uptimerobot, Nagios. The generic tools are used to identify if the service is up, GeoHealthCheck will go a level deeper, identify which layers are available in a getcapabilities response and ask random maps to individual layers to identify if a service is properly running.\n\n\nThe recipe assunes docker desktop installed. Alternatively you can create a personal account at https://demo.geohealthcheck.org (click register in the login page). Start a local GeoHealthCheck container:\ndocker run --name ghc -p 80:80 geopython/geohealthcheck\n\nVisit http://localhost\nLogin as user: admin, password: admin\nClick ADD + on the top bar right, select WMS\nAdd a WMS url, for example https://maps.isric.org/mapserv?map=/map/wrb.map\nOn the next screen click add for WMS Drilldown (so all layers are validated)\nClick Save and test\nWhen finished, click Details to see the test result"
  },
  {
    "objectID": "cookbook/postgraphile.html",
    "href": "cookbook/postgraphile.html",
    "title": "Soil data guidance",
    "section": "",
    "text": "Status: in progress\nGraphQL is a de-facto standard for self describing API’s on hierarchical data. Graphql provides via its API specification capabilities to query datasets using filters, but also indicate which properties to be returned alongside with proper pagination. The GraphQL website provides a quick start on Graphql based on NodeJS.\nGraphQL is currently not endorsed as an INSPIRE Good Practice, but it fits many aspects of an INSPIRE download service. GraphQL is a good fit to disseminate measurement and observation data from soil profiles.\nAt ISRIC we use GraphQL (along WMS/WFS) to disseminate the WOSIS soil profile database.\n\n\nPostgraphile is a NodeJS server application which creates a GraphQL API on any postgres database. Besides being totally compatible with PostgreSQL it also supports PostGIS and therefore it can easily work as a Spatial GraphQL API.\n\n\n\n\n\n\n\n\n\n\n\n\n\npostgraphile\nworkshop"
  },
  {
    "objectID": "cookbook/hale-connect.html",
    "href": "cookbook/hale-connect.html",
    "title": "Soil data guidance",
    "section": "",
    "text": "Hale Connect\nStatus: contribution required\nHale Connect is a Software as a Service solution provided by wetransform to provide view and download services with metadata for rich datasets, such as those following the INSPIRE data models. Transformations are prepared in Hale Studio and effectuated in the Hale Connect Solution.\nThe services are optimised for ease of use and performance, due to the cloud native setup of the data services. A data space connector is in preparation.\n\nWebsite: https://wetransform.to/haleconnect\nGet started at https://help.wetransform.to/docs/getting-started/2018-04-28-quick-start\n\nYou can benefit from a 14-day free trial period when signing up for the platform."
  },
  {
    "objectID": "cookbook/deegree.html",
    "href": "cookbook/deegree.html",
    "title": "Soil data guidance",
    "section": "",
    "text": "Status: contribution required\nAn open source java server implementation of WMS, WMTS, CSW, WCS, WFS, WPS.\nDeegree has 2 options to publish rich GML data.\n\nUsing a relational database and a mapping configuration to generate the GML\nUsing a blob storage to provide individual features without any processing\n\nThe second is easy to setup and efficient when users often request full datasets. The second approach may get problematic if users use advanced filters to select subsets of the dataset.\n\n\nStart a deegree instance locally using the docker hub image as:\ndocker run --name deegree -d -p 8080:8080 deegree/deegree3-docker\nInitial start will take some time, then proceed with your browser to http://localhost:8080/deegree-webservices.\nThe deegree website contains a detailed quick start on how to import and operate a sample workspace\nSetting up a database and importing GML Data is managed via a command line client. The command line client can be accessed via:\ndocker exec -w /opt deegree java -jar deegree-tools-gml.jar -help\nThe client tools are described in the online manual. An example call to export a database creation script to reflect the Soil.xsd schema:\ndocker exec -w /opt/ deegree java -jar deegree-tools-gml.jar SqlFeatureStoreConfigCreator --format=ddl --dialect=postgis --cycledepth=1 -schemaUrl=https://inspire.ec.europa.eu/schemas/so/4.0/Soil.xsd\n\n\n\nSome implementations of INSPIRE Soil data services based on deegree\n\nBaden Württemberg, Germany\nBrandenburg, Germany\n\n\n\n\ndeegree is maintained by a company called LatLon and others\n\nWebsite: http://www.deegree.org/\nDocumentation: https://download.deegree.org/documentation/3.4.32/html/\nDownload: https://www.deegree.org/download/\nDocker: https://hub.docker.com/r/deegree/deegree3-docker/"
  },
  {
    "objectID": "cookbook/sql.html",
    "href": "cookbook/sql.html",
    "title": "Soil data guidance",
    "section": "",
    "text": "SQL & python\nStatus: in progress\nThis recipe implements the GeoPackage Good Practice for Soil Data by converting a sample soil database to GeoPackage based on the INSPIRE Soil Model using SQL statements from a Python environment.\nREQUIREMENTS - basic understanding of python and sql concepts - running environment python, pip, virtualenv (or alternative)\nSetup a python environment\nvirtualenv soildata\ncd soildata\n. bin/activate\nClone the workshop environment and install python requirements\ngit clone \npip install -r requirements.txt\nAs a soil database we use the SOTER database of Cuba which is available as Microsoft Access mdb as well as SQLite."
  },
  {
    "objectID": "cookbook/pycsw.html",
    "href": "cookbook/pycsw.html",
    "title": "Soil data guidance",
    "section": "",
    "text": "Status: in progress\nAn open source python catalogue implementation, used within CKAN-spatial and GeoNode.\nSupports CSW (v2 and v3) and OGC API Records. Uses a fixed metadata model and has various metadata model output formats. Capability to harvest metadata from remote sources (CSW, WMS, WFS, SOS, etc)\n\n\ndocker run -d -name pycsw -p 8000:8000 geopython/pycsw\n\nVisit https://localhost:8000\nPrepare a folder of iso19139 files, mount it on the container and load it with pycsw-admin.py\n\n\n\n\nAt masterclass edition 2023 Tom Kralidis presented the geopython ecosystem, including pycsw.\n\n\n\nWebsite\nDocumentation\nGitHub\nDocker\nDemo server\nOSGeo"
  },
  {
    "objectID": "cookbook/mapserver.html",
    "href": "cookbook/mapserver.html",
    "title": "Soil data guidance",
    "section": "",
    "text": "Status: ready\nMapServer, originally UMN MapServer, is an open source server component which provides OWS services on a variety of data sources. MapServer is commonly used to set up INSPIRE View Services. A detailed guidance on how to use MapServer to set up INSPIRE View Services is available at https://mapserver.org/ogc/inspire.html.\nMapServer supports WFS and WCS as data exchange mechanisms. MapServer is not able to publish datasets having a hierarchical structure, as common in many INSPIRE datasets, which makes MapServer less suitable to provide INSPIRE download Services using WFS. Stored queries are supported. MapServer can be used to set up a WCS Download service.\nMapServer runs as a CGI executable. The progream will start up as soon as a request arrives at the server. This makes mapserver very suitable for situations where many datasets are incidentally queried and scales out very well.\nMapServer is configured using map files. These mapfiles contain metadata for each layer, connection details to the datasource and styling rules for the vizualisation. Various tools exist which create mapfiles automatically, from for example a QGIS layer with GeoCat Bridge. Or by using python script, for example with the mappyfile library. The View services relevant for INSPIRE Soil are described in INSPIRE Data Specification on Soil – Technical Guidelines in chapter 11. 3 types of layers can be distinguished:\n\nSoil body, Soil profile and Soil Site are vector datasets indicating the location of research area’s.\nSoil properties as vector provide a map view of soil observations on soil profiles or the distribution of a soil property in soil bodies, derived from observations in the area and/or expert judgement.\nSoil properties as coverage, coverage (grid) is a common output of statistical models which calculate the distribution of a soil property.\n\n\n\nFor this recipe we’ll prepare a WMS view service on a Soil Body dataset. For each Soil Body some derived soil properties of the top soil are available.\nMapServer is configured using map files. These mapfiles contain metadata for each layer, connection details to the datasource and styling rules for the vizualisation. In a typical configuration a user ‘calls’ the mapserver executable via the web, while indicating the relevant mapfile. For example:\nhttps://example.org/mapserv.cgi?map=/data/soilbody.map&service=WMS&request=GetCapabilities\nVarious tools exist which create mapfiles automatically, from for example a QGIS layer. See for example the GeoCat Bridge software.\nIn this recipe we’ll assemble the mapfile in a text editor. For some of the more advanced text editors, such as Visual Studio Code, mapfile editing plugins are available, which provide validation and syntax highlighting.\nA generic mapfile Quick Start is provided at https://live.osgeo.org/en/quickstart/mapserver_quickstart.html. The quickstart is based on OSGeo Live, a virtual DVD, which offers a preinstalled mapserver and has data from Natural Earth.\nWithin the mapfile, created in the Quickstart, let’s replace some metadata and update the natural earth layer to point to our soil body dataset.\nMAP\n  NAME \"SOILBODY_QUICKSTART\"\n  EXTENT -180 -90 180 90\n  UNITS DD\n  SHAPEPATH \"/home/user/data/\"\n  SIZE 800 600\n\n  IMAGETYPE PNG24\n\n  PROJECTION\n    \"init=epsg:4326\"\n  END\n\n  WEB\n    METADATA\n      ows_title \"Soil Body Quickstart\"\n      ows_enable_request \"*\"\n      ows_srs \"EPSG:4326 EPSG:25832 EPSG:25833\"\n    END\n  END\n\n  LAYER\n    NAME \"Soilbody\"\n    STATUS ON\n    TYPE POLYGON\n    DATA \"soilbody\"\n    CLASS\n      STYLE\n        COLOR 246 241 223\n        OUTLINECOLOR 0 0 0\n      END\n    END\n  END\n\nEND\n\n\n\nMapServer requires a number of dependencies, which may be hard to install on some systems, that’s why this recipe suggests to work with Docker containers which are prepared to run mapserver.\nThe camp2camp mapserver image is a commonly used mapserver container image. While starting the container we provide a number of parameters so the container is able to locate the mapfile and the data files.\ndocker run -p=80:80 \\\n    -v=$(PWD)/soilbody.map:/etc/mapserver/wms.map \\\n    -v=$(PWD)/data:/home/user/data/ \\\n    camptocamp/mapserver\nFor a local installation of mapserver, options vary by platform\n\nWindows; install ms4w\nApple; brew install mapserver\nDebian\nUbuntu\n\n\n\n\nSometimes mapserver reports an error while generating a map response. If the logs of the container do not provide enough information, mapserver provides an interesting utility map2img to validate a mapfile. To run the utility, you have to open bash on the container:\ndocker exec -it <container> /bin/bash \nand then run the utily:\nmap2img -m local.map -o test2.png\n\n-m references the mapfile\n-o references an output file to be generated\n\n\n\n\nThe technical guidance provides quite detailed instructions on how to style the relavant soil layers. For example a SO.SoilBody.WRB is described with dedicated colors for each WRB Soil type.\n\n\n\nWRB RSG Code\nWRB RSG Name\nColour RGB code\nColour HEX code\n\n\n\n\nAC\nAcrisol\n(247, 152, 4)\n#F79804\n\n\nAB\nAlbeluvisol\n(254, 194, 194)\n#FEC2C2\n\n\nAL\nAlisol\n(255, 255, 190)\n#FFFFBE\n\n\n..\n..\n..\n..\n\n\n\nIf many style rules are involved (or if your project already has styling) a tool like GeoCat Bridge is helpfull. Read more about GeoCat Bridge in the recipe Bridge and GeoServer. On QGIS, with the GeoCat Bridge plugin installed, load a SoilBody dataset and assign some of the colors. In the web > bridge menu, activate the Style viewer panel. Notice the various tabs in the panel which represent the layer style in various encodings. The first tab contains Styled Layer Descriptor (SLD), a standardised styling format, used for example in GeoServer. The second tab presents the mapfile syntax, you can copy the value into your mapfile (or let Bridge generate a full mapfile).\n\n\n\nMapfile styler Geocat Bridge\n\n\nAn alternative option for creating mapfiles is the geostyler library. A NodeJS application which is able to read and write various styling formats.\nA web application mapserver studio is in preparation which will be able to create a mapfile in a web environment.\n\n\n\nMapServer includes support for WFS, OGC API Features (8+) and WCS. Note that WFS does not support the hierarchal data models as required for INSPIRE, only flat tables are supported.\n\n\n\nMapServer does not provide tile services (WMTS) itself, but is often combined with a separate tool, mapcache, which provides tile service on top of a MapServer instance. Tile services are generally a safer option with respect to Quality of Service, but less dynamic in update and styling options.\n\n\n\nYou need to set up a tunnel so the INSPIRE validator will be able to assess your local service.\n\n\n\nAt masterclass edition 2023 Seth G presented MapServer.\n\n\n\nWebsite\nGitHub\nDocker\nOSGeo"
  },
  {
    "objectID": "cookbook/xtraserver.html",
    "href": "cookbook/xtraserver.html",
    "title": "Soil data guidance",
    "section": "",
    "text": "Xtra server\nStatus: contribution required\nA proprietary java implementation of WFS, WMS. The package is also distributed as ArcGIS for INSPIRE Classic server extension. The team at interactive-instruments is and has been heavily involved in the definition of GML and the INSPIRE download specifications. They developed ShapeChange which is commonly used to export UML diagrams to XSD. Another product is LDProxy, an open source java implementation of the OGC API suite of standards.\n\nWebsite: https://www.interactive-instruments.de/en/xtraserver/"
  },
  {
    "objectID": "cookbook/zenodo.html",
    "href": "cookbook/zenodo.html",
    "title": "Soil data guidance",
    "section": "",
    "text": "This recipe proposes an approach to use Zenodo to publish a dataset in such a way, that many of the INSPIRE practices are implemented. Zenodo is a generic repository for academic work part of the European OpenAIRE program and operated by CERN. Zenodo resources are clustered in communities. You can join an existing community or start one. Resources from a community can be accessed as a collection. Community moderators (dis)allow contributions to a community.\n\n\nZenodo adopted aspects of the DataCite metadata schema. The procedure below is also relevant for other DataCite oriented repositories, such as Dataverse. Since Zenodo is DataCite oriented, it does not directly match the requirement of INSPIRE to provide iso19139 metadata. However by following the suggested procedure, the metadata is almost semantically identical. Two routes of resolving this challenge are possible:\n\nEngage with the INSPIRE community to enable the DataCite encoding as an additional format to provide INSPIRE metadata\nIntroduce a technical component, such as pygeometa, which could provide a transformation from DataCite to iso19139\n\nClement Lattelais (INRAE) is preparing a comparison tool on how the metadata schemes of various repositories (DataVerse, Zenodo, ..) relate to the INSPIRE Metadata Guidelines. This tool is relevant to evaluate if (and how) a certain repository can be used for INSPIRE. But also to assess any non listed repositories.\n\n\n\nAfter (registering and) logging in you can select the upload resource option.\n\n\n\nupload\n\n\nIn the next step a metadata form opens starting with the obvious fields, title, abstract, keywords, publication date. Notice that an existing DOI can be provided or a new one be generated by the platform.\n\n\n\nmetadata\n\n\nThe list of contact roles is very detailed in Zenodo. Verify that you have at least 1 contact which has the role of Contact point, and provide at least a valid email address.\nINSPIRE metadata guidelines mandate a number of fields which are not available on the Zenodo metadata form. There are 2 options to still provide this metadata.\n\nProvide a iso19139 document as an additional attachement to the record.\nUse the subject field to provide additional metadata\n\nFor option 2, the relevant element is the subject area at the bottom of the metadata form. Each subject is defined by a label and a URI. These URI’s can reference concepts which provide the required context to the data. The table below suggests a number of subjects to be added, related to specific metadata requirements\n\n\n\nRequirement\nThesaurus\nExample\n\n\n\n\nTopic category\nTopicCategory\ngeoscientific Information\n\n\nConditions applying to access and use\nConditionsApplyingToAccessAndUse\nno Conditions Apply\n\n\nGeographic bounding box\nGeoNames\nItaly\n\n\nINSPIRE Theme\nTheme register\nSoil\n\n\nPriority dataset\nPriority dataset\nDirective 2008/56/EC\n\n\nDegree of conformity\nDegree of conformity\nNot evaluated\n\n\nIACS Data\nIACS Data\nlpis\n\n\nSpatial Data Service Type\nSpatial Data Service Type\nView service\n\n\nSpatial Scope\nSpatial Scope\nRegional\n\n\n\n\n\n\nsubject\n\n\n\n\n\nThis approach is based on the recipe on WebDav and Atom services. The suggestion is to upload 2 additional Atom.xml files which describe the resources in an Atom format. The first file describes the Atom Download Service. The second Atom describes the distributions of the dataset. The zenodo approach will not cover all the aspects of a INSPIRE ATOM Download service, the OpenSearch option is missing."
  },
  {
    "objectID": "cookbook/geoserver.html",
    "href": "cookbook/geoserver.html",
    "title": "Soil data guidance",
    "section": "",
    "text": "Status: contribution required\nGeoServer is an open source java imlementation of WFS, WCS, WPS, WMS, CSW. Various OGC API endpoints are available via a OGCAPI community plugin. WMTS is available via a default plugin called GeoWebCache.\nGeoServer is a popular server component because of the initial ease of setup and configuration in a webbased environment. It includes an authentication and authorisation system and advanced styling options. Configuration via a webinterface also has some negative aspects related to reproducability and scaling. GeoServer is able to provide INSPIRE data via the appschema plugin and (INSPIRE plugin)[https://docs.geoserver.org/stable/en/user/extensions/inspire].\n\n\ndocker run -p 8080:8080 kartoza/geoserver:2.22.0\n\nNavigate to http://localhost:8080/geoserver\nLogin as usr:admin pwd:geoserver\n\nLoad some data:\n\ncreate a workspace\ncreate a datastore of type folder of shapefiles\ncreate a layer\n\nPreview the layer.\nConsider that in this setup the configuration is lost at every restart of the container. In a normal scenario, you would mount a volume to persist the geoserver configuration. Optimally you place the volume under version control, so you can easily revert a previous situation.\n\n\n\nA GeoServer INSPIRE plugin is available which adds some of the INSPIRE specific metadata properties to the OWS capabilities documents. For example a link to the service metadata. The main feature is that it adds a bbox for each of the available projection systems. GeoServer is known to list all projection systems (many) as part of the capabilities response. You need to limit this number to prevent the bounds be written in each of this projections.\n\n\n\nAppschema is a plugin for GeoServer which adds the capability to work with hierarchival GML data, such as the INSPIRE Soil data model.\nOnegeology has prepared a workshop on how to set up an appschema dataset in GeoServer. This is an advanced workshop.\nAt Foss4G 2022 in Florence one of the maintainers of GeoServer, GeoSolutions, announced a new approach to appschema in GeoServer, based on templating. I have not been able to test it yet, but it may resolve some of the challenges of the appschema approach.\n\n\n\nGeoServer also provides options to publish view services (WMS or WMTS). Read more about this topic in the recipe GeoCat Bridge and GeoServer.\n\n\n\nWebsite: https://geoserver.org GitHub: https://github.com/geoserver/ Docker: https://docker.osgeo.org/geoserver Issue management: https://osgeo-org.atlassian.net/projects/GEOS/summary OSGeo: https://www.osgeo.org/projects/geoserver/"
  },
  {
    "objectID": "cookbook/pygeoapi.html",
    "href": "cookbook/pygeoapi.html",
    "title": "Soil data guidance",
    "section": "",
    "text": "Status: in progress\nIn this recipe we’ll set up an instance of pygeoapi with some soil data. A good practice is available on how to provide an INSPIRE download service based on OGC API Features. pygeoapi is an open source python server implementation of OGCAPI Features, Tiles, Maps, Coverages, Records and Processes.\n\n\nThe recipe is based on Docker. New to docker? Read more in the Docker recipe.\n\nWith commandline in a new folder, run this command:\n\ndocker run -p 5000:80 geopython/pygeoapi:latest\n\nNavigate with your browser to http://localhost:5000\n\nIf all went fine, you now see the default pygeoapi installation with sample data. In the next step we’ll publish a new soil dataset. pygeoapi’s configuration is stored in a config file. The config file is encoded as YAML. The first part configures the main settings of the service, in the second part individual datasets are configured.\n\nDownload the Dutch INSPIRE dataset ‘soil drills’ from https://service.pdok.nl/bzk/brobhrpvolledigeset/atom/v1_1/downloads/brobhrpvolledigeset.zip\nUnzip the file to the work folder\nCreate a local config file in the folder, download it from here\nRemove all the datasets from the config folder, and replace it for the following:\n\nresources:\n    bro:\n        type: collection\n        title: BRO\n        description: Bro soil drills\n        keywords:\n            - soil\n        links:\n            - type: application/geopackage+sqlite\n              rel: canonical\n              title: source data\n              href: https://service.pdok.nl/bzk/brobhrpvolledigeset/atom/v1_1/downloads/brobhrpvolledigeset.zip\n              hreflang: en-US\n        extents:\n            spatial:\n                bbox: [-180,-90,180,90]\n                crs: http://www.opengis.net/def/crs/OGC/1.3/CRS84\n        providers:\n            -   type: feature\n                name: OGR\n                data:\n                    source_type: GPKG\n                    source: /pygeoapi/brobhrpvolledigeset.gpkg\n                    gdal_ogr_options:\n                        SHPT: POINT\n                id_field: bro_id\n                layer: borehole_research\n\nMount config file in containter\n\ndocker run -p 5000:80 -v ${PWD}/pygeoapi-config.yml:/pygeoapi/local.config.yml -v ${PWD}/brobhrpvolledigeset.gpkg:/pygeoapi/brobhrpvolledigeset.gpkg  geopython/pygeoapi:latest\n\n\n\nJRC recently extended the INSPIRE validator. It can now also validate an OGC API Features service. Because docker runs locally, you need to set up a tunnel for the validator to access the local service. Read the tunnel recipe to see how to do that.\n\n\n\nAt masterclass edition 2023 Tom Kralidis presented the geopython ecosystem, including pygeoapi.\n\n\nThe geopython community has prepared a workshop on getting started with pygeoapi.\n\nWebsite\nGitHub\nDocker\nDemo server\nDocumentation\nOSGeo"
  },
  {
    "objectID": "cookbook/52north.html",
    "href": "cookbook/52north.html",
    "title": "Soil data guidance",
    "section": "",
    "text": "Status: planned\n\n\n\nWebinar SOS and INSPIRE: https://www.youtube.com/watch?v=jyQJrTN4pjk\nGitHub: https://github.com/52North/SOS\nDocker hub: https://hub.docker.com/r/52north/sos\nOSGeo Live Quick Start: https://live.osgeo.org/en/quickstart/52nSOS_quickstart.html\nSpecialised observations for INSPIRE: https://wiki.52north.org/SensorWeb/InspireSpecialisedObservations"
  },
  {
    "objectID": "cookbook/rml.html",
    "href": "cookbook/rml.html",
    "title": "Soil data guidance",
    "section": "",
    "text": "RML.io is a toolset for the generation of knowledge graphs. They automate the creation of RDF from diverse data sources, primarily unstructured tabular data.\nRML.io has programmes to be used on-line and to be installed on computer systems (Linux, MacIntosh and Windows platforms are supported). The former are useful for prototyping, whereas the latter are meant for actual transformations of large datasets.\n\n\nThe RML tools apply data transformations according to a set of rules recorded in a YAML file. This file must respect a specific syntax, named YARRRML. This specification defines a number of sections (or environments) in the YAML file that lay out the structure of the resulting triples.\nThe first of these sections is named prefixes and provides the space for the definition of URI abbreviations, in all similar to the Turtle syntax. Each abbreviation is encoded as a list item and can be used in the reminder of the YARRRML as it would be in a Turtle knowledge graph.\nprefixes:\n rdf: http://www.w3.org/1999/02/22-rdf-syntax-ns#\n xsd: http://www.w3.org/2001/XMLSchema#\n geo: http://www.opengis.net/ont/geosparql#\nNext comes the mappings section, where the actual transformations are encoded. This section is to be populated with sub-sections, one for each individual subject class (or type) necessary in the output RDF. For instance, if the transformation must produce triples for profiles and layers, then a sub-suction for each is necessary. The name of these subject sub-sections is arbitrarily chosen by the user.\nmappings:\n  profile:\n\n  layer:\nFor each subject class sub-section at least one data source needs to be specified in the sources section. The source can be declared within square brackets (i.e. a YAML collection), providing a path to a file followed by a tilde and then a type. The sources section can be more intricate, as YARRRML supports a wide range of different data sources, including flat tables, databases and Web APIs.\nmappings:\n  profile:\n    sources:\n      - ['SoilData.csv~csv']\nThe following sub-section of the class declares the subject and has the simple name of s. Its purpose is to define the URI structure for the instances of the class. In principle this is also the first element that makes reference to the contents of the source file. In the case of CSV, as in this example, the column names are used. They are invoked using the dollar character ($), with the column name within parenthesis. The practical result is the generation of an individual element (subject in this case) for each distinct value found in the source column.\n  profile:\n    sources:\n      - ['SoilData.csv~csv']\n    s: http://my.soil.org#$(profile_id)\nWith the subject defined, triples can be completed with predicates and objects in sub-section po. This section is itself composed by a list, whose items comprise a pair: predicate (item p) and object (item o). The predicate is encoded as a URI in a similar way to the subject, using abbreviations if necessary. As for the object it can be decomposed further into a value and a datatype to accommodate literals.\nThe example below creates triples for the layer class subject, using the layer_id column in the source to generate subject URIs. The source column layer_order is used to complete triples declaring the order of a layer within a profile.\nprefixes:\n xsd: http://www.w3.org/2001/XMLSchema#\n iso28258: http://w3id.org/glosis/model/iso28258/2013#\n\nmappings:\n  layer:\n    sources:\n      - ['SoilData.csv~csv']\n    s: http://my.soil.org#$(layer_id)\n    po:\n      - p: iso28258:ProfileElement.order\n        o:\n           value: \"$(layer_order)\"\n           datatype: xsd:integer\nThe encoding of the predicates and objects list can be shortened with collections. Instead of discriminating value and datatype, they can be expressed as elements of a collection. This formulation is useful when the object is itself a URI. Note how in the example below (for the layer class) the tilde is used again, to indicate the object type.\n    po:\n      - [iso28258:Profile.elementOfProfile, http://my.soil.org#$(layer_id)~iri]\nThis was just a brief introduction to the YARRRML syntax. It goes far deeper, even allowing for some functional programming. While the guidelines in this document make enough of a start to automated RDF generation, the documentation is indispensable to take full advantage of the RML tool set.\n\n\n\nThe simplest way to start using RML.io is through the Matey online user interface. It is an excellent prototyping tool and will help you getting acquainted with the YARRRML syntax.\nThe standard view of Matey has 4 sections:\n\na section for input data;\na section to define YARRRML rules;\na section to display RDF output;\na section to visualise exported RML.io rules from YARRRML.\n\nThere are various examples available to guide you through the basics of YARRRML and RML. Take some time to experiment with these examples, try modifying the output, or even to create further transformation rules.\nEventually you will find the limitations of Matey, while convenient for prototyping, it does not scale for large datasets or to process a large number of source files. For that you need to use the command line interface.\n\n\n\nUsing RML.io in your system requires two programmes, a parser for the YARRRML syntax (yarrrml-parser) and a transformer that converts tabular data to RDF (rmlmapper).\nThe first of these programmes is installed with npm:\nnpm i -g @rmlio/yarrrml-parser\nrmlmapper is a Java programme, that can be downloaded directly from the project GitHub page. For instance:\nwget https://github.com/RMLio/rmlmapper-java/releases/download/v6.1.3/rmlmapper-6.1.3-r367-all.jar\nIt can then be run with the Java Runtime Environment:\njava -jar rmlmapper-6.1.3-r367-all.jar\nAt this stage it might be useful to create a shortcut to call the programme with a simple command like rmlmapper. How to do this depends on your system and is beyond the scope of this document.\n\n\n\nNote: before starting a data transformation into RDF you must devise a URI policy for your data. Please refer to the URI Policy document for details.\nThe file SoilData.csv contains a simple set of hypothetical measurements referring to three soil profiles collected in two different sites. The goal is to transform this dataset into GloSIS compliant RDF.\nsite_id,lat,lon,profile_id,layer_id,upper_depth,lower_depth,pH,SOC,\n1,49.43,8.31,1,11,0,15,7.4,6,\n1,49.43,8.31,1,12,15,40,7.2,4,\n1,49.43,8.31,2,21,0,10,8,3,\n1,49.43,8.31,2,22,10,30,8.1,2,\n2,46.82,11.45,3,31,0,15,6.8,1,\n2,46.82,11.45,3,32,15,30,6.7,1,\n2,46.82,11.45,3,33,30,60,6.7,0,\n\n\nThe simplest place to start is with the profiles. There are three essential elements to generate for each profile: - A new URI for the profile; - The declaration of the new profile as an instance of the class GL_Profile; - The association with the respective site.\nBelow are the contents of the file profile.yarrrml that encodes this transformation. Note how the URIs of both the profile and the site are created using the prefixes.\nprefixes:\n wosis_prf: http://wosis.isric.org/profile#  \n wosis_sit: http://wosis.isric.org/site#\n glosis_pr: http://w3id.org/glosis/model/profile# \n iso28258: http://w3id.org/glosis/model/iso28258/2013#\n\nmappings:\n  profile:\n    sources:\n      - ['SoilData.csv~csv']\n    s: wosis_prf:$(profile_id)\n    po:\n      - [a, glosis_pr:GL_Profile]\n      - [iso28258:Profile.profileSite, wosis_sit:$(site_id)~iri]\nTo perform the actual transformation you must first apply yarrrml-parser to create the RML transformation file and then use rmlmapper to obtain the actual knowledge graph. By default rmlmapper creates a Turtle file that is printed to the standard output (STDOUT). You can use the parameters -o to redirect output to a text file and -s to select an alternative serialisation syntax.\nyarrrml-parser -i profile.yarrrml -o profile.rml.ttl\nrmlmapper -s turtle -m profile.rml.ttl\n\n\n\nSites are the spatial features in the knowledge graph, therefore they require the creation of appropriate GeoSPARQL instances. Three new elements must be addressed in this transformation: - Declaration of the site as an instance of the class geo:Feature; - Creation of a geo:Geometry instance to host the actual geo-spatial information; - A literal of the type geo:wktLiteral or geo:gmlLiteral to encode the geometry.\nThe file site.yarrrml achieves this transformation. Its contents are reproduced below:\nprefixes:\n geo: http://www.opengis.net/ont/geosparql#\n glosis_sp: http://w3id.org/glosis/model/siteplot# \n wosis_sit: http://wosis.isric.org/site#\n wosis_geo: http://wosis.isric.org/geometry#\n\nmappings:\n  site:\n    sources:\n      - ['SoilData.csv~csv']\n    s: wosis_sit:$(site_id)\n    po:\n      - [a, glosis_sp:GL_Site]\n      - [a, geo:Feature]\n      - [geo:hasGeometry, wosis_geo:$(site_id)~iri]\n\n  geometry:\n    sources:\n      - ['SoilData.csv~csv']\n    s: wosis_geo:$(site_id)\n    po:\n      - [a, geo:Point]\n      - p: geo:asWKT\n        o:\n           value: \"POINT($(lon) $(lat))\"\n           datatype: geo:wktLiteral\nThis example also shows the inclusion of two different classes in the same transformation. Note how the Feature is associated with the geometry using the geo:hasGeometry object property. Also important is the creation of the WKT literal, as it requires a verbose declaration of the object to make the type explicit.\n\n\n\nHaving created a transformation for the sites, one for the layers is not much of a challenge. Download the file layer.yarrrml and try it yourself.\nLook carefully at the transformation file, note how the object properties from the ISO28258 module are used to declare the layers depths.\nQuestion: what would be different if in the source dataset horizons were identified instead of layers?\n\n\n\nAn example transformation for the measurements in the original dataset is available in the file measurements.yarrrml. The extra elements to address in this transformation are:\n\nInstance of the respective Observation class;\nInstance of the respective Result class;\nRelation between Observation and Result;\nNumerical literal with the measurement result.\n\nQuestion: Identify in the Layer Horizon module of GloSIS which are the units of measurement associated with the Result instances used in this example.\nExercise I: Create a new yarrrml file including all the transformations given above, creating all necessary triples for sites, profiles, layers and measurements. Make sure it is correctly parsed by yarrrml-parser and generate a new, complete, knowledge graph.\nExercise II: Modify the transformation you obtained in the previous exercise so that it declares all pH measurements as resulting from a H2O procedure (water solution).\n\n\n\n\nNow that you obtained a RDF knowledge graph you can publish it to the internet. Follow the guide on Virtuoso to learn how.\nIn alternative to RML, you may transform tabular data into RDF with SPARQL queries using tarql. Follow that guide for the details."
  },
  {
    "objectID": "cookbook/glosis-db.html",
    "href": "cookbook/glosis-db.html",
    "title": "Soil data guidance",
    "section": "",
    "text": "This recipe describes an approach where data is harmonised to a common relational database.\nThe INSPIRE community has recently started an interesting activity to set up guidelines on how to share INSPIRE data as a relational database. More specifically a database in the GeoPackage format, which is a specialisation of the SQLite format. This will result in a series of new good practices on alternative approaches for data harmonization for various INSPIRE themes and use cases.\nIdea behind the activity is that communities around a certain topic come together to develop a common relational model which substantially represents the INSPIRE UML model. This aspect can be validated by providing a mapping document which maps the relational model to the INSPIRE UML or GML model. This activity is supported by a template that communities can use to share their work with the wider INSPIRE community.\nAfter an initial effort of defining and describing the alternative model, a typical workflow to publish harmonised data is as follows:\n\nUsers download the GeoPackage template as an empty database\nUsers populate the database from various sources using their favourite tool (r, python, FME, Hale studio, DBeaver)\nUsers publish the database as an Atom service or OGC API Features.\n\n\n\nThe INSPIRE Soil Model is designed to capture multiple soil data use cases: - capture profile descriptions in the field based on horizons - capture laboratory results from soil samples at fixed depths (layers) - predicted distribution of soil properties within soil bodies, linked to derived soil profiles\nIn many cases these use cases are not combined in a single database. By creating a dedicated database model for specific use cases (remove the unused database object), the database model will be smaller and easier to understand.\n\n\n\nSoil model\n\n\n\n\n\nIn many cases, such as the Soil Erosion case, data is combined which is described in multiple INSPRE themes, such as soil, hydrology and environmental facilities. Some of the current GeoPackage implementation advertised in the INSPIRE MIF reference this type of combined use cases.\n\n\n\nAs part of the Soils4africa project ISRIC and partners are experimenting with a ISO25258 model, encoded in a relational PostGres database. It is interesting to evaluate if this effort can be ported to GeoPackage and form a starting point for the initial effort.\n\n\n\nAt the masterclass edition 2023 Stefania Morrone (Epsilon) presented an approach to use geopackage as an alternative encoding for Soil data.\n\n\n\nGood practice on GeoPackage\nModel Transformation Rules for alternative encodings"
  },
  {
    "objectID": "cookbook/fme.html",
    "href": "cookbook/fme.html",
    "title": "Soil data guidance",
    "section": "",
    "text": "FME\nStatus: contribution required\nSafe software propides a proprietary solution for data harmonization, e.g. conversion to and from INSPIRE data models. An introduction to the topic is provided at https://www.safe.com/integrate/inspire-gml/"
  },
  {
    "objectID": "cookbook/geonetwork.html",
    "href": "cookbook/geonetwork.html",
    "title": "Soil data guidance",
    "section": "",
    "text": "Status: in progress\nGeoNetwork is a catalogue for registering spatial datasets and services. GeoNetwork does support multiple metadata models based on XML, but it is optimized for iso19139:2007 and iso19115-2:2018. This recipe uses docker to run GeoNetwork locally. It will discuss aspects such as schema plugins, creating metadata records, set up codelists and harvest metadata.\n\n\nGeoNetwork provides a dynamic system to pre load schema plugins providing support for a variety of metadata models, such as iso19139:2007, iso19115-2:2018, national profiles based on these, DCAT, SensorML, EML/GBIF, etc. Many of these plugins are made available via https://github.com/metadata101. Before creating or importing records, verify that the relevant profile is available in the GeoNetwork instance.\nCreation of metadata records is based on templates. For each metadata model a series of templates is available. Users select a relevant template for their use case while creating a new record to describe a resource. Any template is based on a specific metadata model, which determines which properties need to be described and in which format the record will be stored.\nGeoNetwork provides a number of transformation options for crosswalks between metadata models, but you should always consider some loss of information in these crosswalks. Crosswalks occur for example when a user requests a record in DCAT, while it is stored as iso19139:2007 in the database.\n\n\n\n\nStart a GeoNetwork instance locally (initial startup may take some time)\ndocker run -p 8080:8080 geonetwork:3.12\nNavigate to http://localhost:8080/geonetwork\nLet’s load some sample data. Select Login; login as usr:admin, pwd:admin\nSelect Admin console > Metadata records and templates; Select iso19139:2007, click on load samples and load templates\nOn Admin console > Settings, set the title and url of the instance.\nLet’s set up some code lists (which populate the pull downs on the editor). Select Classification systems from Admin console.\nSelect From registry in Add thesaurus.\nClick Use INSPIRE registry for the Url field\nSelect language(s) and INSPIRE theme register and click Upload\nAdd another thesaurus from registry, select INSPIRE metadata code list register and then Spatial scope.\nContinue with other relevant code lists, notice that you can also create a new code list manually.\n\n\n\n\n\nOn Contribute Editor board, click Add new record\nSelect a template (they we’re loaded on the previous step) and click create\nOn the editor notice the view (eye) button on top right, you can switch the editor view between Simple, Full and XML.\nNotice the validate button, which provides a report on the level of completion of the record\nThe associated resources side panel displays links to remote resources, such as data files, data services and thumbnails\nNotice that you can also import a record from a local xml file\nNotice that you can collapse the save button to save as template, in this way others can use this record as a base to start a new record\n\n\n\n\nHarvesting is the process of importing records from remote sources at intervals.\n\nOpen harvesting from Admin console\nSelect OGC CSW 2.0.2 from Add thesaurus\nProvide a name for the harvester\nAction on UUID collision determines the behaviour when similar records are found in multiple remote endpoints\nProvide the url of the remote endpoint (for example https://www.geocatalogue.fr/api-public/servicesRest?request=GetCapabilities&service=CSW)\nAdd a filter Anytext:Soil\nSet a interval schedule for the harvest (only one run)\nHarvesters have many additional options, you can read about it in the documentation\nClick Save and on the next screen harvest, a spinner starts to run, the harvest may take some minutes, depending of the size of the remote catalogue\nNotice a list of harvest run logs at the bottom of the screen, you can click log to check in more detail.\nIf you increase logging on Settings to DEV, the logging on harvesting will also provide more details (in case of non explainable errors)\n\n\n\n\nThe metasearch plugin is a default plugin in QGIS. - Open the plugin from the Web menu (or toolbar). - Click New connection. Provide a name for the connection and the url http://localhost:8080/geonetwork/srv/eng/csw - Switch to the find tab, and search some records. - Select a search result, for some search results the load data button (lower left) is activated and you can load some data to the map\n\n\n\nYou can enable an ATOM download service in GeoNetwork. GeoNetwork provides an opensearch API and will use the metadata content to generate Atom service and dataset files. You can read more about this option in the documentation.\n\n\n\n\nWebsite: https://geonetwork-opensource.org/\nGitHub repository: https://github.com/geonetwork\nDocker composition: https://github.com/geonetwork/docker-geonetwork/blob/main/4.2.1/docker-compose.yml\nDocumentation: https://geonetwork-opensource.org/manuals/4.0.x/en\nTutorial: https://geonetwork-opensource.org/manuals/trunk/en/tutorials/introduction"
  },
  {
    "objectID": "cookbook/hale-studio-consume-gml.html",
    "href": "cookbook/hale-studio-consume-gml.html",
    "title": "Soil data guidance",
    "section": "",
    "text": "Consume Soil GML with Hale Studio\nStatus: in progress\nHale Studio is a familiar tool to transform data from a relational datamodel to INSPIRE Soil GML. However you can also use Hale Studio to connect to an existing WFS or load a GML file from a atom service and use Hale Studio to transform the data to a relational database model, so you can easily combine it with other relational databases.\n!!!note\nIf you're not interested in a specific target model, the [GMLAS functionality within OGR](https://gdal.org/drivers/vector/gmlas.html) may be sufficient for you. GMLAS will create a arbitrary relational model from any GML. With GDAL installed, and a gml file called *soil.gml*, run the following from commandline:\n\nogrinfo -ro GMLAS:soil.gml\n\nogr2ogr -f SQLite tmp.sqlite GMLAS:soil.gml -dsco SPATILIATE=YES -nlt CONVERT_TO_LINEAR -oo EXPOSE_METADATA_LAYERS=YES\nIn this recipe we will transform INSPIRE Soil GML from the city of Berlin to a relational database (GeoPackage).\n\n(Install and) Start the Hale Studio tool\nImport soil data from the Berlin Soil at https://fbinter.stadt-berlin.de/fb/atom/SO/SO_KrBwBoF2015.zip\nunzip the file to a new folder\nIn hale studio, at File > Import > Source model, select the soil.xml included in the zip file\nAt File > Import > Source data, select the INSPIRE GML.gml included in the zip file, select default options on the import wizard\nAt File > import > Target model, select an empty database."
  },
  {
    "objectID": "cookbook/pygeometa.html",
    "href": "cookbook/pygeometa.html",
    "title": "Soil data guidance",
    "section": "",
    "text": "status: in progress\nThis recipe presents a minimalistic, however integrated and standardised approach to metadata management. Each data file on a file system will be accompagnied by a minimal YML metadata file. Crawler scripts will pick up these metadata files and publish them as iso19139 (or alternative models) on a catalogue. iso19139 is the metadata model currently mandated by INSPIRE and very common in the GeoSpatial domain. Other communities tend to use different standards, such as STAC (Earth Observation), DCAT (Open Data), DataCite (Academia), etc.\nThe recipe introduces you to a pythonic metadata workflow step by step.\n\n\nThe inital step assumes a folder of data files on a network drive, sharepoint or git repository. Datasets stored on a database will not be considered for now, but can follow a similar workflow.\nFor each data file in the folder we will create a metadata control file (MCF). MCF is a convention from the pygeometa community. It is a YAML encoded subset of iso19139:2007. YAML is easy to read by humans and an optimal for content versioning (in git).\nConsider to set up a virtual environment for the workshop:\nvirtualenv pygeometa && cd pygeometa && . bin/activate\nThen install the pygeometa library.\npip install pygeometa\n\n\n\nA minimal example of MCF is (see also a more extended version):\nmcf:\n    version: 1.0\n\nmetadata:\n    identifier: 3f342f64-9348-11df-ba6a-0014c2c00eab\n    language: en\n    hierarchylevel: dataset\n    datestamp: 2023-01-01\n\nspatial:\n    datatype: grid\n\nidentification:\n    language: eng\n    title: Soilgrids sample Dataset\n    abstract: This is a sample dataset for the EJP Soil Dataset Assimilation Masterclass\n    dates:\n        creation: 2023-01-01\n    keywords:\n        default:\n            keywords: [\"sample\"]\n    topiccategory:\n        - geoscientificInformation\n    extents:\n        spatial:\n            - bbox: [2,50,4,52]\n              crs: 4326\n    fees: None\n    accessconstraints: otherRestrictions\n    rights: CC-BY\n\ncontact:\n    pointOfContact: \n        organization: ISRIC - World Soil Information\n        url: https://www.isric.org\n        city: Wageningen\n        country: The Netherlands\n        email: info@isric.org\n\ncontent_info:\n    type: image\n    dimensions:\n        - name: N\n          units: g/m3\n          min: 10\n          max: 75\n        - name: P\n          units: mg/m3\n          min: 30\n          max: 75\n        - name: K\n          units: mg/m3\n          min: 0.5\n          max: 1.2\n\ndistribution:\n    wms:\n        url: https://maps.isric.org\n        type: OGC:WMS\n        rel: service\n        name: soilgrids\n\nUse the above template to create a metadata file for a dataset. The file should have the same name as the dataset, but with an extension .yml or .mcf.\n\n\n\n\n\nIf the data file already has a metadata document (for example with a shapefile, if it contains a file with extension .shp.xml), you can try to import it using pygeometa. pygeometa requires to indicate the metadata schema in advance.\n\nFor iso19139:2007 use:\npygeometa metadata import path/to/file.xml --schema=iso19139\nFor fgdc (typically used with shapefiles) use:\npygeometa metadata import path/to/file.xml --schema=fgdc\n\n\n\nAs soon as you have a folder of MCF’s, you can use pygeometa generate to convert them to iso19139:2007.\npygeometa metadata generate path/to/file.yml --schema=iso19139 --output=some_file.xml\nOr for a folder of files:\nFILES=\"/path/to/*.yml\"\nfor f in $FILES\ndo\n  echo \"Processing $f file...\"\n  pygeometa metadata generate $f --schema=iso19139 --output=$f.xml\ndone\nNotice that you can also create your own template for the iso19139 generation. By using a customised template you’re able to optimise the generated iso19139 records to facilitate for example better INSPIRE complience.\npygeometa metadata generate path/to/file.yml --schema_local=/path/to/my-schema --output=some_file.xml\n\n\n\npycsw is a python based OGC reference implementation of Catalog Service for the Web and an early adaptor of OGC API Records and STAC Catalog. We’ll use pycsw via a docker image to publish the metadata records in a search service. We run it in detach mode so we can interact with the running container, type docker stop pycsw to stop the container.\ndocker run -d --rm --name pycsw -p 8000:8000 geopython/pycsw\nWe now have a running pycsw at http://localhost:8000/collections with some sample data. We will now remove the sample data and insert our metadata. For that reason we mount our current folder with xml files into the container\ndocker stop pycsw\ndocker run -d --rm --name pycsw -v ${PWD}:/metadata -p 8000:8000 geopython/pycsw\nNow we can trigger pycsw admin to remove the default records and import our metadata. As part of the calls we reference the config file, which contains the connection details to the database.\ndocker exec -ti pycsw pycsw-admin.py delete-records -c /etc/pycsw/pycsw.cfg\ndocker exec -ti pycsw pycsw-admin.py load-records -c /etc/pycsw/pycsw.cfg -p /metadata -r\nCheck out the new content at http://localhost:8000/collections. Note that if you restart the container, all records are removed, because the database is currently not persisted on a volume.\nTry to mount also a customised configuration file into the container, so you can optimise the configuration of the catalogue. Also have a look at the INSPIRE extension for pycsw.\n\n\n\nYou can evaluate individual iso19139 records in the INSPIRE reference validator. Also you can evaluate the discovery service. If a service is running on localhost, use the tunnel approach to evaluate it.\n\n\n\nQGIS contains a default plugin called MetaSearch which enables catalogue searches from within QGIS. You can find the plugin in the web menu or on the toolbar as a set of binoculars. Open the plugin. First you need to set up a new service connection. On the services tab, click new, choose a name and add the url http://localhost:8080/csw. Click the serviceinfo button to view the metadata of the service. Now return to the Search tab and perform a search. Notice that if you select a search result, it highlights on the map and may trigger the Add data button in the footer (this depends on if QGIS recognises the protocol mentioned in the metadata).\n\n\n\nAt masterclass edition 2023 Tom Kralidis presented the geopython ecosystem, including pycsw.\n\n\n\ngithub the geopython community welcomes your questions and contributions.\npygeometa\npycsw\npyGeoDataCrawler is a set of scripts to manage MCF’s. It supports importing MCF from a CSV, MCF inheritence, generate MCF from a data file, etc.\nModel Driven Metadata Editor A web based GUI for populating MCF’s."
  },
  {
    "objectID": "cookbook/codelist-iso19135.html",
    "href": "cookbook/codelist-iso19135.html",
    "title": "Soil data guidance",
    "section": "",
    "text": "status: draft\nThis recipe presents a minimal approach for publishing a (extended) code list. The code list is stored as a text file on a web accessible folder. A number of potential formats to use for the text file are discussed in this article. Concepts within the codelist are referenced as http://example.com/codes.xml#concept. The concept identifier is concatenated to the url with a ‘#’ character. ‘#’ represents a local link within the document, a convention adopted from the html standard.\n\n\nThe richest and most common format for code list storage is SKOS (RDF). It is the format used by for example Glosis to publish its codelists. Tools like GeoNetwork and Hale Studio are able to ingest codelists based on the SKOS ontology.\nAn alternative format is based on the iso19135:2005 standard. TC 211 develops the iso19135 standard to offer a format for code lists in the spatial data community. The standard is for example used in the gmx-codelists as used in iso19139:2007.\nThe approach described on this page is for example used in Lithuania. The gml described in this catalogue record links to codelists published in an XML format inspired by the INSPIRE registry. See https://inspire-geoportal.lt/resources/codelist/SO/OtherHorizonNotationTypeValue.xml. This format is probably selected because it can directly be ingested by Hale Studio.\n\n\n\nThe location of the codelist can be a webserver folder (apache/nginx), a git repository, even a shared sharepoint folder. But it is important that the url of the file is persistent for a considerable period. Because the datasets which link to codelist items depend on its availability.\nPersistency can be improved by adding an intermediary layer between the location of the file and the url on which it is made available, mechanisms such as provided by DOI and W3ID. Glosis codelists are for example stored at https://github.com/rapw3k/glosis but are referenced as http://w3id.org/glosis/model/codelists and then forwarded.\n\n\n\nA dedicated recipe describes the actual code list extension mechanism.\n\n\n\nWhen a human arrives at a codelist file, if it is not in html format, the syntax with https://example.org/#concept opens the file at the top, and does not point to the relavant section, because the web browser does not understand the format. A mechanism of content negotiation can identify web browsers and present them an alternative format (html). Content negotiation can be set up in an intermediary webserver layer, the transformation from SKOS/iso19135 to html should be managed by an extra utility (such as SKOSMOS).\n\n\n\nBoth SKOS, ISO19135 and the Re3gistry format support the option to provide labels for concepts in multiple languages."
  },
  {
    "objectID": "cookbook/rasdaman.html",
    "href": "cookbook/rasdaman.html",
    "title": "Soil data guidance",
    "section": "",
    "text": "Coverage data is the digital representation of some spatio-temporal phenomenon. Usually in the form of a grid of cells having a certain resolution. Grid cells can provide a value for multiple phenomena. The OGC Web Coverage Service and the upcoming OGC API Coverages provide a standardised mechanism to query coverage data over the web. Web Coverage Service is the main option to provide a Download service on INSPIRE coverage data. An alternative option is INSPIRE Atom.\nRasdaman is software to set up a Web Coverage Service. The Rasdaman team has prepared a tutorial for setting up an INSPIRE Coverage service at Good practice on coverage data. The tutorial is set up using Jupyter. New to Jupyter? Read the Jupyter recipe.\n\n\nAt masterclass edition 2023 Kathi Schleidt presented INSPIRE CSW using Rasdaman.\n\n\n\nWebsite\nDocker\nSources\nTickets\nOsGeo\nRasdaman & INSPIRE"
  },
  {
    "objectID": "cookbook/frost-server.html",
    "href": "cookbook/frost-server.html",
    "title": "Soil data guidance",
    "section": "",
    "text": "Status: review required\nFROST is an open source server implementation of OGC Sensorthings, a modern data exchange standard for Sensor Data. Because the INSPIRE Soil model is based on Observations and Measurements, the Sensorthings API can be used to provide sensor download services, with the soil profile and horizons as FeatureOfInterest. Sensorthings API is generally easier to set up for administrators and easier to consume by clients then INSPIRE Soil data in GML. INSPIRE has recently adopted a good practice on Download services based on SensorThings API.\nKathi Schleidt has prepared a workshop on inspire data in sensorthings API. Also have a look at her sensor presentation in the 2022 edition of the training. After the 2022 training Kathy together with colleagues from Fraunhofer IOSB has put together a SoilThings variant of STA. The experiment has proven to be a most interesting use case, it has shown where flexibility has to be added in the STA model (these changes will be integrated in the upcoming 2.0 version of the standard). The cool thing about this API is that it allows you to do queries like the following: give me all plots that contain a profile that contains a horizon on which pH H2O is measured; while the query syntax can be a bit daunting, the other existing technologies for provision (WFS and the new OGC API) would require 5 separate requests to do this. Links:\n\nBase SoilThings API: https://ogc-demo.k8s.ilt-dmz.iosb.fraunhofer.de/FROST-SoilThings/v1.1/\nQuery on all plots that contain a profile that contains a horizon on which pH H2O is measured, including the measurements:\nhttps://ogc-demo.k8s.ilt-dmz.iosb.fraunhofer.de/FROST-SoilThings/v1.1/SoilPlots?\\(count=true&\\)select=name&\\(filter=ObservedProfile/IsDescribedByHorizon/Datastreams/ObservedProperty/name%20eq%20%27pH%20H2O%27&\\)expand=ObservedProfile(\\(select=name;\\)expand=IsDescribedByHorizon(\\(select=name;\\)expand=Datastreams(\\(select=name;\\)filter=ObservedProperty/name%20eq%20%27pH%20H2O%27;\\(expand=ObservedProperty(\\)select=name),%20Observations)))\n\n\n\nAt masterclass edition 2023 Kathi Schleidt and Hylke van der Schaaf presented Sensorthings API using Frost server.\n\n\n\nGood practice Sensorthings\nGitHub repository\nDocumentation\nWorkshop"
  },
  {
    "objectID": "cookbook/hale-studio.html",
    "href": "cookbook/hale-studio.html",
    "title": "Soil data guidance",
    "section": "",
    "text": "Status: in progress\nHALE Studio aims to enable users to set up a harmonization workflow on datasets from a local model to a common model, such as INSPIRE. The user interface presents the model of the source dataset (derived from database) on the left and the target model on the right (derived from xml schema). Conversion rules are defined by selecting similar properties on both sides.\nThe open source software has been developed in the scope of a European Research project, HUMBOLDT (2006) and is currently maintained by a company called WeTransform in Darmstad Germany. WeTransform hosts the Hale Studio user guide and a user forum. The Git repository for Hale Studio is at https://github.com/halestudio/hale.\nThis recipe has been developed in the scope of a Masterclass on data assimilation within the EJP Soil project and builds on harmonization work performed in the scope of the eDanube project.\nIn this recipe we’ll harmonize a SOTER database to the INSPIRE model. Read more about the INSPIRE Soil model in the relevant technical guidelines. Some samples of harmonised INSPIRE soil data are available from these data providers:\n\nCity of Berlin\nwielkopolski region in Poland\nLihuania (1.6GB)\n\n\n\n\nSOTER database\nPreparing the data\nInstall & get started with Hale Studio\nDefine harmonization rules\nCodelist Mappings\nAnytype in XSD\nExport GML\nExport GeoPackage\nValidate GML\n\n\n\n\nFor this recipe we’re going to use the SOTER database of Cuba. Download the zip file from https://data.isric.org/geonetwork/srv/eng/catalog.search#/metadata/f31ac19f-67a4-4f64-94cc-d4f063ea9add.\nThe SOTER programme was initiated in 1986 by the Food and Agricultural Organization (FAO), the United Nations Environmental Programme and ISRIC, under the auspices of the International Soil Science Society. The aim of the programme was to develop a global SOTER database at scale 1:1 million that was supposed to be the successor of the FAO-UNESCO Soil Map of the World. A SOTER database with global coverage was never achieved, but SOTER databases were developed for various regions, countries and continents.\nThe picture below shows the database structure of a SOTER database. The database structure allows to store terrain, soil profile up to wet chemistry results.\n\n\n\nSoter schema\n\n\nIn this recipe we’re focussing on the RepresentativeHorizonValues table mostly, which contains observed properties for each horizon.\n\n\n\nNotice that, like many other soil databases, the observed soil property values are listed as columns for each horizon. The INSPIRE model instead uses the Observations and Measurments model, in which each observation is an individual entity which includes the feature of interest (e.g. Horizon 2), the observed property (e.g. pH), the result (e.g. 7.2) and a procedure (e.g. pHCaCl2).\n\n\n\nObservations and measurements, source 52North\n\n\nA data transformation required for this step is challenging within Hale, but relatively easy within the database. So before starting up Hale we’ll make an initial transformation within the database.\nThe SOTER zip file contains a SQLite as well as a Access version of the database. In this recipe we’ll work with Access, but you can also use the SQLite version, in that case install for example SQLite browser to interact with the database. Some of the queries may slightly vary between SQLite and Access.\nRun the query below, to create a new OBSERVATIONS table.\nIn Access create a new query in design view.\n\n\n\nQuery design\n\n\nThen open SQL view.\n\n\n\nSQL view\n\n\nRun the query, by clicking Run in the Query design toolbar.\nSELECT * INTO OBSERVATIONS\nFROM (Select HONU,PRID,'SCMO' as PARAM,SCMO as RESULT from RepresentativeHorizonValues where SCMO is not null union\nSelect HONU,PRID,'SCDR' as PARAM,SCDR as RESULT from RepresentativeHorizonValues where SCDR is not null union\nSelect HONU,PRID,'STGR' as PARAM,STGR as RESULT from RepresentativeHorizonValues where STGR is not null union\nSelect HONU,PRID,'STSI' as PARAM,STSI as RESULT from RepresentativeHorizonValues where STSI is not null union\nSelect HONU,PRID,'STTY' as PARAM,STTY as RESULT from RepresentativeHorizonValues where STTY is not null union\nSelect HONU,PRID,'SDVC' as PARAM,SDVC as RESULT from RepresentativeHorizonValues where SDVC is not null union\nSelect HONU,PRID,'SDCO' as PARAM,SDCO as RESULT from RepresentativeHorizonValues where SDCO is not null union\nSelect HONU,PRID,'SDME' as PARAM,SDME as RESULT from RepresentativeHorizonValues where SDME is not null union\nSelect HONU,PRID,'SDFI' as PARAM,SDFI as RESULT from RepresentativeHorizonValues where SDFI is not null union\nSelect HONU,PRID,'SDVF' as PARAM,SDVF as RESULT from RepresentativeHorizonValues where SDVF is not null union\nSelect HONU,PRID,'SDTO' as PARAM,SDTO as RESULT from RepresentativeHorizonValues where SDTO is not null union\nSelect HONU,PRID,'STPC' as PARAM,STPC as RESULT from RepresentativeHorizonValues where STPC is not null union\nSelect HONU,PRID,'CLPC' as PARAM,CLPC as RESULT from RepresentativeHorizonValues where CLPC is not null union\nSelect HONU,PRID,'PSCL' as PARAM,PSCL as RESULT from RepresentativeHorizonValues where PSCL is not null union\nSelect HONU,PRID,'BULK' as PARAM,BULK as RESULT from RepresentativeHorizonValues where BULK is not null union\nSelect HONU,PRID,'ELCO' as PARAM,ELCO as RESULT from RepresentativeHorizonValues where ELCO is not null union\nSelect HONU,PRID,'SSO4' as PARAM,SSO4 as RESULT from RepresentativeHorizonValues where SSO4 is not null union\nSelect HONU,PRID,'HCO3' as PARAM,HCO3 as RESULT from RepresentativeHorizonValues where HCO3 is not null union\nSelect HONU,PRID,'SCO3' as PARAM,SCO3 as RESULT from RepresentativeHorizonValues where SCO3 is not null union\nSelect HONU,PRID,'EXCA' as PARAM,EXCA as RESULT from RepresentativeHorizonValues where EXCA is not null union\nSelect HONU,PRID,'EXMG' as PARAM,EXMG as RESULT from RepresentativeHorizonValues where EXMG is not null union\nSelect HONU,PRID,'EXNA' as PARAM,EXNA as RESULT from RepresentativeHorizonValues where EXNA is not null union\nSelect HONU,PRID,'EXCK' as PARAM,EXCK as RESULT from RepresentativeHorizonValues where EXCK is not null union\nSelect HONU,PRID,'EXAL' as PARAM,EXAL as RESULT from RepresentativeHorizonValues where EXAL is not null union\nSelect HONU,PRID,'EXAC' as PARAM,EXAC as RESULT from RepresentativeHorizonValues where EXAC is not null union\nSelect HONU,PRID,'CECS' as PARAM,CECS as RESULT from RepresentativeHorizonValues where CECS is not null union\nSelect HONU,PRID,'TCEQ' as PARAM,TCEQ as RESULT from RepresentativeHorizonValues where TCEQ is not null union\nSelect HONU,PRID,'GYPS' as PARAM,GYPS as RESULT from RepresentativeHorizonValues where GYPS is not null union\nSelect HONU,PRID,'P2O5' as PARAM,P2O5 as RESULT from RepresentativeHorizonValues where P2O5 is not null union\nSelect HONU,PRID,'PRET' as PARAM,PRET as RESULT from RepresentativeHorizonValues where PRET is not null union\nSelect HONU,PRID,'FEDE' as PARAM,FEDE as RESULT from RepresentativeHorizonValues where FEDE is not null union\nSelect HONU,PRID,'PHAQ' as PARAM,PHAQ as RESULT from RepresentativeHorizonValues where PHAQ is not null union\nSelect HONU,PRID,'PHKC' as PARAM,PHKC as RESULT from RepresentativeHorizonValues where PHKC is not null union\nSelect HONU,PRID,'SONA' as PARAM,SONA as RESULT from RepresentativeHorizonValues where SONA is not null union\nSelect HONU,PRID,'SOCA' as PARAM,SOCA as RESULT from RepresentativeHorizonValues where SOCA is not null union\nSelect HONU,PRID,'SOMG' as PARAM,SOMG as RESULT from RepresentativeHorizonValues where SOMG is not null union\nSelect HONU,PRID,'SOLK' as PARAM,SOLK as RESULT from RepresentativeHorizonValues where SOLK is not null union\nSelect HONU,PRID,'SOCL' as PARAM,SOCL as RESULT from RepresentativeHorizonValues where SOCL is not null union\nSelect HONU,PRID,'FEPE' as PARAM,FEPE as RESULT from RepresentativeHorizonValues where FEPE is not null union\nSelect HONU,PRID,'ALDE' as PARAM,ALDE as RESULT from RepresentativeHorizonValues where ALDE is not null union\nSelect HONU,PRID,'CLAY' as PARAM,CLAY as RESULT from RepresentativeHorizonValues where CLAY is not null union\nSelect HONU,PRID,'TOTC' as PARAM,TOTC as RESULT from RepresentativeHorizonValues where TOTC is not null union\nSelect HONU,PRID,'TOTN' as PARAM,TOTN as RESULT from RepresentativeHorizonValues where TOTN is not null  \n);\nVerify that a new table OBSERVATIONS is available and that it is populated.\n\n\n\nDownload and install Hale Studio from github. There are installers for windows, linux and apple. Kate Lyndegaard from WeTransform published a nice overview of Hale Studio at https://www.youtube.com/watch?v=BKNMV-Jp9HM&t=332s.\n\nFirst create a new project\nImport the SOTER database as source schema.\nImport the same database file again as ‘source data’.\nRepeat these 2 steps for the Cuba shapefile, available in the GIS folder of the zip file\nLoad the INSPIRE Soil model as a target schema. Load the latest version of the model from https://inspire.ec.europa.eu/schemas/so/4.0/Soil.xsd (from url tab, click detect after entering the url).\n\n\n\n\nLoad target\n\n\n\n\n\nWe’ll go through some cases to highlight some of the features, we’ll not produce a full mapping.\n\n\nIn order to link the geometries from the shapefile to the SOTER data, we’ll use a table join. On the left column, select the shapefile as well as the terrain, soils and soilscomponent tables (ctrl-click). On the right colum select the SoilBody type. Now click the blue arrow in the middle and select the join method.\n\n\n\nHale join\n\n\n\n\n\nXML allows to embed a property or to reference the value of the property elsewhere. An example; both snippets below have the same meaning, the first is easier to read, the second is easier to handle by software (prevent duplication).\n<person role=\"student\" name=\"Peter\">\n    <memberOf>\n        <class name=\"2B\">\n            <hasMember>\n                <person role=\"teacher\" name=\"Cynthia\">\n            </hasMember>\n        </class>\n    <memberOf>\n</person>\nAnd\n<person role=\"student\" name=\"Peter\" gml:id=\"#student-peter\">\n    <memberOf xlink:href=\"#class-2b\">\n</person>\n<person role=\"teacher\" name=\"Cynthia\" gml:id=\"#teacher-cynthia\">\n    <memberOf xlink:href=\"#class-2b\">\n</person>\n<class name=\"2B\" gml:id=\"#class-2b\"/>\nA good practice is to add reverse links to the second snippet:\n<class name=\"2B\" gml:id=\"#class-2b\">\n    <hasMember xlink:href=\"#teacher-cynthia\"/ >\n    <hasMember xlink:href=\"#student-peter\" />\n</class>\nBoth approaches are supported and can be combined in Hale Studio, but you have to consider upfront which approach to use when. The first approach becomes quite complex if the levels of nesting increase.\nA suggestion from our side; define Plot, Profile, OM_Observation and Laboratory as root types and embed other types.\n\n\n\n\nA common challenge in harmonization is the adoption and extension of common codelists. Hale Studio facilitates the codelist mapping with the possibility to import codelists from the INSPIRE registry and the possibility to define a mapping file to map a local code to a common code.\nFirst let’s import a codelist from the INSPIRE registry.\n\nIn file > import > Codelist, select Import from INSPIRE registry\nImport the SoilProfileParameterNameValue codelist\n\n\n\n\nImport a codelist in Hale\n\n\nHale studio supports the INSPIRE registry xml format as well as codelists based on the SKOS ontology. However SKOS codelists should be encoded as RDF/XML. In order to load the GLOSIS codelists, which are encoded as turtle, you can use a python snippet below to convert it to RDF/XML first.\nfrom rdflib import Graph\ng = Graph()\ng.parse(\"https://raw.githubusercontent.com/rapw3k/glosis/master/glosis_procedure.ttl\",format='ttl')\ng.serialize(destination='output.xml', format='xml')\nOr use an online conversion tool such as https://issemantic.net/rdf-converter.\nNow assign a mapping from local values to this codelist.\n\nSelect the observed property (bulkdens, organic matter, …) on the source model\nOn target model select the href attribute of the observedProperty of the OM_Observation\nClick the blue button and select the classification item\nProceed to second screen and click the second button Attempt to fill source..., the unique values of the source data are displayed\nDouble click the empty value next to the first value, on the panel select the button to select a codelist\nSelect the codelist, notice the pull down is now populated, select a value from the pulldown\nContinue the mapping for each of the values\n\n\n\n\nselect a concept from list\n\n\n\nIn case you can’t find a relevant target value for a source value, then have a look at the extending codelist recipe.\n\n\n\n\nXSD allows to leave the type of a property as any. From a standardisation perspective, this is not optimal, because every developer may implement a different type for that field. In the INSPRE Soil theme this challenge is very obvious because the type of the result property of an observation is defined as any. Hale Studio is not able to process anytype fields by default. Instead you have to add below snippet to the eu.esdihumboldt.hale.io.schema.read.target resource of the project.halex file, to map the any field to a CharacterString.\n <complex-setting name=\"customTypeContent\">\n    <xsd:typeContentConfig xmlns:xsd=\"http://www.esdi-humboldt.eu/hale/io/xsd\">\n        <core:list xmlns:core=\"http://www.esdi-humboldt.eu/hale/core\">\n            <core:entry>\n                <xsd:association>\n                    <xsd:property>\n                        <core:list>\n                            <core:entry>\n                                <core:name namespace=\"http://www.opengis.net/om/2.0\">OM_ObservationType</core:name>\n                            </core:entry>\n                            <core:entry>\n                                <core:name namespace=\"http://www.opengis.net/om/2.0\">result</core:name>\n                            </core:entry>\n                        </core:list>\n                    </xsd:property>\n                    <xsd:config>\n                        <xsd:typeContent mode=\"elements\">\n                            <xsd:elements>\n                                <core:list>\n                                    <core:entry>\n                                        <core:name namespace=\"http://www.isotc211.org/2005/gco\">CharacterString</core:name>\n                                    </core:entry>\n                                </core:list>\n                            </xsd:elements>\n                        </xsd:typeContent>\n                    </xsd:config>\n                </xsd:association>\n            </core:entry>\n        </core:list>\n    </xsd:typeContentConfig>\n</complex-setting>\n\n\n\n\nOn the File menu, select Export > Transformed data. A panel opens.\nSelect the GML (FeatureCollection) format.\nUse one of projections suggested by INSPIRE, EPSG:4258 if you’re not sure which. Enable the EPSG prefix.\nFinish the wizard, a GML file will be generated.\n\n\n\n\nHale Studio also facilitates an export to GeoPackage. Hale Studio is able to auto generate a relation data model based on the XSD schema. See also the recipe on GeoPackage.\n\nFrom the format selection, select the GeoPackage format.\nSelect the relevant projection and finish.\nUse DBeaver or some other tool to view the outputs. DBeaver has a usefull ER-diagram vizualisation option.\n\n\n\n\nYou can test the generated GML in the INSPIRE Validator. Select Dataset, and from Annex III, the Soil theme. Solve the numeric riddle, upload the GML file, add a label and start the test."
  },
  {
    "objectID": "cookbook/code-listsExtension.html",
    "href": "cookbook/code-listsExtension.html",
    "title": "Soil data guidance",
    "section": "",
    "text": "This recipe describes a semantic approach based on SKOS to extend an INSPIRE code-list.\n\n\nCode-lists specified for the Soil domain within the INSPIRE directive are described in the report D2.8.III.3 Data Specification on Soil – Technical Guidelines. A brief overview of these code-lists is offered below.\n\n\n\nCode-list\nDescription\n\n\n\n\nPhenomenonTypeValue\n[Extensible] A code list of phenomena (e.g. temperature, wind speed). Parent of ProfileElementParameterName, SoilSiteParameterName, SoilProfileParameterName and SoilDerivedObjectParameterName.\n\n\nLayerGenesisProcessStateValue\nAn indication of whether the process specified in layerGenesisProcess is ongoing or ceased in the past.\n\n\nLayerTypeValue\nClassification of a layer according to the concept that fits the purpose, e.g. “topsoil”.\n\n\nProfileElementParameterNameValue\n[Extensible] List of properties that can be observed to characterize the profile element.\n\n\nSoilDerivedObjectParameterNameValue\n[Extensible] List of soil related properties that can be derived from soil and other data.\n\n\nSoilInvestigationPurposeValue\nList of terms indicating the reasons for conducting a survey.\n\n\nSoilPlotTypeValue\nList of possible values that give information on what kind of plot the observation of the soil is made.\n\n\nSoilProfileParameterNameValue\n[Extensible] List of properties that can be observed to characterize the soil profile.\n\n\nSoilSiteParameterNameValue\n[Extensible] List of properties that can be observed to characterize the soil site.\n\n\nWRBQualifierPlaceValue\nList of values to indicate the placement of the Qualifier with regard to the WRB reference soil group (RSG), according to naming rules of the World reference base for soil resources 2006, first update 2007.\n\n\nWRBQualifierValue\nList of possible qualifiers (i.e. prefix and suffix qualifiers of the World Reference Base for Soil Resources 2006, first update 2007).\n\n\nWRBReferenceSoilGroupValue\nList of possible reference soil groups (i.e. first level of classification of the World Reference Base for Soil Resources 2006, first update 2007).\n\n\nWRBSpecifierValue\nList of possible specifiers, comprising only the values specified in the World Reference Base for Soil Resources 2006, first update 2007.\n\n\n\nThese code-lists are all available on-line through a r3gistry instance. Each item in these code-lists is a dereferenceable by URI.\nFour of these lists are extensible, those expressing properties associated with the different features of interest. They are designed to serve as umbrella structures for further specialisation according to local practices. Extension is made in an hierarchical fashion, meaning that each additional item must be a narrower definition of one of the existing items.\nBesides these curated code-lists, the INSPIRE domain model also creates space for code-lists of physio-chemical procedures of soil analysis. These are to be referenced from the Observation instances related to the properties referred above.\nThere is no practical guidance in the Technical Guidelines document on how to extend or create new code-lists towards INSPIRE compliance. This document provides broad guidelines on how to do so with the Simple Knowledge Organisation System (SKOS), described ahead.\n\n\n\nThe GloSIS web ontology includes a large range of code-lists that are relevant within the INSPIRE context. GloSIS follows the pattern of the Sensor, Observation, Sample, and Actuator (SOSA) ontology, which is the Semantic Web counterpart of the Observations & Measurements standard (O&M). These code-lists are expressed with SKOS.\nThe GloSIS code-lists spread along four main categories:\n\nValues meeting Descriptive properties (FAO Guidelines of Soil Description):\n\nSite/Plot (40)\nLayer/Profile (90)\nSurface (5)\n\nPhysio-chemical properties (circa 80 items)\nProcedures of physio-chemical analysis (circa 230 items).\n\nCode-lists in GloSIS are gathered in a generic module. With physio-chemical analysis procedures in a specific module.\n\n\n\n\n\nThe SKOS ontology is remarkably simple, actually one of its strengths. At its core are five primitives:\n\nConcept - a unit of thought, an idea, a meaning, a category or an object. Concepts are identified with URIs.\nLabel - a lexical string used to annotate a concept. The same concept may be annotated in different natural languages.\nRelation - a semantic association between two concepts, conveying hierarchy or simply connecting concepts in a network.\nScheme - an aggregator of related concepts, usually forming a hierarchy.\nNote - provides further semantics or definition to a concept. Often used to associate a concept to other knowledge graphs or other external resources.\n\nThe listing below presents an item from the GloSIS code-list for crop classes. Its URI is http://w3id.org/glosis/model/codelists#cropClassValueCode-Ce_Ba, abbreviated with the Turtle syntax to glosis_cl:cropClassValueCode-Ce_Ba. The label for this item is “Barley” in the English language, an annotation also indicates its short notation: “Ce_Ba”. Two object properties provide semantic associations, this item is a narrower definition of the glosis_cl:cropClassValueCode-Ce item (the “Cereals” concept), and is part of the scheme glosis_cl:cropClassValueCode.\n@prefix skos:  <http://www.w3.org/2004/02/skos/core#> .\n@prefix glosis_cl: <http://w3id.org/glosis/model/codelists#> .\n\nglosis_cl:cropClassValueCode-Ce_Ba a skos:Concept, glosis_cl:CropClassValueCode;\n        skos:prefLabel \"Barley\"@en ;\n        skos:notation \"Ce_Ba\" ;\n        skos:inScheme glosis_cl:cropClassValueCode ;\n        skos:broader glosis_cl:cropClassValueCode-Ce .\nThe next listing shows the scheme integrating the “Barley” concept above. This object is very similar, with a label providing its name and some notes adding further definition. The literal “table 9” indicates the source of this code-list within the FAO Guidelines for Soil Description.\n@prefix rdfs:  <http://www.w3.org/2000/01/rdf-schema#> .\n@prefix skos:  <http://www.w3.org/2004/02/skos/core#> .\n@prefix glosis_cl: <http://w3id.org/glosis/model/codelists#> .\n\nglosis_cl:cropClassValueCode a skos:ConceptScheme ;\n        skos:prefLabel \"Code list for CropClassValue - codelist scheme\"@en;\n        rdfs:label \"Code list for CropClassValue - codelist scheme\"@en;\n        skos:note \"This  code list provides the CropClassValue.\"@en;\n        skos:definition \"table 9\" ;\n        rdfs:seeAlso glosis_cl:CropClassValueCode .\nUsing this simple pattern, a hierarchical code-list can be developed further simply using the predicates skos:inScheme, skos:broader and skos:narrower. The predicates skos:topConceptOf and skos:hasTopConcept can be further used to indicate the root items in a concept hierarchy.\n\n\n\nSuppose you intend to publish results of chemical analyses appraising the Zinc content of the soil. The INSPIRE code-lists do not presently include that metal as property, therefore an additional code-list item must be created.\nThe first action is to identify in which code-list the item should be added. Physio-chemical properties like Zinc content appear associated with the Horizon or Layer class in soil ontologies, thus the appropriate code-list is ProfileElementParameterNameValue. Open that URI in your browser an go through the respective r3gistry record.\nAt the bottom of the page r3gistry lists the items for this code-list, in this case they are biological parameter, chemical parameter and physical parameter. Follow the URI to the chemical parameter, in that page there is another list of items, including other metals. The new item for Zinc should therefore be created at this same level in the hierarchy.\nThe list below presents a RDF document defining a new sub-item for Zinc within the physical parameter code-list. The URI for this item is http://example.com/my-soil/zincContent. As this item is not directly in a INSPIRE code-list, its URI must refer to an authority that you control, i.e. you or your institution are the responsible party for the code-list item, its definition and publication. For more on this, check the URI strategy document.\n@prefix skos: <http://www.w3.org/2004/02/skos/core#> .\n@prefix inspire_cl_profile: <http://inspire.ec.europa.eu/codelist/ProfileElementParameterNameValue/> .\n@prefix glosis_cl: <http://w3id.org/glosis/model/codelists#> .\n@prefix my_soil: <http://example.com/my-soil/> . \n\nmy_soil:zincContent a skos:Concept;\n        skos:prefLabel \"Zinc content\"@en ;\n        skos:definition \"Zinc content of the soil within a profile element.\" ;\n        skos:broader inspire_cl_profile:physicalParameter ;\n        skos:inScheme inspire_cl_profile: ;\n        skos:related glosis_cl:physioChemicalPropertyCode-Zin .\nThe RDF above contains the basic SKOS elements: a Concept instance, a label, an annotation defining the concept and a reference to the parent item with the skos:broader code-list. The last two triples need a closer look. The item is declared as belonging to a Scheme with the URI http://inspire.ec.europa.eu/codelist/ProfileElementParameterNameValue/. While the later is not actually declared as such in r3gistry, it is still important to convey the nature of this item as part of a structured INSPIRE code-list. Finally, the skos:related predicated is employed to refer a similar property in the GloSIS web ontology. This last triple is not at all mandatory, but provides another dimension to the code-list item.\n\n\n\nThe GloSIS web ontology becomes particularly relevant in the sections of the INSPIRE domain model for which code-lists do not yet exist. A prominent case is the list of laboratory procedures to assess physico-chemical properties. As mentioned above, GloSIS includes a specific module for procedures.\nThe listing below presents a RDF document with an extra item for the soil texture procedures code list (SKOS scheme glosis_proc:textProcedure). Compared to the previous example, the most remarkable aspect here is the declaration of a glosis_proc:TextProcedure instance. The latter is a sub-class of the SOSA Procedure class, making the bridge to the Observation instances declared in other GloSIS modules.\n@prefix skos: <http://www.w3.org/2004/02/skos/core#> .\n@prefix glosis_proc: <http://w3id.org/glosis/model/procedure#> .\n@prefix my_soil: <http://example.com/my-soil/> . \n\nmy_soil:textureProcedure-SaSiCl_5-50-2000u a skos:Concept, glosis_proc:TextureProcedure ;\n    skos:inScheme glosis_proc:textProcedure ;\n    skos:topConceptOf glosis_proc:textureProcedure ;\n    skos:prefLabel \"SaSiCl_5-50-2000u\"@en ;\n    skos:notation \"SaSiCl_5-50-2000u\" ;\n    skos:definition \"Sand, silt and clay fractions as used in my country (5-50-2000um)\" .\nThe remaining triples should all be familiar by now. The link to the SKOS scheme is made with the skos:inScheme and skos:topConceptOf predicates. Then come the annotations.\n\n\n\nINSPIRE and GloSIS cover quite a good deal of ground with their code-lists. However, there might be circumstances where you may require an entire new code-list to express a domain property specific to your institution or dataset.\nThe examples above already provide the necessary to build a code-list on your own. In summary, these are the steps:\n\nDevise a URI policy, encompassing the code-list and its items.\nCreate a Scheme instance to aggregate the code-list items. The Scheme provides a URI and annotation for the code-list.\nAdd in the top Concept instances with the predicates skos:inScheme, skos:topConceptOf and skos:hasTopConcept.\nIf necessary, add in narrower terms forming a hierarchy with skos:broader and skos:narrower.\nLink items to similar concepts in other ontologies or vocabularies if possible.\n\n\n\n\n\nNow that you extended or even created a new code-list the next step is to publish it on-line. There are three essentials ways of doing so, briefly described below.\n\nA simple on-line RDF file. Save the items or code-list as a RDF document and deploy it to a web server. Fairly easy, but not the most user friendly.\nA knowledge graph deployed to a triple store. Load the RDF document to a triple store or directly create the code-list triples with SPARQL queries. Triple stores usually provide end-points for interaction with SPARQL queries and other mechanisms that facilitate user access/view of the code-list. Virtuoso is a triple store providing these functionalities.\nA SKOSMOS instance. Load the code-list triples into a triple store and then link it to a SKOSMOS instance. This software leverages the SKOS ontology to presnnt code-lists in a user friendly, browsable complex of HTML pages.\n\n\n\nWhen publishing a code-list, it is important to facilitate the resolution of the respective URIs. This promotes accessibility and use of the code-list. With a simple deployment to a web server, the hypothetical URI used above: http://example.com/my-soil can point directly to the RDF file. In this case it is customary the use of the hash character (#) to distinguish different resources within the same file. E.g. http://example.com/my-soil#zincContent for the Zinc item.\nWith more advanced software like SKOSMOS, a mid-layer mechanism is necessary to translate the URIs declared in the RDF into the corresponding pages or services. More about this topic in the Identification document.\n\n\n\n\nAt the masterclass edition 2023 Luis de Sousa presented an approach to extend codelists based on SKOS.\n\n\nIn this article Wetransform explains how and which INSPIRE codelists can be extended. Either the UML model and/or the INSPIRE registry indicate if a codelist is extensible."
  },
  {
    "objectID": "cookbook/bridge-geoserver-geonetwork.html",
    "href": "cookbook/bridge-geoserver-geonetwork.html",
    "title": "Soil data guidance",
    "section": "",
    "text": "Status: ready\nIn this recipe we use GeoCat Bridge to publish a View Service on a soil dataset in GeoServer, combined with connected metadata in a GeoNetwork instance.\nGeoCat Bridge is a plugin for QGIS or ArcMap developed by GeoCat in Bennekom, the Netherlands. Its goal is to facilitate the complex process of data publication from a well known desktop environment. An introductionary video is available at https://www.youtube.com/watch?v=f-sZCVnR9dc\nGeoServer is a server application providing OGC services on various database backends. See the relevant recipe for a more detailed description.\nGeoNetwork is a server application providing a search interface and various api’s on a collection of metadata records. See the relevant recipe for a more detailed description.\n\n\n\nDeploy GeoServer and GeoNetwork using Docker\nInstall and configure the Bridge plugin\nPublish the dataset\n\n\n\n\nThis recipe is based on Docker, but you can also install each of the components directly on your system. New to Docker? Read the Docker recipe.\nWe use the QGIS edition of Bridge, but an edition for ArcMAP is also available.\nDownload the file https://github.com/ejpsoil/ejpsoil-data-publication-guidance/docker/bridge-geoserver-geonetwork/docker-compose.yml into an empty folder. Navigate to the folder using a shell client (windows powershell or Linux/Apple shell) and run:\ndocker compose up\nThe above statement will download and deploy docker containers for GeoServer, GeoNetwork, PostGreSQL and Elastic Search. When finished (it may take a long time), you can access GeoServer at https://localhost:8000/geoserver and GeoNetwork at https://localhost:8001/geonetwork.\nGeoNetwork starts with an error, because the database is empty. Navigate to http://localhost:8001/geonetwork/srv/eng/admin.console#/metadata, login as user: admin, password: admin. Select the iso19139:2007 profile and click Load templates and Load samples.\n\n\n\nWith QGIS, open the QGIS plugins repository and search for the GeoCat Bridge plugin. Install it.\nAfter succesfull installation, we will configure our GeoServer and GeoNetwork instances.\nFind the Bridge module on the toolbar or in the Web > GeoCat Bridge > Publish menu. From the publish window, open the Servers tab.\nIn the bottom left click the New server option and select GeoServer. Populate the fields.\nClick New server again and select GeoNetwork. Populate the fields.\nYou are now ready to publish your first dataset from QGIS.\n\n\n\nOpen the dataset to be published. Configure the layer with relevant styling and labels.\nOpen the publish window from the menu (or toolbar).\nSelect the layer to be published. A metadata editor will open in which you can configure some metadata. You can also import embedded metadata (use the button with a downward arrow, top right).\nSelect the target servers for the publisation and click the publish button.\nWhen returning to the publish panel, you will notice an icon behind each layer indicating the publication status. You can now right click on the layer to preview the wms layer or the metadata.\nNote that the styling options in QGIS and GeoServer are not a full match. Some styling transformations are applied which may impact the final result on GeoServer. A common caveat is the availability of certain fonts, used for labeling or icons, on the client and the server. Make sure all used fonts are available on the server as well. |\n\n\n\nThe INSPIRE Validator provides a validation of view services. It will mainly test if metadata elements are available and the service is reachable.\nThe docker containers run locally, so the services can not be tested by the INSPIRE Validator. In Local Tunnel an approach is suggested to temporarily host a local service online, so you can run the validation.\nWe have not yet installed the INSPIRE plugin on GeoServer and optimized the configuration of GeoNetwork, so expect some tests to fail"
  },
  {
    "objectID": "cookbook/qgis.html",
    "href": "cookbook/qgis.html",
    "title": "Soil data guidance",
    "section": "",
    "text": "Status: done\nQGIS started as a GIS desktop application. In recent years the community prepared a server edition of QGIS. The server provides WMS, WFS and since recently OGC API Features.\nThe advantage of using QGIS both as a desktop and server component is that the maps generated on the server will display exactly the same as those prepared on a desktop client.\n\n\nUse an existing QGIS project, or create a new project with some sample data. If you include any data, place it in a folder within the current folder, so docker can access it as a docker mounted volume.\nName your project project.qgs.\nOn project settings, open the WMS properties to add relevant metadata.\n\n\n\nNavigate with a console application to the folder which contains project.qgs.\nWe’re running QGIS server as a docker container based on https://hub.docker.com/r/camptocamp/qgis-server. Run the QGIS container:\ndocker run -p 8080:80 --volume=$PWD:/etc/qgisserver camptocamp/qgis-server\nTry the service via:\nhttp://localhost:8080/?SERVICE=WMS&REQUEST=GetCapabilities\n\n\n\nIn order for the INSPIRE validator to access your local service, you need to set up a tunnel.\n\n\n\nQGIS can act as a client for WMS, WMTS, WCS, CSW, Sensorthings API, OGC API Records and OGC API Features.\nThe QGIS data model is based on data layers and has minimalistic support for joins. QGIS is therefore less optimal for rich GML data as used in INSPIRE. There have been initiatives to bring hierarchical data to QGIS, such as GMLAS but adoption has been limited. The Application Schema functionality has been introduced in GDAL/OGR, the QGIS plugin builds on top of it. OGR converts the contents of the GML to a relational database. Individual tables from that database are loaded in the QGIS interface."
  },
  {
    "objectID": "cookbook/webdav.html",
    "href": "cookbook/webdav.html",
    "title": "Soil data guidance",
    "section": "",
    "text": "Status: ready\nIn the technical Guidelines download services INSPIRE atom services are described to provide a light weight alternative to WFS and WCS, while fitting with all the aspects of a webservice as described in the implementing rules.\nThis recipe describes a minimal approach which is based on placing a number of Atom-xml files along side the downloadable resources in a web accessible folder or webdav. Consider that WebDav is used as an example, any online file system would suffice. A plain apache webserver, Zenodo or even sharepoint or dropbox.\n\n\nVarious webdav implmentations exist; apache webdav, NGINX DAV, SFTPGO, wsgidav. For this recipe we’ll use wsgidav, but others will work in a similar way.\nWsgiDAV provides a docker image, with below statement you advertise the current folder via WebDAV.\ndocker run --rm -it -p 8080:8080 -v ${PWD}:/var/wsgidav-root mar10/wsgidav\nOpen http://localhost:8080 in your browser to see the file contents.\n\n\n\nStop the container (ctrl-C). Create a new folder and copy your dataset(s) of choice into it. Create a file service.atom.xml and for each dataset a new text file with the same name, but the atom extension.\nservice.atom.xml is the service feed (comparable to the capabilities operation in OWS). The service feed will contain details about the service and link to each of the dataset feeds. Populate the service feed with (replace relevant sections):\n<feed xmlns=\"http://www.w3.org/2005/Atom\"\n  xmlns:georss=\"http://www.georss.org/georss\" \n  xmlns:inspire_dls=\"http://inspire.ec.europa.eu/schemas/inspire_dls/1.0\" \n  xml:lang=\"en\">\n <!-- feed title -->\n <title>XYZ Example INSPIRE Download Service</title>\n <!-- feed subtitle -->\n <subtitle>INSPIRE Download Service of Soil Properties data in Sahel region</subtitle>\n <!-- self-referencing link to this feed -->\n <link href=\"http://localhost:8080/service.atom.xml\" rel=\"self\" type=\"application/atom+xml\"  hreflang=\"en\" title=\"This document\"/>\n <!-- link to Open Search definition file for this servicen (not implemented) \n<link rel=\"search\" href=\"https://example.org/search/opensearchdescription.xml\" type=\"application/opensearchdescription+xml\" title=\"Open Search Description for XYZ download service\"/> -->\n <!-- identifier -->\n <id>http://localhost:8080/service.atom.xml</id>\n <!-- rights, access restrictions -->\n <rights>Copyright (c) 2021, XYZ; all rights reserved</rights>\n <!-- date/time this feed was last updated -->\n <updated>2021-03-31T13:45:03Z</updated>\n <!-- author contact information -->\n <author><name>John Doe</name><email>doe@example.org</email></author>\n <category term=\"http://inspire.ec.europa.eu/metadata-codelist/SpatialDataServiceCategory/infoFeatureAccessService\" scheme=\"http://inspire.ec.europa.eu/metadata-codelist/SpatialDataServiceCategory\"/>\n <!-- entry for a \"Dataset Feed\" for a pre-defined dataset -->\n <entry>\n    <!-- title for \"Dataset Feed\" for pre-defined dataset -->\n    <title>soil properties ABC Dataset Feed</title>\n    <!-- Spatial Dataset Unique Resource Identifier for this dataset-->\n    <inspire_dls:spatial_dataset_identifier_code>wn_id1</inspire_dls:spatial_dataset_identifier_code> \n    <inspire_dls:spatial_dataset_identifier_namespace>https://example.org/</inspire_dls:spatial_dataset_identifier_namespace>\n    <!-- link to dataset metadata record -->\n    <link href=\"https://example.org/metadata/abcISO19139.xml\" rel=\"describedby\" type=\"application/xml\"/>\n    <!-- link to \"Dataset Feed\" for pre-defined dataset -->\n    <link rel=\"alternate\" href=\"http://localhost:8080/soilproperties.atom.xml\" type=\"application/atom+xml\"  hreflang=\"en\" title=\"Feed containing the soil properties data\"/>\n    <!-- identifier for \"Dataset Feed\" for pre-defined dataset -->\n    <id>http://localhost:8080/soilproperties.atom.xml</id>\n    <!-- rights, access info for pre-defined dataset -->\n    <rights>Copyright (c) 2002-2021, XYZ; all rights reserved</rights>\n    <!-- last date/time this entry was updated -->\n    <updated>2012-03-31T13:45:03Z</updated>\n    <!-- summary -->\n    <summary>This is the entry for soil properties ABC Dataset</summary>\n    <!-- optional GeoRSS-Simple polygon outlining the bounding box of the pre-defined dataset described by the entry. Must be lat lon -->\n    <georss:polygon>47.202 5.755 55.183 5.755 55.183 15.253 47.202 15.253 47.202 5.755</georss:polygon>\n    <!-- CRSs in which the pre-defined Dataset is available -->\n    <category term=\"http://www.opengis.net/def/crs/EPSG/0/4258\" label=\"ETRS89\"/>\n </entry>\n</feed>\nNotice that we’re not implementing the actual opensearch search functionality yet. You can leave the opensearchdescription line empty for now. There are some external options to provide the opensearch, the Technical Guidance document actually includes a PHP script to facilitate opensearch.\nThen for each dataset add a file soilproperties.atom.xml:\n<feed xmlns=\"http://www.w3.org/2005/Atom\"\n xmlns:georss=\"http://www.georss.org/georss\" xml:lang=\"en\">\n    <!-- feed title -->\n    <title>INSPIRE Dataset Soil properties Download</title>\n    <!-- feed subtitle -->\n    <subtitle>INSPIRE Download Service, of organisation XYZ providing dataset Soil Properties</subtitle>\n    <!-- links to INSPIRE Spatial Object Type definitions for this predefined dataset -->\n    <link href=\"https://inspire.ec.europa.eu/featureconcept/SoilProfile\" rel=\"describedby\" type=\"text/html\"/>\n    <!-- self-referencing link to this feed -->\n    <link href=\"http://localhost:8080/soilproperties.atom.xml\" rel=\"self\" \n    type=\"application/atom+xml\"\n    hreflang=\"en\" title=\"This document\"/>\n    <!-- upward link to the corresponding download service feed -->\n    <link href=\"http://localhost:8080/service.atom.xml\" rel=\"up\" type=\"application/atom+xml\" hreflang=\"en\" title=\"The parent service feed document\"/>\n    <!-- identifier -->\n    <id>http://localhost:8080/soilproperties.atom.xml</id>\n    <!-- rights, access restrictions -->\n    <rights>Copyright (c) 2021, XYZ; all rights reserved</rights>\n    <!-- date/time this feed was last updated -->\n    <updated>2021-03-31T13:45:03Z</updated>\n    <!-- author contact information -->\n    <author><name>John Doe</name><email>doe@xyz.org</email></author>\n    <!-- download the pre-defined dataset in GML format in CRS EPSG:25832 --> \n    <entry>\n        <title>soil properties in CRS EPSG:25832 (GML)</title>\n        <link rel=\"alternate\" href=\"http://localhost:8080/soilproperties_25832.gml\" type=\"application/gml+xml;version=3.2\" hreflang=\"en\" length=\"34987\" \n          title=\"soil properties dataset encoded as a GML 3.2 document in ETRS89 UTM zone 32N (http://www.opengis.net/def/crs/EPSG/0/25832)\"/>\n        <id>http://localhost:8080/soilproperties_25832.gml</id>\n        <updated>2021-06-15T11:12:34Z</updated>\n        <category term=\"http://www.opengis.net/def/crs/EPSG/0/25832\" label=\"ETRS89 / UTM zone 32N\"/>\n    </entry>\n    <!-- download the same pre-defined dataset in GML format in CRS EPSG:4258-->\n    <entry>\n        <title>soil properties in CRS EPSG:4258 (GML)</title>\n        <!--file download link-->\n        <link rel=\"alternate\" href=\"http://localhost:8080/soilproperties_WGS84.gml\" type=\"application/gml+xml;version=3.2\" hreflang=\"en\" length=\"37762\" \n          title=\"soil properties encoded as a GML 3.2 document in WGS84 geographic coordinates (http://www.opengis.net/def/crs/OGC/1.3/CRS84)\"/>\n        <id>http://localhost:8080/soilproperties_WGS84.gml</id>\n        <updated>2021-06-14T12:22:09Z</updated>\n        <category term=\"http://www.opengis.net/def/crs/EPSG/0/4258\" label=\"ETRS89\"/>\n    </entry>\n    <!-- download the same pre-defined dataset in ShapeFile format in CRS EPSG:25832, ShapeFile is in a single zip archive.-->\n    <entry>\n        <title>soil properties in CRS EPSG:25832 (ShapeFile)</title>\n        <link rel=\"alternate\" href=\"http://localhost:8080/soilproperties_25832.zip\" type=\"application/xshapefile\" hreflang=\"en\" length=\"89274\" \n        title=\"soil properties dataset encoded as a ShapeFile in ETRS89 UTM zone 32N (http://www.opengis.net/def/crs/EPSG/0/25832)\"/>\n        <id>http://localhost:8080/soilproperties_25832.zip</id>\n        <updated>2021-06-15T11:12:34Z</updated>\n        <category term=\"http://www.opengis.net/def/crs/EPSG/0/25832\" \n        label=\"ETRS89 / UTM zone 32N\"/>\n    </entry>\n</feed>\nNotice that you can provide multiple distributions for the same dataset (in various projections, translations or formats) to facilitate users.\nNotice that the examples above incorperate ongoing work as described in https://github.com/INSPIRE-MIF/gp-data-service-linking-simplification/issues/63.\nThe docker container runs locally, so it can not be tested by the INSPIRE Validator. In Local Tunnel an approach is suggested to temporarily host a local service online, so you can run the validation.\nNote that you have to update the self-references in atom files to use the tunneled web address. Then trigger the Atom validation:\n\n\n\ntrigger the atom test\n\n\n\n\n\nAn alternative Atom implementation exists in GeoNetwork. The approach is described at https://geonetwork-opensource.org/manuals/trunk/en/tutorials/inspire/download-atom.html and https://geonetwork-opensource.org/manuals/3.10.x/en/api/opensearch.html. GeoNetwork provides an internal and external mode, the external mode provides opensearch on a set of remote atom feeds. The internal mode generates the atom feeds from metadata records.\nHale Studio provides an option to generate a Atom feed while exporting a dataset to GML. The Hale Connect platform offers a Atom based service endpoint for every dataset published.\nThe QGIS INSPIRE Atom plugin provides access to Atom services through QGIS."
  },
  {
    "objectID": "cookbook/tarql.html",
    "href": "cookbook/tarql.html",
    "title": "Soil data guidance",
    "section": "",
    "text": "The basic idea behind tarql is to develop a CONSTRUCT SPARQL query that instead of executing against a triple store is executed against a CSV file. The result is a knowledge graph that may itself be deployed to a triple store or outright published on the web.\n\n\nStart by accessing the releases page and download a compressed file with the latest version. Then uncompress the file, it will create a new folder in your system with the release number appended, for instance tarql-1.2.\nThe sub-folder bin contains executables for both Linux and Windows. You may run the executable directly or install it for wider system use. On Linux it is common practice to copy the programme folder to /opt and then create a symbolic link in /usr/local/bin.\n$ unzip tarql-1.2.zip\n$ sudo mv tarql-1.2 /opt\n$ sudo ln -s /opt/tarql-1.2/bin/tarql /usr/local/bin/tarql\nFinally try invoking the executable to make sure it is functioning.\n$ tarql --help\nUse\n\nNote: before starting a data transformation into RDF you must devise a URI policy for your data. Please refer to the URI Policy document for details.\nThe file SoilData.csv contains a simple set of hypothetical measurements referring to three soil profiles collected in two different sites. The goal is to transform this dataset into GloSIS compliant RDF.\nsite_id,lat,lon,profile_id,layer_id,upper_depth,lower_depth,pH,SOC,\n1,49.43,8.31,1,11,0,15,7.4,6,\n1,49.43,8.31,1,12,15,40,7.2,4,\n1,49.43,8.31,2,21,0,10,8,3,\n1,49.43,8.31,2,22,10,30,8.1,2,\n2,46.82,11.45,3,31,0,15,6.8,1,\n2,46.82,11.45,3,32,15,30,6.7,1,\n2,46.82,11.45,3,33,30,60,6.7,0,\n\n\nThe Profile class is the most simple in GloSIS, making it a good place to start. It only requires the declaration of a new instance and its association with the respective site, in essence just two triples. To do so two URIs must be created, one for the profile and another for the site.\nThe Listing below shows a complete example creating URIs according to the policy used for the World Soil Information Service (WoSIS). The CONSTRUCT clause in the query is pretty vanilla, whereas in the WHERE clause the URIs are created with the BIND and URI functions. The important thing to note in this query is the use of column names in the CSV file as variables, ?site_id and ?profile_id. tarql matches every variable with the CSV columns, replacing them with the corresponding values.\nPREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\nPREFIX glosis_pr: <http://w3id.org/glosis/model/profile#>\nPREFIX iso28258: <http://w3id.org/glosis/model/iso28258/2013#>\n\nCONSTRUCT { \n    ?uri_profile rdf:type glosis_pr:GL_Profile ; \n               iso28258:Profile.profileSite ?uri_site .\n}\nWHERE {\n    BIND (URI(CONCAT('http://wosis.isric.org/site#', ?site_id)) AS ?uri_site)\n    BIND (URI(CONCAT('http://wosis.isric.org/profile#', ?profile_id)) AS ?uri_profile)\n}\nExercise: try running this example using the profile.sparql file in the tarql folder against the SoilData.csv file. You should issue a command like:\n$ tarql tarql/profile.sparql data/SoilData.csv\n\n\n\nBy default tarql generates a triple for each line in the CSV file. Most likely the data in the CSV is not normalised, and thus many duplicates result. You can observe this with the example for the Profile class above.\nThe tool provides a specific argument to deal with duplicates: --dedup. It suppresses all duplicate triples up to a given line in the output. In general you will want to use this argument with a large enough number to cover all the triples produced.\n$ tarql --dedup 1000 tarql/profile.sparql data/SoilData.csv\nIf your only intention is to load tarql’s output to a triple store, you might not need to worry about duplicate triples. Most likely the software automatically discards the duplicates on load.\n\n\n\nThe next example deals with the Site class, whose instances are spatial features. Therefore an additional instance of the GeoSPARQL Point class must be created, with the associated geometry literal. The BIND and URI are again at your service to create the URIs. The STRDT function is used to create the geometry instance with the appropriate type. The listing below is available in the file site.sparql.\nPREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\nPREFIX geo: <http://www.opengis.net/ont/geosparql#>\nPREFIX glosis_sp: <http://w3id.org/glosis/model/siteplot#>\n\nCONSTRUCT { \n    ?uri_site rdf:type glosis_sp:GL_Site ;\n              rdf:type geo:Feature ;\n              geo:hasGeometry ?uri_geo .\n\n    ?uri_geo rdf:type geo:Point ; \n             geo:asWKT ?geom .\n}\nWHERE {\n    BIND (URI(CONCAT(\"http://wosis.isric.org/site#\", ?site_id)) AS ?uri_site)\n    BIND (URI(CONCAT(\"http://wosis.isric.org/geometry#\", ?site_id)) AS ?uri_geo)\n    BIND (STRDT(CONCAT(\"POINT(\", $lon, \", \", $lat, \")\"), geo:wktLiteral) AS ?geom)\n}\nExercise: try modifying the query above so that it produces a GML literal instead of WKT. Which of the literals do you prefer?\n\n\n\nInstances for the layer should not be a challenge, after succeeding with sites and profiles. The list below provides and example, creating triples relating the layer to the respective profile and declaring its extent with ISO-28258 predicates.\nPREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\nPREFIX glosis_lh: <http://w3id.org/glosis/model/layerhorizon#>\nPREFIX iso28258: <http://w3id.org/glosis/model/iso28258/2013#>\n\nCONSTRUCT { \n    ?uri_layer rdf:type glosis_lh:GL_Layer ; \n        iso28258:ProfileElement.elementOfProfile ?uri_profile ;\n        iso28258:ProfileElement.upperDepth ?Top ;\n        iso28258:ProfileElement.lowerDepth ?Bottom .\n}\nWHERE {\n    BIND (URI(CONCAT('https://example.org/layer#', ?Layer)) AS ?uri_layer)\n    BIND (URI(CONCAT('https://example.org/profile#', ?Profile)) AS ?uri_profile)\n}\nExercise: save the query below in a file and use tarql to run it against the SoilData.csv file.\nAdvanced exercise: hop on to the WoSIS RDF pilot and verify how a GloSIS Observation instance is formed. Create a new query to generate the approapriate instances for the pH measurements in the SoilData.csv file. Don’t forget to create the associated GloSIS Result instance(s).\n\n\n\n\nNow that you obtained a RDF knowledge graph you can publish it to the internet. Follow the guide on Virtuoso to learn how.\nIn alternative to tarql, you may instead transform tabular data into RDF with RML.io, a transformation tool-set based on YAML configuration files."
  },
  {
    "objectID": "cookbook/uri.html",
    "href": "cookbook/uri.html",
    "title": "Soil data guidance",
    "section": "",
    "text": "The Unified Resource Identifier is a standard set by The Internet Society, that has been in development for more that 30 years. It is a unique sequence of characters that identifies a logical or physical resource used by web technologies.\n\n\n\nThere are two types of URIs that are important to identify: Unified Resource Names (URNs) and Unified Resource Locators (URLs).\n\n\n\nURNs are logical identifiers, simply providing a name for a resource. A URN does not indicate where to find or access the resource, only gives it a name. URNs start with the characters urn: to which follows a namespace and then a succession of other strings that identify sub-categories or paths down to the particular resource name.\nThe lex: namespace is used in some countries to identify legislation documents. As an example, a URN to a European Directive: urn:lex:eu:council:directive:2010-03-09;2010-19-UE\nURNs are used to identify books, media works, trade items, shipping vessels and many other resources. In the context of data provision over the internet that are less used, as they do not provide location. However, they can be useful to identify other resources somehow associated with the data in question.\n\n\nIn its turn, the URL not only identifies a resource, it also provides a location and the mechanism to interact with it. These are the most commonly used URIs in the Semantic Web and in digital data exchange in general.\nA URL is composed by three broad components:\n\nProtocol: identifies a specific data exchange mechanism, for instance Hyper Text Transfer Protocol (HTTP) or File Transfer Protocol (FTP). It composes the first segment of the URL and is followed by the characters ://. Examples: http://, file://.\nAuthority: an individual or institution responsible for the resource and usually the infrastructure that may provide it on the internet. E.g. isric.org.\nPath: the relative path to the resource within the institutional infrastructure. E.g. /locations/id/123.\n\nAlthough meant to locate resource, URLs may not always do so and be simply used as identifiers. They remain useful as identifiers as they express an authority over the resource. URLs that do not locate a resource are termed as “non dereferenceable”, meaning that when a computer programme attempts to use it no result is obtained.\n\n\n\n\nBefore making any data available on the internet, say to publish the result of a transformation into RDF, you must devise an appropriate URI structure. This is particularly important with RDF, as every non-literal element in a knowledge graph must correspond to a URI. But even outside the Semantic Web, URIs are very useful, as they provide unique identifiers on the World Wide Web (WWW) to your datasets, even to each individual datum.\nA simple approach is to construct your URIs with three building blocks:\n\nUse a sub-domain of your institutional domain to identify a single project or dataset. E.g. soil.my-institute.org.\nAdd a path that starts with a name or identifier of the class to which the data instance belongs. This can be a database table, or a OWL or UML class. E.g. /profile.\nComplete the path with a number or string that unequivocally identifies the data instance within the class. If you work with relational databases this may be the table primary key. An example: #prof1.\n\nThe complete template for this approach looks like:\nhttp://project.institution.org/class#identifier\n\n\nISRIC is currently working on a pilot service providing data from the World Soil Information Service (WoSIS) as RDF compliant with the GloSIS Web Ontology. You may browse these data at virtuoso.isric.org.\nIn this example the service is identified in the authority segment of the the URI (wosis.isric.org). Then short identifiers for GloSIS classes make the prefix of the URI path, (site, profile, layer, etc). The path suffix is composed by an hash followed by the WoSIS primary key.\nSome examples currently used with WoSIS data:\n\nhttp://wosis.isric.org/site#72007\nhttp://wosis.isric.org/profile#72007\nhttp://wosis.isric.org/layer#64448\nhttp://wosis.isric.org/observation#4357805\nhttp://wosis.isric.org/result#4357805\n\n\n\n\n\n\nDoes your institution already has a URI policy?\nIs it applied to data provision?\nIf not, can you elaborate a proposal?\n\n\n\n\n\nBest Practices URI Construction\nURIs, URLs, and URNs: Clarifications and Recommendations 1.0\nOn Linking Alternative Representations To Enable Discovery And Publishing"
  },
  {
    "objectID": "cookbook/link-checker.html",
    "href": "cookbook/link-checker.html",
    "title": "Soil data guidance",
    "section": "",
    "text": "INSPIRE Link Checker\nStatus: in progress\nA tool to verify links in metadata\n\nWebsite: https://inspire-geoportal.ec.europa.eu/linkagechecker.html"
  },
  {
    "objectID": "cookbook/virtuoso.html",
    "href": "cookbook/virtuoso.html",
    "title": "Soil data guidance",
    "section": "",
    "text": "Status: in progress\nThis recipe presents steps to publish a code-lists using the SKOS ontology in Virtuoso and Skosmos. Virtuoso is an open source Triple store providing a SPARQL endpoint. Skosmos is an open source web application providing a human friendly browse interface for skos thesauri stored in a triple store.\nThis is a follow up recipe of the extending code-lists recipe.\nIn this recipe we’re going to reproduce the publication of the GLOSIS - Procedures Codelist, maintained at github, initiated by the former SieuSoil project with contributions from the EJP Soil project.\n\n\nWe’re using a docker compose orchestration to deploy virtuoso and skosmos locally. Copy the contents of the virtuoso folder into an empty folder. Navigate to the folder with command line and run:\ndocker compose up\n(ctrl-c to stop the containers)\n\nOpen http://localhost:8890/conductor and login using user: dba, password: dba (the password is configured as part of the docker compose)\n\n\n\n\nLog in to virtuoso\n\n\n\nOn the linked data tab, select Quad store upload.\nSelect Resource URL and paste the url of the ttl file from github (or upload a local file)\nSelect create graph and enter as graph uri http://w3id.org/glosis/model/procedure#, click upload.\n\n\n\n\nUpload Quad store\n\n\n\nNavigate to http://localhost:8890/sparql/ and run a SPARQL query like the one below, the glosis concepts should be returned.\n\nSELECT DISTINCT ?Concept \nFROM <http://w3id.org/glosis/model/procedure#>\nWHERE {[] a ?Concept} \nLIMIT 20\n\n\n\nThe file config-docker.ttl contains the configuration of SKOSMOS. The file is pre-configured for the procedures codelist, but you have to update it if you want to include alternative code lists.\nAfter an update of the config file, you need to restart the docker compose.\n\nOpen http://localhost:8080 and evaluate if the codelist is properly loaded.\n\nIn case of errors, logging of skosmos occurs in the browser log panel, click anywhere in the page and select inspect from the context menu. Then open the console tab and refresh the page.\nIn case you are insterested to update the look and feel of the skosmos instance, notice the skosmos:customCss property on the config file. This property can link to a css file having custom css. Include the css file via a volume mount in the docker compose.\n\n\n\nVirtuoso\n\nWebsite: virtuoso\nGitHub: github\nDocker: docker\nVirtuoso at ISRIC: isric\n\nSkosmos\n\nWebsite: skosmos\nGitHub: github\nDocker: docker\nSkosmos examples: glosis, agrovoc, agclass"
  },
  {
    "objectID": "cookbook/jmeter.html",
    "href": "cookbook/jmeter.html",
    "title": "Soil data guidance",
    "section": "",
    "text": "status: in progress\njmeter is a utility which can run a series of performance and capacity tests on a webservice.\n\n\nJmeter is a java program, which can run on most platforms (if java is installed). Download the latest version from the apache website. Unzip the archive and run jmeter.bat from bin directory.\nJmeter may be quite overwhelming at first, the number of options is high. Follow the tutorials to get introduced to the basic aspects.\n\n\n\nJmeter is typically used to test the performance and capacity of a website or webservice.\n\nStart Jmeter and follow the build web test plan tutorial.\nCreate a web testing plan in jmeter and add a list of sample requests (WMS/WFS/WCS, getcapabilities/getmap/getfeatere etc).\n\n\n\n\nJMeter testplan\n\n\n\nRun the test against the webservice\nRun the test with multiple users, notice the performance decrease of the service.\n\n!!!note\nDo not perform a load test against a production url, it wil severely impact the performance of that service. \n\n\n\n\nhttp://localhost:8080/geoserver/wms?request=getcapabilities\nhttp://localhost:8080/geoserver/ows?request=getcapabilities&version=2.0.0&service=wfs\nhttp://localhost:8080/geoserver/wcs?request=GetCapabilities&version=1.1.1&service=wcs\nhttp://localhost:8080/geoserver/gwc/service/wmts?REQUEST=getcapabilities\nhttp://localhost:8080/geonetwork/srv/eng/csw?REQUEST=GetCapabilities&version=2.0.1&service=CSW\n\n\n\n\n\nWebsite\nGetting started with a web test plan\nDocker\nJmeter test module included in GeoNetwork"
  },
  {
    "objectID": "cookbook/data/bhr-p/readme.html",
    "href": "cookbook/data/bhr-p/readme.html",
    "title": "Soil data guidance",
    "section": "",
    "text": "This is an extract of BRO - Bodemkundig booronderzoek (BHR-P), the Dutch National Database of Soil Profile analyses data (Licensed: CC-0).\nBulk density and organic matter are relevant indicators of Carbon Stock.\nDownloaded at 17-01-2023.\n\nBulk density has been extracted via:\n\nSELECT borehole_research_pk, bro_id, research_report_date, litter_layer_investigated, standardized_location, begin_depth, end_depth, analysis_report_date,  dry_bulk_density_determination.*\nfrom borehole_research, dry_bulk_density_determination, investigated_interval, borehole_sample_analysis\nwhere investigated_interval_fk=investigated_interval_pk\nand borehole_sample_analysis_pk=borehole_sample_analysis_fk\nand borehole_research_pk = borehole_research_fk \nand dry_bulk_density is not null;\n\nOrganic matter has been extracted via:\n\nSELECT borehole_research_pk, bro_id, research_report_date, litter_layer_investigated, standardized_location, begin_depth, end_depth, analysis_report_date,  organic_matter_content_determination.*\nfrom borehole_research, organic_matter_content_determination, investigated_interval, borehole_sample_analysis\nwhere investigated_interval_fk=investigated_interval_pk\nand borehole_sample_analysis_pk=borehole_sample_analysis_fk\nand borehole_research_pk = borehole_research_fk \nand organic_matter_content is not null;\n\n\n\n\n\n\n\n\n\nAttribute\nDescription\n\n\n\n\nborehole_research_pk\nidentifier of the research\n\n\nbro_id\nalternative identifier of the research\n\n\nresearch_report_date\nDate of the fieldwork\n\n\nlitter_layer_investigated\nIndication if the litter layer was considered\n\n\ngeometry\nlLocation of the research\n\n\nbegin_depth\nTop of the layer\n\n\nend_depth\nBottom of the layer\n\n\nanalysis_report_date\nDate of the lab analyses\n\n\ndry_bulk_density_determination_pk\nIdentifier of the analyses\n\n\ninvestigated_interval_fk\nIdentifier of the layer\n\n\ndetermination_procedure\nIdentifer of the procedure\n\n\ndetermination_method\nIdentifier of the method\n\n\nring_diameter\nParameter of the specimen\n\n\nring_height\nParameter of the specimen\n\n\ndrying_temperature\nParameter of the analyses method\n\n\nvolume_water_saturated\nProperty of the sample\n\n\ndry_bulk_density\nObserved value\n\n\nmaterial_irregularity\nProperty of the sample"
  },
  {
    "objectID": "utils/python.html",
    "href": "utils/python.html",
    "title": "Soil data guidance",
    "section": "",
    "text": "A programming language commonly used in data science. From scripts to increase the capabilities or automate processes in ArcMAP or QGIS, maintanance and deployment tasks in DevOps, up to full programs such as GeoNode and pygeoapi.\n\n\nPIP is the package manager of python. Every script or program installs a series of packages from pip as part of its installation.\n\n\n\nTo prevent collision of dependencies between projects the use of virtual environment is common. Each virtual environment has its own python executable and libraries. Popular virtual environments are virtualenv, Conda, poetry. If you’re new to python I would suggest to start with Conda.\nNote that some prefer to work even more isolated and use docker to set up a development container including the required libraries.\n\n\n\nJupyter notebooks combine text and executable (python) code into a single document. Optimal for training, documentation and reporting about research.\n\n\n\nThere is a significat community dedicated to the use of python in the geospatial domain. Interesting libraries used in that community are:\n\nRasterIO\nShapely\nFiona\nOWSLib\nGeoPandas\nPyGRASS\npygeometa\npyproj, etc."
  },
  {
    "objectID": "utils/index.html",
    "href": "utils/index.html",
    "title": "Utils",
    "section": "",
    "text": "This page lists a set of usefull utilities.\n\nDocker - a virtualisation technology\nGdal/OGR - a format conversion utility for spatial files\nGit - a versioning system\nJupyter notebooks - Interactive notebooks\nLocal tunnel - Set up a local tunnel to the web\nPython - A common programming language in data science\nR - A language for statistics\nVisual Studio Code - A text editor"
  },
  {
    "objectID": "utils/git.html",
    "href": "utils/git.html",
    "title": "Overview Git",
    "section": "",
    "text": "Git is a distributed version management system of mainly text files to facilitate shared development of software and/or documentation. Git is one of the creations of Linus Torvalds. In Git everybody checks out the full repository with all its history, you make changes to a version locally and push them back to the server. In the process incidental conflicts may occur, if someone else has made a change in the same version and line as you. The server will reject the push until you resolved the conflict.\nGit is mainly operated from the command line, but a lot of client software is available to facilitate participation. An example is SmartGit, which provides an easy to use interface to manage even complex git tasks such as fixing conflicts. Git GUI is the graphical user interface included with the Git software suite.\nGitHub.com is a well known provider of Git services. They offer a lot of additional functionality on top of the Git version management, such as a web interface, issue management, forking & pull requests, wiki, actions, etc.\nGitlab is a popular open source software stack offering a similar set of functionalities as GitHub, you can use it as SAAS or install it on a server.\nIn 2020 various code forges based in the United States arbitrary blocked access to users from countries deemed hostile. This action affected foreign students and scientists working in Europe or collaborating with European institutions. That same year a community of European software developers registered a non-for-profit in Germany to support an independent code forge hosted in Europe. The result is Codeberg, a free, open source, Git-based code forge accessible to everyone in the world.\nAn interesting quick start to Git is written by Roger Dudler (multiple translations). Or if you prefer a video.\nThe main reference for this software is the Git Book.\n\n\n\nA software project with contributors from around the globe. Including issue management, software releases and automated validation of unit and integration tests.\nThis wiki is maintained in a Git repository. With every new push a new version is build and it replaces the previous version.\nA roll out of an improved composition of Docker containers on a cloud platform like Kubernetes. The helm charts (configuration files) are stored on a Git repository, every change to the helm chart results in an update of the development environment to reflect the latest changes.\nGit is an important tool for Dev Ops staff. Manual interventions are minimized. Every action is scripted, stored in a Git repository and started from a Git action. This improves tracebility and reproducability in software maintenance.\n\n\n\n\n\nInstall git (or smartgit, which has git included) on your machine\nClone the repository of this wiki from the command line:\n\ngit clone https://github.com/ejpsoil/soildata-assimilation-guidance.git\n\nopen the folder and log all events that happened on the repository\n\ncd soildata-assimilation-guidance\ngit log --oneline\n\n\n\nGitHub added an extra interactivity on top of Git, the option to Fork a repository to a personal space. And from the personal space provide an option to propose a change to the upstream repository, this is called a pull request. Now users were able to propose changes to repositories of which they were not even a member yet. The fork and pull mechanism has now been adopted by other git platforms, such as Gitlab, but because it is not part of the Git specification itself, it may work slightly different on other platforms. Let’s try and improve this wiki via a pull request:\n\nIf you do not have a github account yet, we invite you to set up one now at https://github.com/signup.\nWhen logged in, open https://github.com/ejpsoil/soildata-assimilation-guidance, and click the fork button in the top right. This will clone the repo to your personal space.\nOn https://github.com/YOU/soildata-assimilation-guidance, edit a file (maybe you found a typo somewhere, or would like to comment/extend something) via the github web interface by opening the file and clicking on the pencil button.\nBelow the text you notice a commit message and button. Every commit in Git requires a usefull message, so others understand what you did. Select the option “Create a new branch for this commit …”.\nThen create the branch, but not the pull request in the next step, because this pull request would arrive in your own repository\nNow navigate back to https://github.com/ejpsoil/soildata-assimilation-guidance. A banner shows indicating you can compare and pull request your recent change. Click this button and create the pull request.\n\nThe process of a pull request is quite overwhelming. But it is an important aspect in collaborative development these days. Many repositories only allow code changes via pull requests, because it is a guarantee that at least 2 persons have looked at it."
  },
  {
    "objectID": "utils/docker.html",
    "href": "utils/docker.html",
    "title": "Overview Docker",
    "section": "",
    "text": "Docker is a virtualisation technology, slightly more efficient then running a virtual machine. With docker you run a full virtual environment (container) within your PC. Most containers run a flavour of Linux and you access them as if you access a remote server. Containers are based on a docker-image, a prepared set of operating system and software. Docker images are build locally from a Dockerfile (recipe) or downloaded from a repository such as dockerhub. Learn more about docker in the Docker Overview.\nIn this recipe we run most examples using Docker, because in this way there is no need to install any software on your computer, which may either not be allowed, give errors due to missing dependencies or in a worse case scenario can corrupt an existing configuration.\nOn Windows and Apple we recommend to install Docker Desktop. Docker Desktop provides an additional panel to manage running containers. On Linux you can install docker engine. Start docker engine from the start menu, if it is not already running. A general check can be triggered by typing docker ps in a console, this will provide a notification of docker availability and running containers.\nIn this recipe we will use the following Docker concepts:\n\nDocker container; an image deployed as a virtual environment. Most containers have an assigned port, so you can access the service of the container via the browser (for example, http://localhost:5000). You can also interact with containers from your commandline (docker ps, docker logs xxx, docker run geopython/pygeoapi)\nDocker volume; a folder on the host system which you mount as a folder into the container. Containers are destroyed when stopped, any file stored in the container file system is lost. By mounting a host folder into the container, you can persist items between runs of the container.\nDocker compose; with a compose file you orchestrate a cluster of containers into a functional system. One container runs a database, another container runs a webserver and the third container runs the web application. The compose file arranges that the containers can connect, on which port they run and which volumes they load.\nDocker build is the command to build a docker image from a Dockerfile\nWith Docker pull you can pull an image from a repository\nDocker run starts a container, press ctrl-c to stop is again (unless you run it with -d (detach) option then stop a container with docker stop <id>)\n\nInstead of running containers for permanent server applications, you can also start a container to run a short process, similar to running a command line utility. The container will stop when the process is finished.\n\n\n\nInstall Docker Desktop, verify it is running, else start it from the start menu.\nOn commandline run this command\ndocker run -p 80:80 --name test uzyexe/tetris:latest\nOpen browser at http://localhost\nOpen a shell within the container\n\ndocker exec -it test /bin/bash\n\ntype exit to return to your pc\nctrl-c to stop the container\n\n(if other processes are running on port 80, Docker will throw an error, select another port, e.g. -p 81:80, and open http://localhost:81)\n\n\n\nThe virtual environments take quite some memory and CPU, you will notice this on older computers. Make sure to stop all containers after you finish using them. You can easily start them again later. docker ps (or the docker desktop window) indicate which containers are still running.\nAfter your experiments you will notice the size of your harddisk has considerably been reduced. Images, containers, volumes all use quite some space. There is a single command to clean up everything\ndocker system prune -a\nOnly use it if you are sure you don’t want to keep any docker resources."
  },
  {
    "objectID": "utils/r.html",
    "href": "utils/r.html",
    "title": "Soil data guidance",
    "section": "",
    "text": "r is a statistics utility commonly used in the soil statistics. R is typically extended with a series of libraries, like GDAL and grassgis. Due to the amount of plugins required, some with a difficult installation procedure and potential high use of resources, the use of virtualisation, such as Docker, is common in soil data statistics.\n\n\nR is used to generate SoilGrids from the global Soil Profile database WoSIS.\nAt intervals ISRIC organises a spring school on soil mapping using R in case you’re interested in this topic.\n\n\n\nSince many soil observation data is stored in relational databases, but processing in R requires a flat data view, it is relevant to have reproducable conventions to access sql data in R. David Rositter described an approach to load soil profile data into R from sqlite in 2017. For PostGreSQL a similar approach should be possible using RPostres.\n\n\n\n\nWebsite\nDocker"
  },
  {
    "objectID": "utils/gdal.html",
    "href": "utils/gdal.html",
    "title": "Soil data guidance",
    "section": "",
    "text": "OGR/GDAL is a swiss army knife for spatial data. It can read u multitude of grid and vector formats and interact with OGC services. The tool includes 2 clusters of scripts:\n\nGDAL utilities interact with grids\nOGR utilities interact with vector formats\n\n\n\n\nOn Windows, most easy install is using Conda. Other option is via ms4w. Note that QGIS (C:Files3.xx.exe) and ArcMap are also packaged with GDAL.\nFor Apple, use brew install gdal\nOn Ubuntu, run add-apt-repository ppa:ubuntugis/ppa && apt update && apt install gdal-bin\nOn Debian, use Conda\nFor Docker, use this container\n\n\n\n\n\nGet details of a gridded dataset\n\ngdalinfo https://files.isric.org/soilgrids/latest/data/bdod/bdod_0-5cm_mean.vrt\n\nGet details of a geojson file\n\nogrinfo https://github.com/gregoiredavid/france-geojson/raw/master/cantons-avec-outre-mer.geojson\n\nConvert geojson to shapefile\n\nogr2ogr cantons.shp https://github.com/gregoiredavid/france-geojson/raw/master/cantons-avec-outre-mer.geojson\n\n\n\nIn 2017 a large work on GDAL introduced support for GML Application Schema. GDAL will read the schema of the xml and generate a relational database schema required to store the data. Verify that the build of GDAL includes GMLAS, it needs a special library XERCES\n\nConvert a gml file to geopackage\n\nogr2ogr -f GPKG test.gpkg GMLAS:/path/to/the.gml\n\nUse DBeaver or similar to evaluate the contents of the GeoPackage\n\n\n\n\nGDAL includes python bindings, but many prefer the Fiona and Rasterio library, which do the same in a more pythonic way.\n\n\n\nRGDAL and sf provide bindings to GDAL within R.\n\n\n\n\nWebsite\nDocker\nGMLAS\nPresentation on FOSS4G Europe 2017"
  },
  {
    "objectID": "utils/jupyter.html",
    "href": "utils/jupyter.html",
    "title": "Jupyter Notebooks",
    "section": "",
    "text": "Status: in progress\nJupyter notebooks are a combination of text and code (JUlia, PYThon and R), the code can directly be run within the notebook. Jupyter notebooks are mainly used in training or documentation settings in data science. Some argue that jupyter notebooks will replace articles in scientific magazines to communicate research results.\nSome examples of Jupyter notebooks of interest:\n\nhttps://inspire.rasdaman.org/apps/jupyter-notebook/index.html\nhttps://github.com/geopython/geopython-workshop\n\nA Quick Start on using Jupyter is written by Antonino Ingargiola.\n\n\n\nWebsite\nDocumentation\nWikipedia"
  },
  {
    "objectID": "utils/localtunnel.html",
    "href": "utils/localtunnel.html",
    "title": "Overview Local Tunnel",
    "section": "",
    "text": "This recipe describes an approach to temporarily host a local webservice as an online service. A utility opens a tunnel to a service provicer, the service provider routes all traffic for a specific domain via the tunnel to your machine. The tunnel stops if you quit the utility (ctrl-c).\nThis technology is for example relevant if you want to test a local service with the INSPIRE validator (which requires a service to be online).\nVarious (free) service providers exist offering this service:\n\nlocaltunnel requires nodejs\nngrok web based, but requires registration\nlocalhost.run requires SSH to be installed\n\n\n\n\nVerify a docker image is running, for example:\ndocker run -p 80:80 -d uzyexe/tetris:latest\nEnter this command:\nssh -R 80:localhost:80 nokey@localhost.run\nThe utility will display a url on which the service will be available, try this url in your browser (and phone, to make sure it works also outside your computer)"
  },
  {
    "objectID": "utils/vscode.html",
    "href": "utils/vscode.html",
    "title": "Overview Text Editors",
    "section": "",
    "text": "Overview Text Editors\nWhen working with a variety of text files (HTML, CSV, YAML, XML, GeoJSON, JS, Python, Markdown), you need a replacement for the basic Notepad. A magnitude of options exists, such as Vim, Notepad++, PyCharm, Eclipse, Sublime. The last years my personal favourite has become Visual Studio Code. But let’s try to stay neutral and list what features we expect from a daily used text editor:\n\nFind in files (and replace) is an important feature when looking for a certain pattern in a folder of files.\nSyntax highlighting for xml, json, python and yml facilitate readability of the file\nCode formatting/validation. In Python and YAML indenting is essential, that’s when code formatting is extra important.\nA tree view of the project structure, so you can easily open files from the project\nCode completion/suggestions when you start typing\n\nOptional features:\n\nPreview HTML & Markdown\nXML schema validation\nGit operations\nContent comparison, compare 2 (or more) files.\n\nMany of the text editor communities have a range of plugins available to extend the functionality of the editor. Various text editors have for example a MapServer Mapfile or SLD plugin."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Soil Data Assiilation Guidance",
    "section": "",
    "text": "This WIKI is a collaborative effort to collect and describe hands-on good practices on data assimilation in the Soil domain, with a focus on Europe. The INSPIRE directive has been and is an important effort for standardisation in the environmental data domain, therefore this WIKI has a lot of links to INSPIRE sources. Because INSPIRE adopts industry standards, this WIKI does reference common standards from ISO, Open Geospatial Consortium, Global Soils Partnership, IANA and W3C, giving it a global relevance.\nThe term data assimilation has been chosen by the autors as an alternative to the terms harmonisation and standardisation, which already have a specific meaning in the soil domain:\n\nstandardisation; aligning soil data to a common model, using common codelists.\nharmonisation; transforming results from observations and measurements to values as if all results for a property are measured using the same procedure, by applying so called Pedotransfer Functions (PTF).\n\nThe wiki has been initiated by ISRIC - World Soil Information and Wageningen University as part of 2 workshops for Soil Data Providers, held in 2022 and 2023 in the scope of the EJPSoil project."
  },
  {
    "objectID": "license.html",
    "href": "license.html",
    "title": "License",
    "section": "",
    "text": "CC-BY 4.0"
  },
  {
    "objectID": "QOS.html",
    "href": "QOS.html",
    "title": "Overview Quality of Service",
    "section": "",
    "text": "Via the quality of service conventions data providers report about the quality of their services. Aspects which are monitored are:\n\nAvailability (% of the time that the service has been available)\nPerformance and capacity (a period in which performance and capacity requirements are not met, is considered downtime)\nUsage (how much is the service used)\n\nIn each of the technical guidelines on view, download and discovery services a chapter is dedicated to Quality of Service. It includes the aspects of:\nReporting about usage of the service is documented in the monitoring guidelines paragraph 1.7. As a data provider you may be requested to provide this data to the national contact point. The national contact point reports these numbers to Europe. Aside the reporting obligations, usage reports are interesting feedback to guide future developments.\nMeasuring and reporting about Quality of Service is an aspect of step 7) Soil Information User Consideration in the soil information workflow. Consult your IT department or hosting company if they have tools available to assess these aspects. Confirm with them on how to extend and/or share with you these measurements for the requested parameters.\n\n\nTo assess the availability of a service, it requires to monitor the availability of the service at intervals. A basic availability-test every 5 minutes is sufficient. Many software exists for availability monitoring, such as Zabbix, Nagios, CheckMK, pingdom. A special mention for the Python based GeoHealthCheck package, which includes the capability on WMS/WFS services to drill down to the data level starting from the GetCapabilities operation.\n\n\n\nCookbook\nSoftware\nDescription\n\n\n\n\nGeoHealthCheck\nGeoHealthCheck\nA utility to report on availability of a service\n\n\n\n\n\n\nTo know the capacity and performance of a service you can perform some load tests prior to moving to production. An alternative approach to evaluate performance is to extract the access logs of the service into an aggregation tool like Kibana and evaluate the number of requests exceeding the limits.\n\n\n\nCookbook\nSoftware\nDescription\n\n\n\n\nCapacity tests with jmeter\njmeter\nUtility for performance and capacity testing\n\n\n\n!!!note\nA common challenge to service performance is the provision of a WMS service on a big vector dataset. When requesting that dataset on a national level, the server runs into problems drawing all the features at once. In such case consider to set up some cache/aggregation mechanism for larger areas. Setting proper min/max scale denominators may be a solution also.\n\n\n\nTo capture the usage of a service you can extract the usage logs and import them in a tool like Kibana, Splunk or AW stats. Defining rules to extract the requested layer name from a WMS request is useful. Mind that not all requests are GET requests, some WFS requests use POST, which may need some configuration on the webserver to enable POST parameter logging. Make sure the logging includes the ‘Referer’ and ‘User-agent’ parameters, which allows to differentiate types of uses. Finally consider there is a GDPR privacy aspect to collecting access logs. Consider to exclude the IP address of the user and define a maximal retention for access logs.\n\n\n\nCookbook\nSoftware\nDescription\n\n\n\n\nAWStats\nAWStat\nA utility to report on service usage"
  },
  {
    "objectID": "glossary.html",
    "href": "glossary.html",
    "title": "Glossary",
    "section": "",
    "text": "Glossary\nThis page includes terms that we use in our wiki, so that you have a reference for how we’re using them.\nApplication programming interface (API) is a way for two or more computer programs to communicate with each other (source wikipedia)\nAssimilation is a catchy term indicating the processes involved to combine multiple datasets with different origin into a common dataset, the term is somewhat similarly used in psychology as incorporation of new concepts into existing schemes (source wikipedia). But is not well aligned with its usage in the data science community: updating a numerical model with observed data (source wikipedia)\nCatalogue or metadata registry is a central location in an organization where metadata definitions are stored and maintained (source wikipedia)\nATOM is a standardised interface to exchange news feeds over the internet. It has been adopted by INSPIRE as a basic alternative to download services via WFS or WCS.\nConceptual model or domain model represents concepts (entities) and relationships between them (source wikipedia)\nContent negotiation refers to mechanisms that make it possible to serve different representations of a resource at the same URI (source wikipedia)\nDigital exchange of soil-related data (ISO 28258:2013) presents a conceptual model of a common understanding of what soil profile data are\nDiscovery service is a concept from INSPRE indicating a service type which enables discovery of resources (search and find). Typically implemented as CSW.\nDownload service is a concept from INSPRE indicating a service type which enables download of a (subset of a) dataset. Typically implemented as WFS, WCS, SOS or Atom.\nEncoding is the format used to serialise a resource to a file, common encodings are xml, json, turtle\nGeography Markup Language (GML) is an xml based standardised encoding for spatial data.\nGlobal Soil Information System (GLOSIS) is an activity of FAO Global Soil Partnership enabling a federation of soil information systems and interoperable data sets\nGLOSIS domain model is an abstract, architectural component that defines how data are organised; it embodies a common understanding of what soil profile data are.\nGLOSIS Web Ontology is an implementation of the GLOSIS domain model using semantic technology\nGLOSIS Codelists is a series of codelists supporting the GLOSIS web ontology. Including the codelists as published in the FAO Guidelines for Soil Description (v2007), soil properties as collected by FAO GfSD and procedures as initally collected by Johan Leenaars.\nHarmonization is the process of transforming 2 datasets to a common model, a common projection, usage of common domain values and align their geometries\nObservations and Measurements (O&M)\nOGC API\nOntology\nPublication\nRelational model\nRDF\nREST\nRegistry\nRepository\nSensorthingsAPI\nSOS\nUML model\nView service is a concept from INSPRE indicating a service type which presents a (pre)view of a dataset. Typically implemented as WMS or WMTS.\nWebservice\nWMS\nWFS\nWCS\nWMTS\nXSD"
  }
]