{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>This WIKI is a collaborative effort to collect and describe hands-on good practices on data assimilation and dissemination in the Soil domain, with a focus on Europe. The INSPIRE directive has been and is an important effort for standardisation in the environmental data domain, therefore this WIKI has a lot of links to INSPIRE sources. Because INSPIRE adopts industry standards, this WIKI does reference common standards from ISO, Open Geospatial Consortium, Global Soils Partnership, IANA and W3C, giving it a global relevance.</p> <p>The term <code>data assimilation</code> has been chosen by the autors as an alternative to the terms harmonisation and standardisation, which already have a specific meaning in the soil domain:</p> <ul> <li><code>standardisation</code>; aligning soil data to a common model, using common codelists.</li> <li><code>harmonisation</code>; transforming results from observations and measurements to values as if all results for a property are measured using the same procedure, by applying so called Pedotransfer Functions (PTF).</li> </ul> <p>The process of assimilation also aims to capture additional aspects, such as finding, downloading and using the data. Most of these aspects are also well described as part of the FAIR principles.</p> <ul> <li>Findable, provide relevant metadata via online portals</li> <li>Accessible, make the data available as download or API</li> <li>Interoperable, adopt common data models and code lists</li> <li>Reusable, add documentation and provide tools to reuse the data</li> </ul>"},{"location":"#organisation-of-the-articles","title":"Organisation of the articles","text":"<p>Toner et al, 2022 identified 6 steps in a typical soil information workflow from a data producer perspective and a separate category for the data user perspective. These steps form a relevant categorisation of the articles in this wiki. Much of the articles apply to the categories:</p> <ul> <li>4) Data organisation </li> <li>6) data and info sharing </li> <li>7) Soil Information User Consideration </li> </ul> <p>We've labeled each of the articles as to which step in the workflow they apply.</p> <p></p> <p>This wiki lists a series of options for publishing data according to the Technical Guidelines and/or the Good Practices dedicated to use cases from the Soil domain. For each option recipes on various technologies are provided. The practices are categorized at 3 levels:</p> <ul> <li>Minimal, based on a minimal effort</li> <li>Traditional, following initial technical guidelines</li> <li>Experimental, following recent and upcoming good practices</li> </ul> <p>The practices cover 7 topics.</p> <ul> <li>Identification and namespaces</li> <li>Data harmonization</li> <li>Code lists</li> <li>Metadata and Discovery</li> <li>View services</li> <li>Download services</li> <li>Quality of service</li> </ul> <p>Info</p> <p>Disclaimer: References to products and approaches are examples. We do not aim to provide a complete listing, nor endorse a specific technology or service provider. Please consult any alternative software provider to what extent INSPIRE is supported in their products. In that scenario consider to contribute your experiences to this WIKI. </p>"},{"location":"#reading-guide","title":"Reading guide","text":"<p>While reading the vast amount of recipes in this cookbook you may realize; if there are so many options, how are the differences between the implementations bridged. - Some implementation options differ in technology, but generate a similar result - Some options actually need some bridging before data can be combined, but the number of bridges is limited (and could also be bridged by intermediaries) - All options share some basic principles, such as identifier persistence and adoption of common codelists, which make any option beneficial</p> <p>When selecting one of the available options, consider the following aspects:</p> <ul> <li>The minimal implementation will have limitations for end users (for example having to download the full dataset, if they are only interested in a small section). On the other hand, minimal implementations tend to be less complex in setup which makes understanding the implementation easier.</li> <li>The traditional implementations have the most active users, so dedicated documentation and tooling is available with high Technical Readiness Level (TRL). However, some technologies are based on conventions of almost 20 years ago. These conventions are outdated, compared to current IT practices.</li> <li>An experimental approach brings the risk of incomplete documentation and tools. Also there is less evidence on usability. But it does give opportunity to use current technologies and engage with the community to design the next iteration of data sharing.</li> </ul> <p>Before selecting an option evaluate the following aspects in your organization.</p> <ul> <li>What are current IT tools and conventions used in the organization to understand which of the approaches fits best with the current knowledge and experience</li> <li>Combine an implementation of INSPIRE with business cases that generate direct benefit for your organization or partners. For example, adoption of the open data directive, better documentation and reporting of service levels, improved archival of data, discoverability on search engines.</li> <li>Assess the projected audience. Verify that the complexity and nature of the implementation matches with the expectations and capabilities of that audience.</li> </ul> <p>We recommend to start with a minimal implementation and validate it with the provided compliance test tooling. From there, extend the implementation while continuing the tests with each iteration. With such an you are able to focus on the important aspects and prevent caveats early on in the process.</p>"},{"location":"QOS/","title":"Overview Quality of Service","text":"<p>Via the quality of service conventions data providers report about the quality of their services. Aspects which are monitored are:</p> <ul> <li>Availability (% of the time that the service has been available)</li> <li>Performance and capacity (a period in which performance and capacity requirements are not met, is considered downtime)</li> <li>Usage (how much is the service used)</li> </ul> <p>In each of the technical guidelines on view, download and discovery services a chapter is dedicated to Quality of Service. It includes the aspects of:</p> <p>Reporting about usage of the service is documented in the monitoring guidelines paragraph 1.7.  As a data provider you may be requested to provide this data to the national contact point. The national contact point reports these numbers to Europe. Aside the reporting obligations, usage reports are interesting feedback to guide future developments.</p> <p>Measuring and reporting about Quality of Service is an aspect of step <code>7) Soil Information User Consideration</code> in the soil information workflow. Consult your IT department or hosting company if they have tools available to assess these aspects. Confirm with them on how to extend and/or share with you these measurements for the requested parameters.</p>"},{"location":"QOS/#availability-monitoring","title":"Availability monitoring","text":"<p>To assess the availability of a service, it requires to monitor the availability of the service at intervals. A basic availability-test every 5 minutes is sufficient. Many software exists for availability monitoring, such as Zabbix, Nagios, CheckMK, pingdom. A special mention for the Python based GeoHealthCheck package, which includes the capability on WMS/WFS services to drill down to the data level starting from the GetCapabilities operation.</p> Cookbook Software Description GeoHealthCheck GeoHealthCheck A utility to report on availability of a service"},{"location":"QOS/#performance-capacity-testing","title":"Performance &amp; Capacity testing","text":"<p>To know the capacity and performance of a service you can perform some load tests prior to moving to production. An alternative approach to evaluate performance is to extract the access logs of the service into an aggregation tool like Kibana and evaluate the number of requests exceeding the limits.</p> Cookbook Software Description Capacity tests with jmeter jmeter Utility for performance and capacity testing <p>Note</p> <p>A common challenge to service performance is the provision of a WMS service on a big vector dataset. When requesting that dataset on a national level, the server runs into problems drawing all the features at once. In such case consider to set up some cache/aggregation mechanism for larger areas. Setting proper min/max scale denominators may be a solution also.</p>"},{"location":"QOS/#usage-monitoring","title":"Usage monitoring","text":"<p>To capture the usage of a service you can extract the usage logs and import them in a tool like Kibana, Splunk or AW stats. Defining rules to extract the requested layer name from a WMS request is useful. Mind that not all requests are GET requests, some WFS requests use POST, which may need some configuration on the webserver to enable POST parameter logging. Make sure the logging includes the 'Referer' and 'User-agent' parameters, which allows to differentiate types of uses. Finally consider there is a GDPR privacy aspect to collecting access logs. Consider to exclude the IP address of the user and define a maximal retention for access logs.</p> Cookbook Software Description AWStats AWStat A utility to report on service usage"},{"location":"codelists/","title":"Overview Code Lists","text":"<p>Adoption of common code lists is an important aspect of data harmonization. For INSPIRE the INSPIRE registry is the source of common code lists. Other common codelists relevant to the soil domain are available in FAO Agrovoc, GEMET, OGC definition server, ISO TC211, and GLosis. At a national level some countries have implemented a national repository for common national codelists which may be relevant (either as a source or as a target, to publish an extended list).</p> <p>Adoption of common code lists is an aspect of step <code>4) data organization</code> in the soil information workflow, although it could also impact step <code>1) data collection</code>.</p> <p>The adoption of code lists has three aspects:</p> <ul> <li>Inventarisation of the code lists used in the source model</li> <li>Evaluation of the differences between local and common code lists</li> <li>Some code lists are a full match</li> <li>Some code lists need to be extended, or values mapped</li> <li>Some local code lists do not (yet) have a common code list available</li> <li>In cases where the common code list cannot (fully) be adopted, the code list needs to be published in a local repository</li> </ul> <p>Adoption of a dedicated codelist is relevant for example for Soil classification. Many of the national soil classification systems have much more detail than the World Reference Base, as suggested to be used by the TG Soil.</p> Please note that the harmonization meant here is harmonization of the description of the data, for example describing a soil observation of pH KCl with dilution 1:10 in the same way across Europe. The harmonization of the data itself, for example transforming pH KCl values to pH H2O values, is a separate step and not described in this wiki. More information on that harmonization can be found in D6.1 chapter 3.5 page 122. <p>The soil theme has a large number of code lists, ranging from soil type to ranges of grain size. Many code lists originate from the FAO soil classification and are published in the INSPIRE registry.</p> <p>If you missed the 2022 EJP Training on Soil data good practices, you can still have a look at a presentation about codelists.</p> <p>Implementation options for managing and publishing a code list:</p>"},{"location":"codelists/#minimal","title":"Minimal","text":"<p>The most basic form of publishing an alternative or extended code list is to place a code list file on a web location and reference values in it as https://example.org/codelist.xml#concept (see for example http://schemas.opengis.net/iso/19139/20070417/resources/Codelist/gmxCodelists.xml)</p> Cookbook Software Description Code list as iso19135 Publish an XML file on a web location"},{"location":"codelists/#traditional","title":"Traditional","text":"<p>Extended code lists can be published in a local or national instance of the Re3gistry software. This open source project is hosted by JRC to facilitate the INSPIRE registry.</p> Cookbook Software Description Code list in Re3gistry Re3gistry Publish a codelist in Re3gistry"},{"location":"codelists/#experimental","title":"Experimental","text":"<p>A standard for the definition of code lists is Simple Knowledge Organization System (SKOS). Any SPARQL endpoint can be used to publish a code list based on SKOS. Software exists which facilitates the consumption of SKOS data from a SPARQL endpoint in a human friendly way. An example is Skosmos.</p> <p>A powerfull aspect of SKOS is that you can link from a concept to existing concepts in other codelists using link relations such as: sameAs, Broader, Narrower.</p> Cookbook Software Description Extend a codelist How to extend an INSPIRE codelist Publish a SKOS codelist Virtuoso Skosmos Publish a code list in semantic web"},{"location":"consumer/","title":"Consumer of Soil Data","text":"<p>In each of the articles we aim to describe the producer and consumer aspect of a topic. However in many cases we mainly describe the producer aspect.  On this page we provide some specific consumer guidance and reference a number of articles which have explicit sections on the consumer aspect of discovering, accessing and assimilating Soil Data.</p> <p>Consuming and assimilating soil data has various challenges. </p> <ul> <li>Discovery of data; not much data is advertised and on various platforms. In many cases the search engine is the only option to find something. In other cases there's too much similar results, which prevent you from easily finding the gems between the others. </li> <li>Soil data is quite complex by nature; samples representing a soil horizon at a location, being analysed in some laboratory. These observations are used to derive a region sharing a common value, being vizualised on a map. Capturing this complexity in a standardised model has been quite a challenge. It resulted in iso28258 and derived models such as GLOSIS and the INSPIRE model. The process of adopting the standard requires expert skills, consider that working with this type of models is challenging, especcially in generic GIS tools such as QGIS/ArcGIS. It resulted in the fact that many organisations haven't fully adopted these models and provide the data in a local model or that the harmonised data is a subset of the original dataset. As a consumer it is hard to assess the level of implementation of the standard. </li> <li>INSPIRE identified WFS as a relevant exchange mechanism for rich data such as soil data. However the industry has not moved with those efforts and hardly any tooling exists which is able to use WFS to explore the richness of WFS's providing rich (soil) data.</li> <li>As part of the harmonisation it is important to adopt common code lists, or, for unique concepts, extend existing code lists in a proper way. Many implementations have failed to follow this principle, resulting in a multitude of code list references which are redundant or under documented. In many cases a consumer will need to harmonize code list values as part of using the data.</li> </ul> <p>Below some guidance on how to face some of these challenges:</p>"},{"location":"consumer/#discovery-of-data","title":"Discovery of data","text":"<p>Some guidance on locating soil data sources.</p> <ul> <li>Datasets of soil institutes of EU memberstates are typically found on the INSPIRE GeoPortal, filter by theme 'soil'.</li> <li>ESDAC hosts a series of pan European datasets, including the Lucas database.</li> <li>Datasets of academic projects can best be located at OpenAire Explore</li> <li>ISRIC - World soil information hosts a discovery service including some 200 datasets at https://data.isric.org, and a separate collection of external soil resources </li> <li>Global Soil Partnership provides a number of global soil maps</li> <li>FAO provides a number of legecay soil maps and databases in the Soils portal</li> <li>Google provides a dedicated dataset search option, operating on structured data of websites</li> </ul>"},{"location":"consumer/#multiple-source-data-models","title":"Multiple source data models","text":"<p>INSPIRE provides guidance on how to indicate in metadata the level of harmonisation of the service. Unfortunately this feature is not broadly adopted yet, many data providers provide non harmonised data without indicating it as such.</p> <p>In case data is provided using the INSPIRE Soil Model you can use Hale Studio to import gml to an alternative datamodel.</p> <p>Because INSPIRE Soil is based on the Observations and Measurements model, it is possible to publish soil data using a SOS or SensorThings API. At this moment there are no known implementations of SOS or STA to provide Soil data. I'm not aware of procedures to download full datasets from a sensor service and transform them, but the use case may not be relevant to sensor data. A multitude of sensor clients is available to interact directly with Sensor Services, providing the options to filter the results and extract only the relevant data for a use case.</p>"},{"location":"consumer/#importing-wfs-data","title":"Importing WFS data","text":"<p>Because a single INSPIRE Soil dataset consists of multiple featuretypes (plots, profiles, horizons, observations, ...) requesting the full dataset is not possible using a single WFS GetFeature request. Instead a client should consult the service capabilities to evaluate which featuretypes are available and iterate over each of them.</p> <p>INSPIRE mandates the availability of a stored query on any WFS which is able to download the full dataset in a single request. This is by far the easiest option to download a dataset. However consider that this option is not provided by some providers.</p> <p>The GDAL utility supports GMLAS (GML Application Schema) and is able to import gml from WFS and store it in for example a database. See the GDAL recipe for an example. Hale studio is also able to import data from a WFS.</p> <p>Note</p> <p>If you open a rich INSPIRE WFS service in QGIS, you will get unexpected results. In some cases QGIS will be able to extract a geometry and display the layer, but in other cases it will open each featuretype as a table (without geometry). Links between layers and tables are not imported. A GMLAS plugin (based on the GDAL GMLAS functionality) has been developed in 2016, but adoption has been low and the project seems currently unmaintained.</p>"},{"location":"consumer/#code-list-mappings","title":"Code list mappings","text":"<p>Common codelists are stored on the INSPIRE registry, or common registries such as Agrovoc or Gemet. If a value needs to be registered which is not in those registries, organisations have the option to publish an extended version of the code list locally. Code list values are typically provided as a URI referencing the concept in a registry. </p> <p>The soil model has various codelists. Important codelists are the (WRB) soil clasification, the observed soil property and the methods used in the laboratory. It is important to understand if the method used impacts the measured value. For example there are various methods to measure pH, which each give a specific value. Pedo transfer functions exist to recalculate the value based on the method used. Prior to applying a pedo transfer function, you have to map the codelist from the remote dataset to the code list used locally. Some cases can exist:</p> <ul> <li>Both datasets reference the same value from a common code list</li> <li>The dataset references a remote concept, but it matches with a concept used locally</li> <li>The dataset references a remote concept, but no local representation exists for the concept </li> <li>No information exists on the concept used</li> </ul> <p>Option 1 is optimal, option 2 requires a mapping table (a suggestion can be made to include the concept on the inspire registry), option 3 and 4 are problematic, these cases should be decided on case by case. Hale studio includes an option to provide mapping tables to map codelist values. </p>"},{"location":"data-collection/","title":"Data Collection","text":"<p>This article describes the <code>data collection</code> step in the Soil Information Workflow and references various cookbook recipes related to this step in the workflow.</p> <p>Data collection is done in the field and laboratory. During this process, data collection tools are used. Proper selection and preparation of data collection tooling impacts how data is passed to the next steps of the data workflow. Verify on the collection tooling:</p> <ul> <li>Relevant aspects to the observation are captured (staff, date, sample-id, depth, location, procedure, ...)</li> <li>Use of common code lists (definition of texture classes, colours, units of measure)</li> </ul>"},{"location":"data-organisation/","title":"Data Organisation","text":"<p>This article describes the <code>data organisation</code> step in the Soil Information Workflow and references various cookbook recipes related to this step in the workflow.</p> <p>In this step a data curator prepares the organisation for incoming soil observations from field studies and laboratory work. Data from external organisations is prepared and made available to augment and/or validate local data. Relevant measures are put in place to authorise relevant staff, prevent data loss and data redundancy and assess privacy concerns. Concrete deliverables of this phase are:  - data model materialised in a physical database  - implemented procedures to prevent data loss  - access grants to relevant staff  - metadata document in which these aspects are described - procedures to evaluate quality of the data and infrastructure </p> <p>Note that in this phase no effort is made to publish data to remote partners or the general public, but it does make sense to capture on the metadata if any access constraints apply which may prevent publication to a wider audience in a later stage.</p>"},{"location":"data-organisation/#types-of-soil-data","title":"Types of soil data","text":"<p>Within the soil domain (and wider earth sciences) typically 4 types of data are identified: </p> <ol> <li>\u2018As-is\u2019 data, legacy datasets in their original form or field and analytical data as it is received from field surveyors and laboratories, including reporting about the applied procedures.</li> <li>Soil analytical data and field descriptions of a) soil profiles and b) soil plots, standardised to a common model (iso28258). The data includes the lineage, the definition of the soil in space and time (xyzt), the attribute concerned, the procedure used and the unit of expression. Domain values are selected from common code lists. </li> <li>A dataset in which the soil parameter values are harmonised as if they were analysed using a single procedure. The metadata of individual observations can be left out, resulting in a abbreviated dataset, typically presented as multiple parameter values (texture fractions, pH, Ca content, Vegetation, Groundwater level) of a feature of interest (horizon, profile, plot defined by xyzt) in a flat table.</li> <li>Predicted distribution of soil properties or soil classes, either as grid cells through a statistical model or as vector polygons drawn with expert knowledge in the field. </li> </ol>"},{"location":"data-organisation/#code-lists","title":"Code lists","text":"<p>Another aspect to consider as part of data standardization is the adoption of common code lists that support interoperability of data. Datasets which use for example different texture classes are hard to compare. Within the soil community there are a number of common code lists available.  </p> <ul> <li>World Reference Base (WRB), available in digital form via Agrovoc. </li> <li>FAO Guidelines of Soil Description (these practices are integrated in the latest version of WRB), digitised as part of the GLOSIS Web Ontology effort. </li> <li>The GLOSIS Web Ontology includes code lists of common soil properties and analysis procedures. This list was originally collected in A compilation of georeferenced and standardised legacy soil profile data for Sub-Saharan Africa (Africa Soil Profiles Database; Leenaars J.G.B et al 2014). And later extended by the soil community. </li> </ul> <p>In some cases it is relevant to define code lists at a country or even regional level. A regional code list repository supports practitioners in standardizing their data early in the data lifecycle. Extending a codelist is then a relevant recipe.</p>"},{"location":"data-organisation/#metadata-management","title":"Metadata management","text":"<p>Metadata documents are preferably based on a common ontology such as DCAT, DataCite or  iso19115. Roughly 2 approaches, with various implementation options, exist to maintain metadata within an organisation:</p> <ul> <li>Metadata is included in the data file or is stored as a sidecar file next to the actual data file. At intervals a crawler imports metadata to make the data findable from a central location.</li> <li>Metadata is captured in a separate system and linkage is maintained between the actual dataset and its metadata.</li> </ul> <p>The second option is relatively easy to adopt, however it comes with a potential big effort to keep the metadata system in sync within the organisation. In practice organisations tend to implement a combination of both options. </p> <p>This step includes the organisation of data arriving from external sources and the standardisation of that data with local data (transform to a common model). As soon as a user imports a remote dataset for use in a project, it is useful to include a metadata record about that dataset in the local repository, so colleagues understand where/when the data was imported. If the dataset is available online, it is better to link to the remote dataset, so users would always use the latest version. </p>"},{"location":"data-organisation/#related-recipes-in-this-cookbook","title":"Related recipes in this cookbook:","text":"<ul> <li>a URI strategy</li> <li>A Pythonic metadata workflow</li> <li>A discovery service in GeoNetwork includes a section on   metadata authoring in GeoNetwork</li> <li>Metadata and View Service with GeoCat Bridge, GeoNetwork and   GeoServer includes a section on metadata management in QGIS</li> <li>DCAT</li> </ul> <p>Consider that if a project uses a dedicated data model, transforming external data from a common model to the dedicated project model is relevant. The recipe on data harmonisation to a locally used data model is relevant (See also the general Data standardisation recipe).</p>"},{"location":"data-sharing/","title":"Data and information sharing","text":"<p>This article describes the <code>data and information sharing</code> step in the Soil Information Workflow and references various cookbook recipes related to this step in the workflow.</p> <p>In this step the data curator manages processes that maintain discovery, view and download services on the web. The main activity is probably the creation of new services, as part of that activity it should be assessed if the services comply with the relevant regulation. Another important activity is the monitoring on Quality of service.</p>"},{"location":"data-sharing/#related-recipes-in-this-cookbook","title":"Related recipes in this cookbook:","text":"<ul> <li>Metadata and Discovery</li> <li>View services</li> <li>Download services</li> <li>Quality of service</li> </ul>"},{"location":"download/","title":"Overview Download Services","text":"<p>Download services facilitate the download of vector, grid or sensor data. </p> <p>If you missed the initial EJP Training on Soil data good practices, you can still have a look at a presentation about download services as Coverage and SensorThings or a presentation on Atom/WMS/WFS.</p> <p>Setting up download services is an aspect of step <code>6) data and info sharing</code> in the soil information workflow.</p> <p>This page lists some implementation options for providing INSPIRE Download Services.</p>"},{"location":"download/#minimal","title":"Minimal","text":"<p>The INSPIRE Atom Service provides a minimal download service implementation on a series of downloadable files placed in a web accessible folder. For every file a 'dataset feed' document can be generated and linked to a service feed describing the 'service'. A metadata record points to the service feed to complete the implementation.</p> <p>Alternatively, products like GeoNetwork and Hale Connect (Annex 1) can provide an Atom interface on top of a set of registered datasets. The TG download services also provides some PHP scripts which create an Atom OpenSearch interface for a series of files.</p> Cookbook Software Description Webdav Wsgidav Setting up atom service using webdav Zenodo zenodo Data publication on Zenodo"},{"location":"download/#traditional","title":"Traditional","text":"<p>Web Feature Service (WFS) and Web Coverage Service (WCS) are commonly used to download Featurecollections (vector) and Coverages (grids). Consider that most of the INSPIRE themes (including soil) require publication of hierarchical (app-schema) features, this aspect is not supported by many WFS server implementations. Some tools with this capability are listed in the table below. For WCS a good practice is available to facilitate implementation.</p> Cookbook Software Description GeoServer GeoServer The app-schema plugin extends the WFS implementation with support for hierarchical features. On the fly transformation is managed from a configuration file. Marcus Sen (Onegeology) create a cookbook for this approach. Get started with Hale Connect Hale Connect Optimized for performance, stores pregeneralised xml fragments in combination with an elastic search index for filtering deegree deegree A java based spatial application server Coverages with rasdaman Rasdaman A Web Coverage Service implementation SensorThings API via FROST server FROST server Soil observation data as sensor stream <p>Consider that a product advertising WFS support does not automatically qualify for INSPIRE, the product has to support hierarchical GML.</p>"},{"location":"download/#experimental","title":"Experimental","text":"<p>A good practice document has been adopted on the use of OGC API Features as download service. With its 20 years of history WFS and GML are out of synch with current IT practices. OGC API is a new direction of standards within OGC adopting some of the latest IT conventions, such as Open API, Rest services, JSON encodings, content negotiation, etc. The use of OGC API will increase in coming years while OGC adopts more standards. Various products exist implementing the final and/or draft specifications. </p> <p>As described in the harmonization paragraph, Sensor Observation Service and SensorThings API offer an alternative download option for themes including much observation data, such as Soil.</p> <p>GraphQL and SPARQL are powerfull query api's to request data over the web. Both have not gone through the proces of Good practice adoption. But they are serious candidates to provide a modern INSPIRE download service.</p> Cookbook Software Description OGC API Features via pygeoapi pygeoapi A python based open source implementation of OGC APIs including OGC API Features. Configuration is managed from a configuration file. Proxy a WFS as OGC API Features using LDProxy ldproxy A java based open source implementation of OGC APIs. Originally developed by Interactive Instruments as an easy way to consume API (proxy) on top of existing WFS. These experiments were a main driver for OGC in the direction of OGC API. Configuration is managed from a web interface. Soil data via GraphQL Postgraphile Soil data from a postgres database via graphQL"},{"location":"etl/","title":"Data Standardisation (vector data)","text":"<p>Data Standardisation is an aspect of step <code>4) data organization</code> in the soil information workflow.</p> <p>Data standardisation is the process of aligning one or more datasets to a common (standardised) data model. For the soil domain, ISO 28258:2013 Soil quality \u2014 Digital exchange of soil-related data is a standardised conceptual model which has been developed by the soil community to facilitate the capture of soil plot and profile data. The core of the model is the Observations and Measurements model of the Open Geospatial Community. The ISO28258 model has proven to be generic to capture a lot of soil data use cases, and facilitates interoperability with partners. </p> <p></p> <p>These days various encodings of the ISO28258 conceptual model are available -   A xml/xsd oriented model  -   A relational model (PostGreSQL)  -   A semantic web ontology, called GLOSIS Web Ontology </p> <p>During implantation of ISO28258 limitations of the model are likely to be  discovered related to the local Use Cases. These limitations can usually be bypassed by extending the model. In recent years some extensions to ISO28258 have been published, which could be relevant to the your cases. Examples are GLOSIS domain model and the INSPIRE Soil data model.  Annex D of TG Soil has a specific example on extending the model for a soil contamination use case. Options for model extensions vary per technology. Wetransform has a dedicated section on model extension on their website, based on an R&amp;D project from 2016. </p> <p>An important activity related to Standardisation is the adoption of common code lists and extending those lists to capture regional conventions. The role of code lists is explained in a dedicated codelists section. </p> <p>If you missed the initial EJP Training on Soil data good practices, you can still have a look at a presentation about vector data standardisation.</p> <p>This document lists various implementation options for data Standardisation. </p>"},{"location":"etl/#minimal","title":"Minimal","text":"<p>The good practice on GeoPackage describes a relational database format to share standardised data. GeoPackage is a standardised format for storing relational data by the Open Geospatial Consorium, building on the SQLite database specification. Becuase many soil data is stored in the form of relational databases, the transformation to GeoPackage is relatively easy. The transformation process could for example be triggered by a series of database queries within a GIS Desktop client such as QGIS, in Python scripts or an ETL tool such as Hale Studio or FME. </p> <p>Pro's and Con's:</p> <ul> <li>The GeoPackage format is easy to consume by average users.</li> <li>To capture the hierarchical structure of the INSPIRE datamodels, a lot of tables are needed, resulting in a complex data-model. </li> <li>The good practice is recent, so not a lot of community experience is available yet </li> <li>Users download a full dataset, no (filering) api's are defined for GeoPackages yet</li> </ul> Cookbook Software Description INSPIRE in a relational database Geopackage Standardise soil data using GeoPackage"},{"location":"etl/#traditional","title":"Traditional","text":"<p>Tools like Hale Studio and FME are typically used to configure a conversion from data in a relational data model to data in a GML based INSPIRE model. The output is a big GML file which can be published using a WFS server or Atom download service. Below table links to detailed pages on various relevant technologies.</p> Cookbook Software Description Hale Studio Hale Studio Humboldt Alignment Editor Studio is a Desktop tool to author 'data alignments'. - FME Feature Manipulation Engine is a visually oriented data integration platform <p>You may not have considered before, but consuming a rich GML is not straight forward in common GIS clients like ArcGIS or QGIS. To consume a rich GML you need software which is able to traverse xml hierarchies and links. Tools like Hale Studio can also be used to read rich GML and convert it back to a relational database. Unfortunately you can not automatically reverse a ETL-configuration. But you can set up a new ETL configuration to read and transform the rich GML. A recipe is available which imports INSPIRE Soil GML from the city of Berlin and converts it to a relational database.</p>"},{"location":"etl/#experimental","title":"Experimental","text":"<p>Alternative ETL procedures are based on Semantic Web technology. The INSPIRE community is exploring various semantic web options, but no good practice has been adopted yet.</p> <p>See also the presenation about semantic web (GLOSIS) from the 2022 EJP Soil training.</p> Product Software Description Semantic Standardisation using TARQL TARQL command-line tool for converting CSV files to RDF using SPARQL 1.1 Semantic mapping using YARRRML RML mapper Human readable RDF mappings for RML.io Publish data through semantic web Yed Coby BlazeGraph Cookbook by INRAE on publishing soild data as RDF tarql"},{"location":"glossary/","title":"Glossary","text":"<p>This page includes terms that we use in our wiki, so that you have a reference for how we\u2019re using them.</p> <p>Application programming interface (API) is a way for two or more computer programs to communicate with each other (source wikipedia)</p> <p>Assimilation is a catchy term indicating the processes involved to combine multiple datasets with different origin into a common dataset, the term is somewhat similarly used in psychology as <code>incorporation of new concepts into existing schemes</code> (source wikipedia). But is not well aligned with its usage in the data science community: <code>updating a numerical model with observed data</code> (source wikipedia)</p> <p>Catalogue or metadata registry is a central location in an organization where metadata definitions are stored and maintained (source wikipedia)</p> <p>ATOM is a standardised interface to exchange news feeds over the internet. It has been adopted by INSPIRE as a basic alternative to download services via WFS or WCS.</p> <p>Conceptual model or domain model represents concepts (entities) and relationships between them (source wikipedia)</p> <p>Content negotiation refers to mechanisms that make it possible to serve different representations of a resource at the same URI (source wikipedia)</p> <p>Digital exchange of soil-related data (ISO 28258:2013) presents a conceptual model of a common understanding of what soil profile data are</p> <p>Discovery service is a concept from INSPRE indicating a service type which enables discovery of resources (search and find). Typically implemented as CSW.</p> <p>Download service is a concept from INSPRE indicating a service type which enables download of a (subset of a) dataset. Typically implemented as WFS, WCS, SOS or Atom.</p> <p>Encoding is the format used to serialise a resource to a file, common encodings are xml, json, turtle</p> <p>Geography Markup Language (GML) is an xml based standardised encoding for spatial data.</p> <p>Global Soil Information System (GLOSIS) is an activity of FAO Global Soil Partnership enabling a federation of soil information systems and interoperable data sets </p> <p>GLOSIS domain model is an abstract, architectural component that defines how data are organised; it embodies a common understanding of what soil profile data are.</p> <p>GLOSIS Web Ontology is an implementation of the GLOSIS domain model using semantic technology</p> <p>GLOSIS Codelists is a series of codelists supporting the GLOSIS web ontology. Including the codelists as published in the FAO Guidelines for Soil Description (v2007), soil properties as collected by FAO GfSD and procedures as initally collected by Johan Leenaars.</p> <p>Harmonization is the process of transforming 2 datasets to a common model, a common projection, usage of common domain values and align their geometries</p> <p>Observations and Measurements (O&amp;M)</p> <p>OGC API</p> <p>Ontology</p> <p>Publication</p> <p>Relational model</p> <p>RDF</p> <p>REST</p> <p>Registry</p> <p>Repository</p> <p>SensorthingsAPI</p> <p>SOS</p> <p>UML model</p> <p>View service is a concept from INSPRE indicating a service type which presents a (pre)view of a dataset. Typically implemented as WMS or WMTS.</p> <p>Webservice</p> <p>WMS</p> <p>WFS</p> <p>WCS</p> <p>WMTS</p> <p>XSD</p>"},{"location":"identification/","title":"Identification, namespaces and URI strategy","text":"<p>An important aspect of publication of data on the web is universal identification of objects within the European INSPIRE infrastructure. Identifiers are constant, unique and authoritative.</p> <p>Resource identificiation is an aspect of step <code>4) data organization</code> in the soil information workflow.</p> <p>Any identifier is typically combined with a namespace for that identifier, or both aspects are combined into a single Universal Resource Identifier (URI) for the object. Namespaces need to be authoritative but should not be sensible to change. For example, a project name is not a good namespace, because the project is bound to end after a certain period. Examples of good namespaces are: w3id.org, doi.org, data.gouv.fr.</p> <p>Some countries have a registry of national namespaces (Netherlands, Germany). Select a namespace from that registry, or consider to add your namespace to such a registry.</p>"},{"location":"identification/#minimal-implementation","title":"Minimal implementation","text":"<p>In a minimal implementation you can concatenate the database identifier, the featuretype and a namespace to create the INSPIRE identification for the object. For example:</p> <p>https://data.gouv.fr/inrae/collections/{featuretype}/items/{id}</p> <p>To facilitate users to use the INSPIRE identification to open the object in a web browser, you can set up a routing mechanism to forward the request to the location where the catalogue or feature server is located.</p> <p>An example of such a routing rule in Apache webserver:</p> <pre><code>RewriteRule\n  \"https://data.gouv.fr/inrae/collections/(.\\*)/items/(.\\*)$\"\n  \"http://data.recover.inrae.fr:8081/geoserver/vulter/wfs?typeNames=$1&amp;featureID=$2&amp;request=GetFeature\"\n</code></pre>"},{"location":"identification/#traditional","title":"Traditional","text":"<p>The Technical Guidelines have a long section on identification within INSPIRE, including dates indicating the validity of a feature. An interesting blog about the use of Namespaces and Identifiers is written by Thorsten Reitz at https://www.wetransform.to/news/2018/02/12/best-practices-for-inspire-ids/.</p>"},{"location":"identification/#experimental","title":"Experimental","text":"<p>These days the INSPIRE community recommends the use of URI's to identify things. This aspect is described in https://inspire.ec.europa.eu/implementation-identifiers-using-uris-inspire-%E2%80%93-frequently-asked-questions/59309.</p> <p>The aspect of identification is one of the major benefits of the upcoming OGC API's over traditional WMS, WFS, WCS. By design any feature, coverage, record or tile in OGC API has a unique URI, including content negotiation to be able to request the object in one of the available encodings (xml, json, html, Geopackage)</p>"},{"location":"identification/#common-mistakes","title":"Common mistakes","text":"<p>In many cases catalogue records and service definitions are populated manually in separate locations. Verify that at each location the identification and namespace of links between metadata and services are correct. Initially JRC did not have testing procedures to test these linkages. In practice a lot of these links where not correct, causing users not to be able to download a dataset from a search result in the national and INSPIRE GeoPortal.</p> <p>JRC provides the resource linkage checker to evaluate linkage of resources.</p>"},{"location":"identification/#read-more","title":"Read more","text":"<p>See also the article about uri strategy</p>"},{"location":"metadata/","title":"Overview Metadata &amp; discovery","text":"<p>Discovery of available data is important for potential users to be aware what data is available, evaluate if the data is relevant for them and how they can fetch it, or who to contact for more details. Essentially, the initial goal of any Spatial Data Infrastructure (SDI) is to describe its content. Metadata of datasets and services is described in documents, which are made accessible via a discovery service as records in a catalogue.</p> <p>Capturing metadata prior and during data collection is an aspect of step <code>4) data organization</code> in the soil information workflow, publishing the metadata as part of the data dissemination is an aspect of step <code>6) data and info sharing</code>. Evaluating the findability of datasets and assessing broken links on existing metadata is an aspect of step <code>7) Soil Information User Consideration</code>.</p> <p>In the soil domain we generally have 2 types of datasets, actual soil observations (calcium content in a horizon of a soil profile at a certain date or a soil profile field classification) and derived grids or polygon maps which represent parameter or soil type distribution for an area. For the second type of datasets describing the lineage (history) of the data is very important. Typically, you would describe the dataset with point observations in 1 document and link another document, describing the derived dataset, and link it as a parent-child relation. The document describing the derived dataset will contain 'processing-steps' describing the model that was used to calculate or derive the parameter or soil type distribution (D6.1 ch 5). This aspect is important for the usage of the derived dataset, to be able to evaluate if the estimate is valid for the envisioned use.</p> <p>If you missed the initial EJP Training on Soil data good practices, you can still have a look at the presentation about metadata and discovery.</p> <p>This page lists various implementation options both for creating metadata, as well as setting up a discovery service.</p>"},{"location":"metadata/#minimal","title":"Minimal","text":"<p>In a minimal implementation you can describe your dataset as well as your services in a single metadata document. This 'good practice' is described in https://github.com/INSPIRE-MIF/gp-data-service-linking-simplification. Basic metadata editors exist, of which the most basic is Notepad++. In the Python domain exists the pygeometa and OWSLib projects, which offer capabilities to generate ISO19139 metadata from other formats.</p> <p>These metadata documents can be placed in a Web Accessible Folder. Products exist which are able to ingest documents from such a folder and expose it as a CSW discovery service. Such an ingest point could be installed at a national level, to facilitate the European INSPIRE GeoPortal (which currently only supports ingests via CSW).</p> Cookbook Software Description A Pythonic metadata workflow pygeometa A minimalistic approach to data discovery Data in zenodo zenodo Zenodo is a data repository by CERN/Horizon2020, including rich metadata options"},{"location":"metadata/#traditional","title":"Traditional","text":"<p>The TG metadata (Technical Guidelines metadata) defines 2 types of metadata; documents which describe a dataset which are linked to documents which describe the service via which the datasets are published.</p> <p>The TG discovery describes how the metadata documents need to be published as a CSW discovery service. The table below lists some products which can be used to set up such a service. Mind that the TG extends the OGC CSW specification with some specific INSPIRE elements, for identification and multilingualism.</p> Cookbook Software Description GeoNetwork GeoNetwork A java based open source catalogue application widely used by member states for INSPIRE discovery. Provides a public portal application. Supports CSW, metadata authoring, validation and harvesting. pycsw pycsw A Python based open source CSW server. Supports CSW, OGC API Records. Used in portal software such as CKANSpatial and GeoNode. Geoportal server ArcGIS Geoportal A java based open source CSW implementation for the ArcGIS platform. A CSW client for ArcGIS desktop is included. Note that this package is not the same as ArcGIS Portal. Hale Connect Hale Connect A metadata authoring and CSW interface is provided as part of the HALE Connect SAAS offering."},{"location":"metadata/#experimental","title":"Experimental","text":"<p>A good practice exists related to Geo-DCAT-ap. It explains how to publish metadata using the Geo-DCAT-ap vocabulary as an additional metadata format. At present the use of ISO19139 is required by all guidelines. However, it is expected that it will soon be possible to offer metadata in a DCAT only. DCAT facilitates records to be discovered via google dataset search (and other search engines and semantic web platforms).</p> <p>Currently no 'good practice' exists to offer discovery services in alternative protocols then CSW. A good practice to adopt OGC API Records is being prepared. OpenSearch, OData and SPARQL could be alternative discovery service protocols.</p> Cookbook Software Description dcat - A dcat approach to dataset discovery"},{"location":"view/","title":"Overview INSPIRE View Services","text":"<p>The TG View services prescribes the adoption of view services, which offer the capability of visualization of spatial data, possibly in a portal, GIS software or webpage. The service provides a quick view on the data, without the need to transfer the data itself to the client. The TG Soil prescribes that for each measured soil parameter a view service <code>layer</code> is made available online. Layers can relate to actual site observations (soil profiles) as well as parameter distribution grids or vector maps.</p> <p>Setting up view services is an aspect of step <code>6) data and info sharing</code> in the soil information workflow.</p> <p>This page lists some implementation options for providing INSPIRE View Services.</p>"},{"location":"view/#minimal","title":"Minimal","text":"<p>In a minimal implementation the Web Map Tiling Service (WMTS) standard is used to provide view services. Tile services have little risk with respect to the Quality of Service requirements. The alternative option, Web Map Service (WMS), is quite prone to exceed the performance limits in cases when it has to 'draw' a lot of data at once.</p> <p>Tile services are however not optimal for dynamic data and may require a large (tile) storage. Also consider that the adoption of WMTS is less wide spread then WMS in GIS clients.</p> <p>The getFeatureInfo (gfi) operation is not mandatory for INSPIRE (however useful for end users). Without getFeatureInfo, data used as a source for the view service can be minimal (geometry only).</p> Cookbook Software Description MapServer MapServer C based FastCGI WMS/WFS/WCS server implementation configured using 'mapfiles' Bridge &amp; GeoServer Bridge GeoServer The recipe describes how to publish view services from QGIS ArcMAP using GeoCat Bridge QGis server QGIS Qgis as a server"},{"location":"view/#traditional","title":"Traditional","text":"<p>Most current view services are based on the Web Map Service (WMS) standard. These services are usually easy to set up on top of an existing traditional Web Feature Service or Web Coverage service implementation.</p> <p>Examples are in the download services section.</p>"},{"location":"view/#experimental","title":"Experimental","text":"<p>OGC API Tiles and OGC API Maps are upcoming standards for map visualization. The Open Geospatial Consortium (OGC) is preparing the final standardization documents, however initial implementations are available in. There is no good practice document for adoption of OGC API Tiles or Maps in preparation yet within INSPIRE.</p> Cookbook Software Description - GeoServer OGC API Tiles is available via the OGC API community plugin - pygeoapi Python package which exposes a cache of tiles as OGC API Tiles - LDProxy Java based opensource OGC API implementation <p>Within the sector there is a shift to the use of Vector tiles for vector map visualization. Vector tiles usually require less bandwidth and provide a sharper view on the data, especially on mobile devices with high resolution. INSPIRE does not provide Guidance on the use of vector tiles yet. OGC API Tiles and the MapBox Vector tiles specification are common API's used to publish vector tiles.</p>"},{"location":"cookbook/52north/","title":"INSPIRE SOS download service using 52 North","text":"<p>Status: planned</p>"},{"location":"cookbook/52north/#read-more","title":"Read more","text":"<ul> <li>Webinar SOS and INSPIRE: https://www.youtube.com/watch?v=jyQJrTN4pjk</li> <li>GitHub: https://github.com/52North/SOS</li> <li>Docker hub: https://hub.docker.com/r/52north/sos</li> <li>OSGeo Live Quick Start: https://live.osgeo.org/en/quickstart/52nSOS_quickstart.html</li> <li>Specialised observations for INSPIRE: https://wiki.52north.org/SensorWeb/InspireSpecialisedObservations</li> </ul>"},{"location":"cookbook/awstats/","title":"AWStat","text":"<p>Status: in progress</p> <p>AWStat is a utility to report on service usage. These days there are advanced tools such as Splunk, Elastic Search, Grafana which can report on service usage. These tools are recommended to use if your orchanisation provides it. However if such a tool is not provided AWStat could be an interesting alternative. It is a small utility with some nice vizualisation options to present aggregated usage statistics in a dashboard.</p> <p>Most of these tools, including AWStats use the Access logs of the webserver to extract and aggregate usefull information. Consider that these applications need a carefull privacy strategy, because ip-adresses from logs can be used to identify users.</p>"},{"location":"cookbook/awstats/#deploy-awstats","title":"Deploy AWStats","text":"<p>Navigate to an empty folder, place a sample log file in the folder.</p> <pre><code>192.168.2.20 - - [28/Jul/2012:10:27:10 -0300] \"GET /cgi-bin/try/ HTTP/1.0\" 200 3395\n83.149.9.216 - - [04/Jan/2015:05:13:42 +0000] \"GET /presentations/logstash-monitorama-2013/images/kibana-search.png HTTP/1.1\" 200 203023 \"http://semicomplete.com/presentations/logstash-monitorama-2013/\" \"Mozilla/5.0 (Macintosh; Intel\"\nMac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36\"\n127.0.0.1 - - [28/Jul/2011:10:22:04 -0300] \"GET / HTTP/1.0\" 200 2216\n211.0.23.16 - - [28/Jul/2016:10:27:32 -0300] \"GET /hidden/ HTTP/1.0\" 404 7218\n211.168.17.20 - - [28/Jul/2021:10:27:10 -0300] \"GET /cgi-bin/try/ HTTP/1.0\" 200 3395\n18.12.120.17 - - [28/Jul/2022:10:22:04 -0300] \"GET / HTTP/1.0\" 200 2216\n163.22.12.13 - - [28/Jul/2018:10:27:32 -0300] \"GET /hidden/ HTTP/1.0\" 404 7218\n</code></pre> <p>Start a container</p> <pre><code>docker run -d --restart always --publish 3000:80 --name awstats --volume $(PWD):/var/local/log:ro pabra/awstats\n</code></pre> <p>Parse the logs:</p> <pre><code>docker exec awstats awstats_updateall.pl now\n</code></pre> <p>View the dashboard:</p> <p>http://localhost:3000</p>"},{"location":"cookbook/awstats/#read-more","title":"Read more","text":"<ul> <li>Website</li> <li>Docker</li> <li>Configuration</li> </ul>"},{"location":"cookbook/bridge-geoserver-geonetwork/","title":"Metadata and View Service with GeoCat Bridge, GeoNetwork and GeoServer","text":"<p>Status: ready</p> <p>In this recipe we use GeoCat Bridge to publish a View Service on a soil dataset in GeoServer, combined with connected metadata in a GeoNetwork instance. </p> <p>GeoCat Bridge is a plugin for QGIS or ArcMap developed by GeoCat in Bennekom, the Netherlands. Its goal is to  facilitate the complex process of data publication from  a well known desktop environment. An introductionary video is available at https://www.youtube.com/watch?v=f-sZCVnR9dc</p> <p>GeoServer is a server application providing OGC services on various database backends. See the relevant recipe for a more detailed description. </p> <p>GeoNetwork is a server application providing a search interface and various api's on a collection of metadata records. See the relevant recipe for a more detailed description.</p>"},{"location":"cookbook/bridge-geoserver-geonetwork/#contents-of-the-recipe","title":"Contents of the recipe:","text":"<ul> <li>Deploy GeoServer and GeoNetwork using Docker</li> <li>Install and configure the Bridge plugin</li> <li>Publish the dataset</li> </ul>"},{"location":"cookbook/bridge-geoserver-geonetwork/#deploy-geoserver-and-geonetwork-using-docker","title":"Deploy GeoServer and GeoNetwork using Docker","text":"<p>This recipe is based on Docker, but you can also install each of the components directly on your system. New to Docker? Read the Docker recipe.</p> <p>We use the QGIS edition of Bridge, but an edition for ArcMAP is also available.</p> <p>Download the file https://github.com/ejpsoil/ejpsoil-data-publication-guidance/docker/bridge-geoserver-geonetwork/docker-compose.yml into an empty folder. Navigate to the folder using a shell client (windows powershell or Linux/Apple shell) and run:</p> <pre><code>docker compose up\n</code></pre> <p>The above statement will download and deploy docker containers for GeoServer, GeoNetwork, PostGreSQL and Elastic Search. When finished (it may take a long time), you can access GeoServer at https://localhost:8000/geoserver and GeoNetwork at https://localhost:8001/geonetwork.</p> <p>GeoNetwork starts with an error, because the database is empty. Navigate to <code>http://localhost:8001/geonetwork/srv/eng/admin.console#/metadata</code>, login as user: <code>admin</code>, password: <code>admin</code>. Select the <code>iso19139:2007</code> profile and click <code>Load templates</code> and <code>Load samples</code>.</p>"},{"location":"cookbook/bridge-geoserver-geonetwork/#install-and-configure-the-bridge-plugin","title":"Install and configure the Bridge plugin","text":"<p>With QGIS, open the QGIS plugins repository and search for the <code>GeoCat Bridge</code> plugin. Install it.</p> <p>After succesfull installation, we will configure our GeoServer and GeoNetwork instances.</p> <p>Find the Bridge module on the toolbar or in the <code>Web &gt; GeoCat Bridge &gt; Publish</code> menu. From the publish window, open the <code>Servers</code> tab.</p> <p>In the bottom left click the <code>New server</code> option and select <code>GeoServer</code>. Populate the fields.</p> <p>Click <code>New server</code> again and select <code>GeoNetwork</code>. Populate the fields.</p> <p>You are now ready to publish your first dataset from QGIS.</p>"},{"location":"cookbook/bridge-geoserver-geonetwork/#publish-the-dataset","title":"Publish the dataset","text":"<p>Open the dataset to be published. Configure the layer with relevant styling and labels.</p> <p>Open the publish window from the menu (or toolbar). </p> <p>Select the layer to be published. A metadata editor will open in which you can configure some metadata. You can also import embedded metadata (use the button with a downward arrow, top right).</p> <p>Select the target servers for the publisation and click the publish button.</p> <p>When returning to the publish panel, you will notice an icon behind each layer indicating the publication status. You can now right click on the layer to preview the wms layer or the metadata.</p> <p>| Note that the styling options in QGIS and GeoServer are not a full match. Some styling transformations are applied which may impact the final result on GeoServer. A common caveat is the availability of certain fonts, used for labeling or icons, on the client and the server. Make sure all used fonts are available on the server as well. |</p>"},{"location":"cookbook/bridge-geoserver-geonetwork/#validate-the-view-service","title":"Validate the view service","text":"<p>The INSPIRE Validator provides a validation of view services. It will mainly test if metadata elements are available and the service is reachable.</p> <p>The docker containers run locally, so the services can not be tested by the INSPIRE Validator.  In Local Tunnel an approach is suggested to temporarily host a local service online, so you can run the validation.</p> <p>We have not yet installed the INSPIRE plugin on GeoServer and optimized the configuration of GeoNetwork, so expect some tests to fail</p>"},{"location":"cookbook/code-listsExtension/","title":"Code-lists","text":"<p>This recipe describes a semantic approach based on SKOS to extend an INSPIRE code-list.</p>"},{"location":"cookbook/code-listsExtension/#inspire","title":"INSPIRE","text":"<p>Code-lists specified for the Soil domain within the INSPIRE directive are described in the report D2.8.III.3 Data Specification on Soil \u2013 Technical Guidelines. A brief overview of these code-lists is offered below.</p> Code-list Description PhenomenonTypeValue [Extensible] A code list of phenomena (e.g. temperature, wind speed). Parent of ProfileElementParameterName, SoilSiteParameterName, SoilProfileParameterName and SoilDerivedObjectParameterName. LayerGenesisProcessStateValue An indication of whether the process specified in layerGenesisProcess is ongoing or ceased in the past. LayerTypeValue Classification of a layer according to the concept that fits the purpose,  e.g. \"topsoil\". ProfileElementParameterNameValue [Extensible] List of properties that can be observed to characterize the profile element. SoilDerivedObjectParameterNameValue [Extensible] List of soil related properties that can be derived from soil and other data. SoilInvestigationPurposeValue List of terms indicating the reasons for conducting a survey. SoilPlotTypeValue List of possible values that give information on what kind of plot the observation of the soil is made. SoilProfileParameterNameValue [Extensible] List of properties that can be observed to characterize the soil profile. SoilSiteParameterNameValue [Extensible] List of properties that can be observed to characterize the soil site. WRBQualifierPlaceValue List of values to indicate the placement of the Qualifier with regard to the WRB reference soil group (RSG), according to naming rules of the World reference base for soil resources 2006, first update 2007. WRBQualifierValue List of possible qualifiers (i.e. prefix and suffix qualifiers of the World Reference Base for Soil Resources 2006, first update 2007). WRBReferenceSoilGroupValue List of possible reference soil groups (i.e. first level of classification of the World Reference Base for Soil Resources 2006, first update 2007). WRBSpecifierValue List of possible specifiers, comprising only the values specified in the World Reference Base for Soil Resources 2006, first update 2007. <p>These code-lists are all available on-line through a r3gistry instance. Each item in these code-lists is a dereferenceable by URI. </p> <p>Four of these lists are extensible, those expressing properties associated with the different features of interest. They are designed to serve as umbrella structures for further specialisation according to local practices. Extension is made in an hierarchical fashion, meaning that each additional item must be a narrower definition of one of the existing items.</p> <p>Besides these curated code-lists, the INSPIRE domain model also creates space for code-lists of physio-chemical procedures of soil analysis. These are to be referenced from the Observation instances related to the properties referred above.</p> <p>There is no practical guidance in the Technical Guidelines document on how to extend or create new code-lists towards INSPIRE compliance. This document provides broad guidelines on how to do so with the Simple Knowledge Organisation System (SKOS), described ahead. </p>"},{"location":"cookbook/code-listsExtension/#glosis","title":"GloSIS","text":"<p>The GloSIS web ontology includes a large range of code-lists that are relevant within the INSPIRE context. GloSIS follows the pattern of the Sensor, Observation, Sample, and Actuator (SOSA) ontology, which is the Semantic Web counterpart of the Observations &amp; Measurements standard (O&amp;M). These code-lists are expressed with SKOS.</p> <p>The GloSIS code-lists spread along four main categories:</p> <ul> <li>Values meeting Descriptive properties (FAO Guidelines of Soil   Description):<ul> <li>Site/Plot (40)</li> <li>Layer/Profile (90)</li> <li>Surface (5)</li> </ul> </li> <li>Physio-chemical properties (circa 80 items)</li> <li>Procedures of physio-chemical analysis (circa 230 items).</li> </ul> <p>Code-lists in GloSIS are gathered in a generic module. With physio-chemical analysis procedures in a specific module.</p>"},{"location":"cookbook/code-listsExtension/#extending-code-lists","title":"Extending code-lists","text":""},{"location":"cookbook/code-listsExtension/#skos","title":"SKOS","text":"<p>The SKOS ontology is remarkably simple, actually one of its strengths. At its core are five primitives:</p> <ul> <li> <p>Concept - a unit of thought, an idea, a meaning, a category or an object.   Concepts are identified with URIs.</p> </li> <li> <p>Label - a lexical string used to annotate a concept. The same concept may   be annotated in different natural languages.</p> </li> <li> <p>Relation - a semantic association between two concepts, conveying   hierarchy or simply connecting concepts in a network.</p> </li> <li> <p>Scheme - an aggregator of related concepts, usually forming a hierarchy.</p> </li> <li> <p>Note - provides further semantics or definition to a concept. Often used   to associate a concept to other knowledge graphs or other external resources.</p> </li> </ul> <p>The listing below presents an item from the GloSIS code-list for crop classes. Its URI is http://w3id.org/glosis/model/codelists#cropClassValueCode-Ce_Ba, abbreviated with the Turtle syntax to <code>glosis_cl:cropClassValueCode-Ce_Ba</code>. The label for this item is \"Barley\" in the English language, an annotation also indicates its short notation: \"Ce_Ba\". Two object properties provide semantic associations, this item is a narrower definition of the <code>glosis_cl:cropClassValueCode-Ce</code> item (the \"Cereals\" concept), and is part of the scheme <code>glosis_cl:cropClassValueCode</code>.  </p> <pre><code>@prefix skos:  &lt;http://www.w3.org/2004/02/skos/core#&gt; .\n@prefix glosis_cl: &lt;http://w3id.org/glosis/model/codelists#&gt; .\n\nglosis_cl:cropClassValueCode-Ce_Ba a skos:Concept, glosis_cl:CropClassValueCode;\n        skos:prefLabel \"Barley\"@en ;\n        skos:notation \"Ce_Ba\" ;\n        skos:inScheme glosis_cl:cropClassValueCode ;\n        skos:broader glosis_cl:cropClassValueCode-Ce .\n</code></pre> <p>The next listing shows the scheme integrating the \"Barley\" concept above. This object is very similar, with a label providing its name and some notes adding further definition. The literal \"table 9\" indicates the source of this code-list within the FAO Guidelines for Soil Description.</p> <pre><code>@prefix rdfs:  &lt;http://www.w3.org/2000/01/rdf-schema#&gt; .\n@prefix skos:  &lt;http://www.w3.org/2004/02/skos/core#&gt; .\n@prefix glosis_cl: &lt;http://w3id.org/glosis/model/codelists#&gt; .\n\nglosis_cl:cropClassValueCode a skos:ConceptScheme ;\n        skos:prefLabel \"Code list for CropClassValue - codelist scheme\"@en;\n        rdfs:label \"Code list for CropClassValue - codelist scheme\"@en;\n        skos:note \"This  code list provides the CropClassValue.\"@en;\n        skos:definition \"table 9\" ;\n        rdfs:seeAlso glosis_cl:CropClassValueCode .\n</code></pre> <p>Using this simple pattern, a hierarchical code-list can be developed further simply using the predicates <code>skos:inScheme</code>, <code>skos:broader</code> and <code>skos:narrower</code>. The predicates <code>skos:topConceptOf</code> and <code>skos:hasTopConcept</code> can be further used to indicate the root items in a concept hierarchy. </p>"},{"location":"cookbook/code-listsExtension/#extend-an-inspire-code-list-with-skos","title":"Extend an INSPIRE code-list with SKOS","text":"<p>Suppose you intend to publish results of chemical analyses appraising the Zinc content of the soil. The INSPIRE code-lists do not presently include that metal as property, therefore an additional code-list item must be created.</p> <p>The first action is to identify in which code-list the item should be added. Physio-chemical properties like Zinc content appear associated with the Horizon or Layer class in soil ontologies, thus the appropriate code-list is ProfileElementParameterNameValue. Open that URI in your browser an go through the respective <code>r3gistry</code> record.</p> <p>At the bottom of the page <code>r3gistry</code> lists the items for this code-list, in this case they are biological parameter, chemical parameter and physical parameter. Follow the URI to the chemical parameter, in that page there is another list of items, including other metals. The new item for Zinc should therefore be created at this same level in the hierarchy.</p> <p>The list below presents a RDF document defining a new sub-item for Zinc within the physical parameter code-list. The URI for this item is <code>http://example.com/my-soil/zincContent</code>. As this item is not directly in a INSPIRE code-list, its URI must refer to an authority that you control, i.e. you or your institution are the responsible party for the code-list item, its definition and publication. For more on this, check the URI strategy document. </p> <pre><code>@prefix skos: &lt;http://www.w3.org/2004/02/skos/core#&gt; .\n@prefix inspire_cl_profile: &lt;http://inspire.ec.europa.eu/codelist/ProfileElementParameterNameValue/&gt; .\n@prefix glosis_cl: &lt;http://w3id.org/glosis/model/codelists#&gt; .\n@prefix my_soil: &lt;http://example.com/my-soil/&gt; . \n\nmy_soil:zincContent a skos:Concept;\n        skos:prefLabel \"Zinc content\"@en ;\n        skos:definition \"Zinc content of the soil within a profile element.\" ;\n        skos:broader inspire_cl_profile:physicalParameter ;\n        skos:inScheme inspire_cl_profile: ;\n        skos:related glosis_cl:physioChemicalPropertyCode-Zin .\n</code></pre> <p>The RDF above contains the basic SKOS elements: a Concept instance, a label, an annotation defining the concept and a reference to the parent item with the <code>skos:broader</code> code-list. The last two triples need a closer look. The item is declared as belonging to a Scheme with the URI <code>http://inspire.ec.europa.eu/codelist/ProfileElementParameterNameValue/</code>. While the later is not actually declared as such in <code>r3gistry</code>, it is still important to convey the nature of this item as part of a structured INSPIRE code-list. Finally, the <code>skos:related</code> predicated is employed to refer a similar property in the GloSIS web ontology. This last triple is not at all mandatory, but provides another dimension to the code-list item.</p>"},{"location":"cookbook/code-listsExtension/#extend-a-glosis-code-list","title":"Extend a GloSIS code-list","text":"<p>The GloSIS web ontology becomes particularly relevant in the sections of the INSPIRE domain model for which code-lists do not yet exist. A prominent case is the list of laboratory procedures to assess physico-chemical properties. As mentioned above, GloSIS includes a specific module for procedures.</p> <p>The listing below presents a RDF document with an extra item for the soil texture procedures code list (SKOS scheme <code>glosis_proc:textProcedure</code>). Compared to the previous example, the most remarkable aspect here is the declaration of a <code>glosis_proc:TextProcedure</code> instance. The latter is a sub-class of the SOSA Procedure class, making the bridge to the Observation instances declared in other GloSIS modules.  </p> <pre><code>@prefix skos: &lt;http://www.w3.org/2004/02/skos/core#&gt; .\n@prefix glosis_proc: &lt;http://w3id.org/glosis/model/procedure#&gt; .\n@prefix my_soil: &lt;http://example.com/my-soil/&gt; . \n\nmy_soil:textureProcedure-SaSiCl_5-50-2000u a skos:Concept, glosis_proc:TextureProcedure ;\n    skos:inScheme glosis_proc:textProcedure ;\n    skos:topConceptOf glosis_proc:textureProcedure ;\n    skos:prefLabel \"SaSiCl_5-50-2000u\"@en ;\n    skos:notation \"SaSiCl_5-50-2000u\" ;\n    skos:definition \"Sand, silt and clay fractions as used in my country (5-50-2000um)\" .\n</code></pre> <p>The remaining triples should all be familiar by now. The link to the SKOS scheme is made with the <code>skos:inScheme</code> and <code>skos:topConceptOf</code> predicates. Then come the annotations.</p>"},{"location":"cookbook/code-listsExtension/#create-a-new-code-list","title":"Create a new code-list","text":"<p>INSPIRE and GloSIS cover quite a good deal of ground with their code-lists. However, there might be circumstances where you may require an entire new code-list to express a domain property specific to your institution or dataset.</p> <p>The examples above already provide the necessary to build a code-list on your own. In summary, these are the steps:</p> <ol> <li> <p>Devise a URI policy, encompassing the code-list and its items.</p> </li> <li> <p>Create a Scheme instance to aggregate the code-list items. The Scheme    provides a URI and annotation for the code-list.</p> </li> <li> <p>Add in the top Concept instances with the predicates <code>skos:inScheme</code>,    <code>skos:topConceptOf</code> and <code>skos:hasTopConcept</code>.</p> </li> <li> <p>If necessary, add in narrower terms forming a hierarchy with <code>skos:broader</code>    and <code>skos:narrower</code>.</p> </li> <li> <p>Link items to similar concepts in other ontologies or vocabularies if possible.</p> </li> </ol>"},{"location":"cookbook/code-listsExtension/#publishing-your-own-code-list-items","title":"Publishing your own code-list items","text":"<p>Now that you extended or even created a new code-list the next step is to publish it on-line. There are three essentials ways of doing so, briefly described below. </p> <ol> <li> <p>A simple on-line RDF file. Save the items or code-list as a RDF document    and deploy it to a web server. Fairly easy, but not the most user friendly. </p> </li> <li> <p>A knowledge graph deployed to a triple store. Load the RDF document to a    triple store or directly create the code-list triples with SPARQL queries. Triple stores usually provide end-points for interaction with SPARQL queries and other mechanisms that facilitate user access/view of the code-list. Virtuoso is a triple store providing these functionalities.</p> </li> <li> <p>A SKOSMOS instance. Load the code-list triples into a triple store and then    link it to a SKOSMOS instance. This software leverages the SKOS ontology to presnnt code-lists in a user friendly, browsable complex of HTML pages.</p> </li> </ol>"},{"location":"cookbook/code-listsExtension/#dereferenceable-uris","title":"Dereferenceable URIs","text":"<p>When publishing a code-list, it is important to facilitate the resolution of the respective URIs. This promotes accessibility and use of the code-list. With a simple deployment to a web server, the hypothetical URI used above: <code>http://example.com/my-soil</code> can point directly to the RDF file. In this case it is customary the use of the hash character (<code>#</code>) to distinguish different resources within the same file. E.g.  <code>http://example.com/my-soil#zincContent</code> for the Zinc item.</p> <p>With more advanced software like SKOSMOS, a mid-layer mechanism is necessary to translate the URIs declared in the RDF into the corresponding pages or services. More about this topic in the Identification document. </p>"},{"location":"cookbook/code-listsExtension/#read-more","title":"Read more","text":"<p>At the masterclass edition 2023 Luis de Sousa presented an approach to extend codelists based on SKOS.</p> <p>In this article Wetransform explains how and which INSPIRE codelists can be extended. Either the UML model and/or the INSPIRE registry indicate if a codelist is extensible.</p>"},{"location":"cookbook/codelist-iso19135/","title":"Publish a codelist as a file in a web accessible folder","text":"<p>status: draft</p> <p>This recipe presents a minimal approach for publishing a (extended) code list. The code list is stored as a text file on a web accessible folder. A number of potential formats to use for the text file are discussed in this article. Concepts within the codelist are referenced as http://example.com/codes.xml#concept. The concept identifier is concatenated to the url with a '#' character. '#' represents a local link within the document, a convention adopted from the html standard.</p>"},{"location":"cookbook/codelist-iso19135/#codelist-format-options","title":"Codelist format options","text":"<p>The richest and most common format for code list storage is SKOS (RDF). It is the format used by for example Glosis to publish its codelists. Tools like GeoNetwork and Hale Studio are able to ingest codelists based on the SKOS ontology.</p> <p>An alternative format is based on the iso19135:2005 standard. TC 211 develops the iso19135 standard to offer a format for code lists in the spatial data community. The standard is for example used in the gmx-codelists as used in iso19139:2007.</p> <p>The approach described on this page is for example used in Lithuania. The gml described in this catalogue record links to codelists published in an XML format inspired by the INSPIRE registry. See https://inspire-geoportal.lt/resources/codelist/SO/OtherHorizonNotationTypeValue.xml. This format is probably selected because it can directly be ingested by Hale Studio.</p>"},{"location":"cookbook/codelist-iso19135/#host-your-codelist","title":"Host your codelist","text":"<p>The location of the codelist can be a webserver folder (apache/nginx), a git repository, even a shared sharepoint folder. But it is important that the url of the file is persistent for a considerable period. Because the datasets which link to codelist items depend on its availability. </p> <p>Persistency can be improved by adding an intermediary layer between the location of the file and the url on which it is made available, mechanisms such as provided by DOI and W3ID. Glosis codelists are for example stored at https://github.com/rapw3k/glosis but are referenced as http://w3id.org/glosis/model/codelists and then forwarded.</p>"},{"location":"cookbook/codelist-iso19135/#extending-inspire-codelists","title":"Extending INSPIRE codelists","text":"<p>A dedicated recipe describes the actual code list extension mechanism.</p>"},{"location":"cookbook/codelist-iso19135/#content-negotiation-optional","title":"Content negotiation (optional)","text":"<p>When a human arrives at a codelist file, if it is not in html format, the syntax with https://example.org/#concept opens the file at the top, and does not point to the relavant section, because the web browser does not understand the format. A mechanism of content negotiation can identify web browsers and present them an alternative format (html). Content negotiation can be set up in an intermediary webserver layer, the transformation from SKOS/iso19135 to html should be managed by an extra utility (such as SKOSMOS).</p>"},{"location":"cookbook/codelist-iso19135/#multilingual-concept-labels-optional","title":"Multilingual concept labels (optional)","text":"<p>Both SKOS, ISO19135 and the Re3gistry format support the option to provide labels for concepts in multiple languages.</p>"},{"location":"cookbook/dcat/","title":"DCAT","text":"<p>DCAT is an ontology from W3C to describe datasets in the semantic web. It is supported by software such as CKAN, GeoNetwork. The geodcat-ap group has developed a specialisation of DCAT v1 to describe spatial datasets. Many of the conventions of geodcat-ap have been introduced in follow-up editions of DCAT (v2, v3).   </p> <ul> <li>Standard: https://www.w3.org/ns/dcat</li> <li>GeoDCAT-AP: https://semiceu.github.io/GeoDCAT-AP</li> <li>Good Paractice: https://inspire.ec.europa.eu/good-practice/geodcat-ap</li> </ul>"},{"location":"cookbook/dcat/#ontologies-used-in-dcat","title":"Ontologies used in DCAT","text":"<p>DCAT makes use of other ontologies, following the best practices of the Semantic Web. They are all relevant to produce consistent and usable meta-data for the web.</p> <ul> <li> <p>vCard: For the description of People   and Organisations according to the specification issued by the Internet Engineering Task Force (IETF) (RFC6350). Also considers addresses, communication means and inter-personal relations.</p> </li> <li> <p>Dublin   Core: A small ontology implementing the fifteen element ISO 15836-1:2017 standard for documentation meta-data. The ontology expands on the original Dublin Core with additional terms meant to facilitate meta-data creation and publication with RDF.</p> </li> <li> <p>PROV-O: An OWL translation of the Prov   Data Model specified by the W3C. Provides a set of classes, properties, and restrictions that can be used to represent and interchange provenance information.</p> </li> </ul>"},{"location":"cookbook/dcat/#query-a-dcat-resource","title":"Query a DCAT resource","text":"<p>The query below provides an example of how to interact with a knowledge graph of metadata making use of the DCAT ontology. It returns a list of datasets tagged with keywords containing the string \"soil\". The data property <code>dcat:keyword</code> was originally meant exclusively for instances of the <code>dcat:Dataset</code> class, but since version 2 of the ontology it can be used with any class. </p> <ul> <li>Follow the virtuoso/skosmos recipe up to step <code>On the linked data tab, select Quad store upload.</code> Instead of adding a remote url you unzip and upload a rdf snapshot of the dutch spatial catalogue. </li> <li>Navigate to http://localhost:8890/sparql/ and run the query below.</li> </ul> <pre><code>PREFIX dcat: &lt;http://www.w3.org/ns/dcat#&gt;\nPREFIX dct: &lt;http://purl.org/dc/terms/&gt;\n\nSELECT ?dataset, ?title\nWHERE {\n    ?dataset a dcat:Dataset ;\n             dct:title ?title ;\n             dcat:keyword ?keyword .\n    FILTER CONTAINS(?keyword, \"EIGEN\") .\n}\n</code></pre> <p>The <code>CONTAINS</code> function in the query above is used to partially match the string. For an exact match the equals operator can be used instead (<code>=</code>). To match more than one keyword, successive <code>FILTER</code> clauses can be concatenated with the or operator (<code>||</code>).</p> <p>If at some point your database is corrupt, you can remove all triples by running:</p> <pre><code>DELETE FROM DB.DBA.RDF_QUAD ;\n</code></pre> <p>Colin Maundry provides some dcat sample sparql queries. Notice that you replace the graph url <code>http://www.data.maudry.com/uk</code> in the queries with yours.</p>"},{"location":"cookbook/deegree/","title":"deegree","text":"<p>Status: contribution required</p> <p>An open source java server implementation of WMS, WMTS, CSW, WCS, WFS, WPS.</p> <p>Deegree has 2 options to publish rich GML data. </p> <ul> <li>Using a relational database and a mapping configuration to generate the GML</li> <li>Using a <code>blob</code> storage to provide individual features without any processing</li> </ul> <p>The second is easy to setup and efficient when users often request full datasets. The second approach may get problematic if users use advanced filters to select subsets of the dataset.</p>"},{"location":"cookbook/deegree/#get-started-with-deegree","title":"Get started with deegree","text":"<p>Start a deegree instance locally using the docker hub image as:</p> <pre><code>docker run --name deegree -d -p 8080:8080 deegree/deegree3-docker\n</code></pre> <p>Initial start will take some time, then proceed with your browser to http://localhost:8080/deegree-webservices.</p> <p>The deegree website contains a detailed quick start on how to import and operate a sample workspace</p> <p>Setting up a database and importing GML Data is managed via a command line client. The command line client can be accessed via:</p> <pre><code>docker exec -w /opt deegree java -jar deegree-tools-gml.jar -help\n</code></pre> <p>The client tools are described in the online manual. An example call to export a database creation script to reflect the Soil.xsd schema:</p> <pre><code>docker exec -w /opt/ deegree java -jar deegree-tools-gml.jar SqlFeatureStoreConfigCreator --format=ddl --dialect=postgis --cycledepth=1 -schemaUrl=https://inspire.ec.europa.eu/schemas/so/4.0/Soil.xsd\n</code></pre>"},{"location":"cookbook/deegree/#featured-implementations","title":"Featured implementations","text":"<p>Some implementations of INSPIRE Soil data services based on deegree</p> <ul> <li>Baden W\u00fcrttemberg, Germany</li> <li>Brandenburg, Germany</li> </ul>"},{"location":"cookbook/deegree/#read-more","title":"Read more","text":"<p>deegree is maintained by a company called LatLon and others</p> <ul> <li>Website: http://www.deegree.org/</li> <li>Documentation: https://download.deegree.org/documentation/3.4.32/html/</li> <li>Download: https://www.deegree.org/download/</li> <li>Docker: https://hub.docker.com/r/deegree/deegree3-docker/</li> </ul>"},{"location":"cookbook/fme/","title":"FME","text":"<p>Status: contribution required</p> <p>Safe software propides a proprietary solution for data harmonization, e.g. conversion to and from INSPIRE data models. An introduction to the topic is provided at https://www.safe.com/integrate/inspire-gml/</p>"},{"location":"cookbook/frost-server/","title":"FROST server","text":"<p>Status: review required</p> <p>FROST is an open source server implementation of OGC Sensorthings, a modern data exchange standard for Sensor Data. Because the INSPIRE Soil model is based on Observations and Measurements, the Sensorthings API can be used  to provide sensor download services, with the soil profile and horizons as <code>FeatureOfInterest</code>. Sensorthings API is generally easier to set up for administrators and easier to consume by clients then INSPIRE Soil data in GML. INSPIRE has recently adopted a good practice on Download services based on SensorThings API.</p> <p>Kathi Schleidt has prepared a workshop on inspire data in sensorthings API. Also have a look at her sensor presentation in the 2022 edition of the training. After the 2022 training Kathy together with colleagues from Fraunhofer IOSB has put together a <code>SoilThings</code> variant of STA. The experiment has proven to be a most interesting use case, it has shown where flexibility has to be added in the STA model (these changes will be integrated in the upcoming 2.0 version of the standard). The cool thing about this API is that it allows you to do queries like the following: <code>give me all plots that contain a profile that contains a horizon on which pH H2O is measured</code>; while the query syntax can be a bit daunting, the other existing technologies for provision (WFS and the new OGC API) would require 5 separate requests to do this. Links: </p> <ul> <li>Base SoilThings API: https://ogc-demo.k8s.ilt-dmz.iosb.fraunhofer.de/FROST-SoilThings/v1.1/ </li> <li> <p>Query on all plots that contain a profile that contains a horizon on which pH H2O is measured, including the measurements: </p> <p>https://ogc-demo.k8s.ilt-dmz.iosb.fraunhofer.de/FROST-SoilThings/v1.1/SoilPlots?$count=true&amp;$select=name&amp;$filter=ObservedProfile/IsDescribedByHorizon/Datastreams/ObservedProperty/name%20eq%20%27pH%20H2O%27&amp;$expand=ObservedProfile($select=name;$expand=IsDescribedByHorizon($select=name;$expand=Datastreams($select=name;$filter=ObservedProperty/name%20eq%20%27pH%20H2O%27;$expand=ObservedProperty($select=name),%20Observations))) </p> </li> </ul>"},{"location":"cookbook/frost-server/#read-more","title":"Read more","text":"<p>At masterclass edition 2023 Kathi Schleidt and Hylke van der Schaaf presented Sensorthings API using Frost server.</p> <ul> <li>Good practice Sensorthings</li> <li>GitHub repository</li> <li>Documentation</li> <li>Workshop</li> </ul>"},{"location":"cookbook/geohealthcheck/","title":"GeoHealthCheck","text":"<p>Status: ready</p> <p>A tool to monitor availability of spatial services. The tool will query a list of configured spatial services at intervals and report on availability using charts. The tool can also send out notifications in case of disruptions.</p> <p>The tool can be compared to (and is often combined with) generic web availability tools such as Zabbix, Uptimerobot, Nagios. The generic tools are used to identify if the service is up, GeoHealthCheck will go a level deeper, identify which layers are available in a getcapabilities response and ask random maps to individual layers to identify if a service is properly running.</p>"},{"location":"cookbook/geohealthcheck/#exercise","title":"Exercise","text":"<p>The recipe assunes docker desktop installed. Alternatively you can create a personal account at https://demo.geohealthcheck.org (click <code>register</code> in the login page). Start a local GeoHealthCheck container:</p> <pre><code>docker run --name ghc -p 80:80 geopython/geohealthcheck\n</code></pre> <ul> <li>Visit http://localhost</li> <li>Login as user: <code>admin</code>, password: <code>admin</code></li> <li>Click <code>ADD +</code> on the top bar right, select <code>WMS</code></li> <li>Add a WMS url, for example <code>https://maps.isric.org/mapserv?map=/map/wrb.map</code></li> <li>On the next screen click <code>add</code> for <code>WMS Drilldown</code> (so all layers are validated)</li> <li>Click <code>Save</code> and <code>test</code></li> <li>When finished, click <code>Details</code> to see the test result</li> </ul>"},{"location":"cookbook/geonetwork/","title":"A discovery service in GeoNetwork","text":"<p>Status: in progress</p> <p>GeoNetwork is a catalogue for registering spatial datasets and services. GeoNetwork does support multiple metadata models based on XML, but it is optimized for iso19139:2007 and iso19115-2:2018. This recipe uses docker to run GeoNetwork locally. It will discuss aspects such as schema plugins, creating metadata records, set up codelists and harvest metadata.</p>"},{"location":"cookbook/geonetwork/#schema-plugins","title":"Schema plugins","text":"<p>GeoNetwork provides a dynamic system to pre load schema plugins providing support for a variety of metadata models, such as iso19139:2007, iso19115-2:2018, national profiles based on these, DCAT, SensorML, EML/GBIF, etc. Many of these plugins are made available via https://github.com/metadata101. Before creating or importing records, verify that the relevant profile is available in the GeoNetwork instance.</p> <p>Creation of metadata records is based on templates. For each metadata model a series of templates is available. Users select a relevant template for their use case while creating a new record to describe a resource. Any template is based on a specific metadata model, which determines which properties need to be described and in which format the record will be stored. </p> <p>GeoNetwork provides a number of transformation options for crosswalks between metadata models, but you should always consider some loss of information in these crosswalks. Crosswalks occur for example when a user requests a record in DCAT, while it is stored as iso19139:2007 in the database.</p>"},{"location":"cookbook/geonetwork/#quick-start","title":"Quick Start","text":"<ul> <li>Start a GeoNetwork instance locally (initial startup may take some time)</li> </ul> <p><code>docker run -p 8080:8080 geonetwork:3.12</code></p> <ul> <li>Navigate to http://localhost:8080/geonetwork</li> <li>Let's load some sample data. Select <code>Login</code>; login as usr:<code>admin</code>, pwd:<code>admin</code></li> <li>Select <code>Admin console</code> &gt; <code>Metadata records and templates</code>; Select <code>iso19139:2007</code>, click on <code>load samples</code> and <code>load templates</code></li> <li>On <code>Admin console</code> &gt; <code>Settings</code>, set the title and url of the instance.</li> <li>Let's set up some code lists (which populate the pull downs on the editor). Select <code>Classification systems</code> from <code>Admin console</code>.</li> <li>Select <code>From registry</code> in <code>Add thesaurus</code>.</li> <li>Click <code>Use INSPIRE registry</code> for the <code>Url</code> field</li> <li>Select language(s) and <code>INSPIRE theme register</code> and click <code>Upload</code></li> <li>Add another thesaurus from registry, select <code>INSPIRE metadata code list register</code> and then <code>Spatial scope</code>.</li> <li>Continue with other relevant code lists, notice that you can also create a new code list manually.</li> </ul>"},{"location":"cookbook/geonetwork/#create-records","title":"Create records","text":"<ul> <li>On <code>Contribute</code> Editor board, click <code>Add new record</code></li> <li>Select a template (they we're loaded on the previous step) and click <code>create</code></li> <li>On the editor notice the view (eye) button on top right, you can switch the editor view between <code>Simple</code>, <code>Full</code> and <code>XML</code>.</li> <li>Notice the <code>validate</code> button, which provides a report on the level of completion of the record</li> <li>The associated resources side panel displays links to remote resources, such as data files, data services and thumbnails</li> <li>Notice that you can also import a record from a local xml file</li> <li>Notice that you can collapse the <code>save</code> button to <code>save as template</code>, in this way others can use this record as a base to start a new record</li> </ul>"},{"location":"cookbook/geonetwork/#harvest-records","title":"Harvest records","text":"<p>Harvesting is the process of importing records from remote sources at intervals.</p> <ul> <li>Open <code>harvesting</code> from <code>Admin console</code></li> <li>Select <code>OGC CSW 2.0.2</code> from <code>Add thesaurus</code></li> <li>Provide a name for the harvester</li> <li><code>Action on UUID collision</code> determines the behaviour when similar records are found in multiple remote endpoints</li> <li>Provide the url of the remote endpoint (for example https://www.geocatalogue.fr/api-public/servicesRest?request=GetCapabilities&amp;service=CSW)</li> <li>Add a filter <code>Anytext:Soil</code></li> <li>Set a interval schedule for the harvest (<code>only one run</code>)</li> <li>Harvesters have many additional options, you can read about it in the documentation</li> <li>Click <code>Save</code> and on the next screen <code>harvest</code>, a spinner starts to run, the harvest may take some minutes, depending of the size of the remote catalogue</li> <li>Notice a list of harvest run logs at the bottom of the screen, you can click <code>log</code> to check in more detail.</li> <li>If you increase logging on <code>Settings</code> to <code>DEV</code>, the logging on <code>harvesting</code> will also provide more details (in case of non explainable errors)</li> </ul>"},{"location":"cookbook/geonetwork/#discovery-from-qgis","title":"Discovery from QGIS","text":"<p>The <code>metasearch</code> plugin is a default plugin in QGIS.  - Open the plugin from the <code>Web</code> menu (or toolbar).  - Click <code>New connection</code>. Provide a name for the connection and the url http://localhost:8080/geonetwork/srv/eng/csw - Switch to the find tab, and search some records.  - Select a search result, for some search results the <code>load data</code> button (lower left) is activated and you can load some data to the map</p>"},{"location":"cookbook/geonetwork/#atom-download-service-from-geonetwork","title":"Atom download service from GeoNetwork","text":"<p>You can enable an ATOM download service in GeoNetwork. GeoNetwork provides an opensearch API and will use the metadata content to generate Atom service and dataset files. You can read more about this option in the documentation.</p>"},{"location":"cookbook/geonetwork/#read-more","title":"Read more","text":"<ul> <li>Website: https://geonetwork-opensource.org/</li> <li>GitHub repository: https://github.com/geonetwork</li> <li>Docker composition: https://github.com/geonetwork/docker-geonetwork/blob/main/4.2.1/docker-compose.yml</li> <li>Documentation: https://geonetwork-opensource.org/manuals/4.0.x/en</li> <li>Tutorial: https://geonetwork-opensource.org/manuals/trunk/en/tutorials/introduction</li> </ul>"},{"location":"cookbook/geoportal-server/","title":"Esri Geoportal server","text":"<p>Status: contribution required</p> <p>Esri Geoportal Server provides seamless communication with data services that use a wide range of communication protocols, and also supports searching, publishing, and managing standards-based resources.</p> <p>Website: https://enterprise.arcgis.com/en/inspire/10.8/get-started/introduction-to-geoportals.htm GitHub: https://github.com/Esri/geoportal-server-catalog/blob/master/docker/gpt_stack/geoportal/Dockerfile</p>"},{"location":"cookbook/geoserver/","title":"Appschema WFS in GeoServer","text":"<p>Status: contribution required</p> <p>GeoServer is an open source java imlementation of WFS, WCS, WPS, WMS, CSW. Various OGC API endpoints are available via  a OGCAPI community plugin. WMTS is available via a default plugin called <code>GeoWebCache</code>.</p> <p>GeoServer is a popular server component because of the initial ease of setup and configuration in a webbased environment. It includes an authentication and authorisation system and advanced styling options. Configuration via a webinterface also has some negative aspects related to reproducability and scaling. GeoServer is able to provide INSPIRE data via the appschema plugin and (INSPIRE plugin)[https://docs.geoserver.org/stable/en/user/extensions/inspire].</p>"},{"location":"cookbook/geoserver/#run-as-docker-container","title":"Run as docker container","text":"<pre><code>docker run -p 8080:8080 kartoza/geoserver:2.22.0\n</code></pre> <ul> <li>Navigate to http://localhost:8080/geoserver</li> <li>Login as usr:admin pwd:geoserver</li> </ul> <p>Load some data:</p> <ul> <li>create a workspace</li> <li>create a datastore of type <code>folder of shapefiles</code></li> <li>create a layer</li> </ul> <p>Preview the layer.</p> <p>Consider that in this setup the configuration is lost at every restart of the container. In a normal scenario, you would mount a volume to persist the geoserver configuration. Optimally you place the volume under version control, so you can easily revert a previous situation.</p>"},{"location":"cookbook/geoserver/#inspire-plugin","title":"INSPIRE plugin","text":"<p>A GeoServer INSPIRE plugin is available which adds some of the INSPIRE specific metadata properties to the OWS capabilities documents. For example a link to the service metadata. The main feature is that it adds a bbox for each of the available projection systems. GeoServer is known to list all projection systems (many) as part of the capabilities response. You need to limit this number to prevent the bounds be written in each of this projections.</p>"},{"location":"cookbook/geoserver/#appschema-support","title":"Appschema support","text":"<p>Appschema is a plugin for GeoServer which adds the capability to work with hierarchival GML data, such as the INSPIRE Soil data model.</p> <p>Onegeology has prepared a workshop on how to set up an appschema dataset in GeoServer. This is an advanced workshop. </p> <p>At Foss4G 2022 in Florence one of the maintainers of GeoServer, GeoSolutions, announced a new approach to appschema in GeoServer, based on templating. I have not been able to test it yet, but it may resolve some of the challenges of the appschema approach.</p>"},{"location":"cookbook/geoserver/#geoserver-as-a-view-service","title":"GeoServer as a View service","text":"<p>GeoServer also provides options to publish view services (WMS or WMTS). Read more about this topic in the recipe GeoCat Bridge and GeoServer.</p>"},{"location":"cookbook/geoserver/#read-more","title":"Read more","text":"<p>Website: https://geoserver.org GitHub: https://github.com/geoserver/ Docker: https://docker.osgeo.org/geoserver Issue management: https://osgeo-org.atlassian.net/projects/GEOS/summary OSGeo: https://www.osgeo.org/projects/geoserver/</p>"},{"location":"cookbook/glosis-db/","title":"INSPIRE Soil in a relational database","text":"<p>This recipe describes an approach where data is harmonised to a common relational database.</p> <p>The INSPIRE community has recently started an interesting activity to set up guidelines on how to share INSPIRE data as a relational database. More specifically a database in the GeoPackage format, which is a specialisation of the SQLite format. This will result in a series of new good practices on alternative approaches for data harmonization for various INSPIRE themes and use cases. </p> <p>Idea behind the activity is that communities around a certain topic come together to develop a common relational model which substantially represents the INSPIRE UML model. This aspect can be validated by providing a mapping document which maps the relational model to the INSPIRE UML or GML model. This activity is supported by a template that communities can use to share their work with the wider INSPIRE community. </p> <p>After an initial effort of defining and describing the alternative model, a typical workflow to publish harmonised data is as follows:</p> <ul> <li>Users download the GeoPackage template as an empty database</li> <li>Users populate the database from various sources using their favourite tool (r, python, FME, Hale studio, DBeaver)</li> <li>Users publish the database as an Atom service or OGC API Features.</li> </ul>"},{"location":"cookbook/glosis-db/#partial-models-for-dedicated-use-cases","title":"Partial models for dedicated use cases","text":"<p>The INSPIRE Soil Model is designed to capture multiple soil data use cases: - capture profile descriptions in the field based on horizons - capture laboratory results from soil samples at fixed depths (layers) - predicted distribution of soil properties within soil bodies, linked to derived soil profiles</p> <p>In many cases these use cases are not combined in a single database. By creating a dedicated database model for specific use cases (remove the unused database object), the database model will be smaller and easier to understand. </p> <p></p>"},{"location":"cookbook/glosis-db/#use-cases-with-combinations-of-data-themes-in-a-single-database","title":"Use cases with combinations of data themes in a single database","text":"<p>In many cases, such as the <code>Soil Erosion</code> case, data is combined which is described in multiple INSPRE themes, such as soil, hydrology and environmental facilities. Some of the current GeoPackage implementation advertised in the INSPIRE MIF reference this type of combined use cases.</p>"},{"location":"cookbook/glosis-db/#iso25258-in-a-postgres-database","title":"ISO25258 in a PostGres database","text":"<p>As part of the Soils4africa project ISRIC and partners are experimenting with a ISO25258 model, encoded in a relational PostGres database. It is interesting to evaluate if this effort can be ported to GeoPackage and form a starting point for the initial effort.</p>"},{"location":"cookbook/glosis-db/#read-more","title":"Read more","text":"<p>At the masterclass edition 2023 Stefania Morrone (Epsilon) presented an approach to use geopackage as an alternative encoding for Soil data.</p> <ul> <li>Good practice on GeoPackage</li> <li>Model Transformation Rules for alternative encodings</li> </ul>"},{"location":"cookbook/hale-connect/","title":"Hale Connect","text":"<p>Status: contribution required</p> <p>Hale Connect is a Software as a Service solution provided by wetransform to provide view and download services with metadata for rich datasets, such as those following the INSPIRE data models. Transformations are prepared in Hale Studio and effectuated in the Hale Connect Solution.</p> <p>The services are  optimised for ease of use and performance, due to the cloud native setup of the data services. A data space connector is in preparation.</p> <ul> <li>Website: https://wetransform.to/haleconnect</li> <li>Get started at https://help.wetransform.to/docs/getting-started/2018-04-28-quick-start </li> </ul> <p>You can benefit from a 14-day free trial period when signing up for the platform.</p>"},{"location":"cookbook/hale-studio-consume-gml/","title":"Consume Soil GML with Hale Studio","text":"<p>Status: in progress</p> <p>Hale Studio is a familiar tool to transform data from a relational datamodel to INSPIRE Soil GML. However you can also use Hale Studio to connect to an existing WFS or load a GML file from a atom service and use Hale Studio to transform the data to a relational database model, so you can easily combine it with other relational databases.</p> <p>Note</p> <p>If you're not interested in a specific target model, the GMLAS functionality within OGR may be sufficient for you. GMLAS will create a arbitrary relational model from any GML. With GDAL installed, and a gml file called soil.gml, run the following from commandline:</p> <p>ogrinfo -ro GMLAS:soil.gml</p> <p>ogr2ogr -f SQLite tmp.sqlite GMLAS:soil.gml -dsco SPATILIATE=YES -nlt CONVERT_TO_LINEAR -oo EXPOSE_METADATA_LAYERS=YES</p> <p>In this recipe we will transform INSPIRE Soil GML from the city of Berlin to a relational database (GeoPackage).</p> <ul> <li>(Install and) Start the Hale Studio tool</li> <li>Import soil data from the Berlin Soil at https://fbinter.stadt-berlin.de/fb/atom/SO/SO_KrBwBoF2015.zip</li> <li>unzip the file to a new folder</li> <li>In hale studio, at File &gt; Import &gt; Source model, select the soil.xml included in the zip file</li> <li>At File &gt; Import &gt; Source data, select the <code>INSPIRE GML.gml</code> included in the zip file, select default options on the import wizard</li> <li>At File &gt; import &gt; Target model, select an empty database.</li> </ul>"},{"location":"cookbook/hale-studio/","title":"HALE Studio","text":"<p>Status: in progress</p> <p>HALE Studio aims to enable users to set up a harmonization workflow on datasets from a local model to a common model, such as INSPIRE. The user interface presents the model of the source dataset (derived from database) on the left and the target model on the right (derived from xml schema). Conversion rules are defined by selecting similar properties on both sides.</p> <p>The open source software has been developed in the scope of a European Research project, HUMBOLDT (2006) and is currently maintained by a company called WeTransform in Darmstad Germany. WeTransform hosts the Hale Studio user guide and a user forum. The Git repository for Hale Studio is at https://github.com/halestudio/hale.</p> <p>This recipe has been developed in the scope of a Masterclass on data assimilation within the EJP Soil project and builds on harmonization work performed in the scope of the eDanube project.</p> <p>In this recipe we'll harmonize a SOTER database to the INSPIRE model. Read more about the INSPIRE Soil model in the relevant technical guidelines. Some samples of harmonised INSPIRE soil data are available from these data providers: </p> <ul> <li>City of Berlin</li> <li>wielkopolski region in Poland</li> <li>Lihuania (1.6GB)</li> </ul>"},{"location":"cookbook/hale-studio/#contents-of-the-recipe","title":"Contents of the recipe","text":"<ul> <li>SOTER database</li> <li>Preparing the data</li> <li>Install &amp; get started with Hale Studio</li> <li>Define harmonization rules</li> <li>Codelist Mappings</li> <li>Anytype in XSD</li> <li>Export GML</li> <li>Export GeoPackage</li> <li>Validate GML</li> </ul>"},{"location":"cookbook/hale-studio/#soter-database","title":"SOTER Database","text":"<p>For this recipe we're going to use the SOTER database of Cuba. Download the zip file from https://data.isric.org/geonetwork/srv/eng/catalog.search#/metadata/f31ac19f-67a4-4f64-94cc-d4f063ea9add. </p> <p>The SOTER programme was initiated in 1986 by the Food and Agricultural Organization (FAO), the United Nations Environmental Programme and ISRIC, under the auspices of the International Soil Science Society. The aim of the programme was to develop a global SOTER database at scale 1:1 million that was supposed to be the successor of the FAO-UNESCO Soil Map of the World. A SOTER database with global coverage was never achieved, but SOTER databases were developed for various regions, countries and continents.</p> <p>The picture below shows the database structure of a SOTER database. The database structure allows to store terrain, soil profile up to wet chemistry results.</p> <p></p> <p>In this recipe we're focussing on the <code>RepresentativeHorizonValues</code> table mostly, which contains observed properties for each horizon. </p>"},{"location":"cookbook/hale-studio/#preparing-the-data","title":"Preparing the data","text":"<p>Notice that, like many other soil databases, the observed soil property values are listed as columns for each horizon. The INSPIRE model instead uses the Observations and Measurments model, in which each observation is an individual entity which includes the feature of interest (e.g. <code>Horizon 2</code>), the observed property (e.g. <code>pH</code>), the result (e.g. <code>7.2</code>) and a procedure (e.g. <code>pHCaCl2</code>). </p> <p></p> <p>A data transformation required for this step is challenging within Hale, but relatively easy within the database. So before starting up Hale we'll make an initial transformation within the database.</p> <p>The SOTER zip file contains a SQLite as well as a Access version of the database. In this recipe we'll work with Access, but you can also use the SQLite version, in that case install for example SQLite browser to interact with the database. Some of the queries may slightly vary between SQLite and Access.</p> <p>Run the query below, to create a new <code>OBSERVATIONS</code> table. </p> <p>In Access create a new <code>query</code> in <code>design view</code>. </p> <p></p> <p>Then open <code>SQL view</code>.</p> <p></p> <p>Run the query, by clicking <code>Run</code> in the <code>Query design</code> toolbar.</p> <pre><code>SELECT * INTO OBSERVATIONS\nFROM (Select HONU,PRID,'SCMO' as PARAM,SCMO as RESULT from RepresentativeHorizonValues where SCMO is not null union\nSelect HONU,PRID,'SCDR' as PARAM,SCDR as RESULT from RepresentativeHorizonValues where SCDR is not null union\nSelect HONU,PRID,'STGR' as PARAM,STGR as RESULT from RepresentativeHorizonValues where STGR is not null union\nSelect HONU,PRID,'STSI' as PARAM,STSI as RESULT from RepresentativeHorizonValues where STSI is not null union\nSelect HONU,PRID,'STTY' as PARAM,STTY as RESULT from RepresentativeHorizonValues where STTY is not null union\nSelect HONU,PRID,'SDVC' as PARAM,SDVC as RESULT from RepresentativeHorizonValues where SDVC is not null union\nSelect HONU,PRID,'SDCO' as PARAM,SDCO as RESULT from RepresentativeHorizonValues where SDCO is not null union\nSelect HONU,PRID,'SDME' as PARAM,SDME as RESULT from RepresentativeHorizonValues where SDME is not null union\nSelect HONU,PRID,'SDFI' as PARAM,SDFI as RESULT from RepresentativeHorizonValues where SDFI is not null union\nSelect HONU,PRID,'SDVF' as PARAM,SDVF as RESULT from RepresentativeHorizonValues where SDVF is not null union\nSelect HONU,PRID,'SDTO' as PARAM,SDTO as RESULT from RepresentativeHorizonValues where SDTO is not null union\nSelect HONU,PRID,'STPC' as PARAM,STPC as RESULT from RepresentativeHorizonValues where STPC is not null union\nSelect HONU,PRID,'CLPC' as PARAM,CLPC as RESULT from RepresentativeHorizonValues where CLPC is not null union\nSelect HONU,PRID,'PSCL' as PARAM,PSCL as RESULT from RepresentativeHorizonValues where PSCL is not null union\nSelect HONU,PRID,'BULK' as PARAM,BULK as RESULT from RepresentativeHorizonValues where BULK is not null union\nSelect HONU,PRID,'ELCO' as PARAM,ELCO as RESULT from RepresentativeHorizonValues where ELCO is not null union\nSelect HONU,PRID,'SSO4' as PARAM,SSO4 as RESULT from RepresentativeHorizonValues where SSO4 is not null union\nSelect HONU,PRID,'HCO3' as PARAM,HCO3 as RESULT from RepresentativeHorizonValues where HCO3 is not null union\nSelect HONU,PRID,'SCO3' as PARAM,SCO3 as RESULT from RepresentativeHorizonValues where SCO3 is not null union\nSelect HONU,PRID,'EXCA' as PARAM,EXCA as RESULT from RepresentativeHorizonValues where EXCA is not null union\nSelect HONU,PRID,'EXMG' as PARAM,EXMG as RESULT from RepresentativeHorizonValues where EXMG is not null union\nSelect HONU,PRID,'EXNA' as PARAM,EXNA as RESULT from RepresentativeHorizonValues where EXNA is not null union\nSelect HONU,PRID,'EXCK' as PARAM,EXCK as RESULT from RepresentativeHorizonValues where EXCK is not null union\nSelect HONU,PRID,'EXAL' as PARAM,EXAL as RESULT from RepresentativeHorizonValues where EXAL is not null union\nSelect HONU,PRID,'EXAC' as PARAM,EXAC as RESULT from RepresentativeHorizonValues where EXAC is not null union\nSelect HONU,PRID,'CECS' as PARAM,CECS as RESULT from RepresentativeHorizonValues where CECS is not null union\nSelect HONU,PRID,'TCEQ' as PARAM,TCEQ as RESULT from RepresentativeHorizonValues where TCEQ is not null union\nSelect HONU,PRID,'GYPS' as PARAM,GYPS as RESULT from RepresentativeHorizonValues where GYPS is not null union\nSelect HONU,PRID,'P2O5' as PARAM,P2O5 as RESULT from RepresentativeHorizonValues where P2O5 is not null union\nSelect HONU,PRID,'PRET' as PARAM,PRET as RESULT from RepresentativeHorizonValues where PRET is not null union\nSelect HONU,PRID,'FEDE' as PARAM,FEDE as RESULT from RepresentativeHorizonValues where FEDE is not null union\nSelect HONU,PRID,'PHAQ' as PARAM,PHAQ as RESULT from RepresentativeHorizonValues where PHAQ is not null union\nSelect HONU,PRID,'PHKC' as PARAM,PHKC as RESULT from RepresentativeHorizonValues where PHKC is not null union\nSelect HONU,PRID,'SONA' as PARAM,SONA as RESULT from RepresentativeHorizonValues where SONA is not null union\nSelect HONU,PRID,'SOCA' as PARAM,SOCA as RESULT from RepresentativeHorizonValues where SOCA is not null union\nSelect HONU,PRID,'SOMG' as PARAM,SOMG as RESULT from RepresentativeHorizonValues where SOMG is not null union\nSelect HONU,PRID,'SOLK' as PARAM,SOLK as RESULT from RepresentativeHorizonValues where SOLK is not null union\nSelect HONU,PRID,'SOCL' as PARAM,SOCL as RESULT from RepresentativeHorizonValues where SOCL is not null union\nSelect HONU,PRID,'FEPE' as PARAM,FEPE as RESULT from RepresentativeHorizonValues where FEPE is not null union\nSelect HONU,PRID,'ALDE' as PARAM,ALDE as RESULT from RepresentativeHorizonValues where ALDE is not null union\nSelect HONU,PRID,'CLAY' as PARAM,CLAY as RESULT from RepresentativeHorizonValues where CLAY is not null union\nSelect HONU,PRID,'TOTC' as PARAM,TOTC as RESULT from RepresentativeHorizonValues where TOTC is not null union\nSelect HONU,PRID,'TOTN' as PARAM,TOTN as RESULT from RepresentativeHorizonValues where TOTN is not null  \n);\n</code></pre> <p>Verify that a new table <code>OBSERVATIONS</code> is available and that it is populated.</p>"},{"location":"cookbook/hale-studio/#install-hale-studio","title":"Install Hale Studio","text":"<p>Download and install Hale Studio from github. There are installers for windows, linux and apple.  Kate Lyndegaard from WeTransform published a nice overview of Hale Studio at https://www.youtube.com/watch?v=BKNMV-Jp9HM&amp;t=332s.</p> <ul> <li>First create a new project </li> <li>Import the SOTER database as <code>source schema</code>. </li> <li>Import the same database file again as 'source data'. </li> <li>Repeat these 2 steps for the Cuba shapefile, available in the GIS folder of the zip file </li> <li>Load the INSPIRE Soil model as a target schema. Load the latest version of the model from https://inspire.ec.europa.eu/schemas/so/4.0/Soil.xsd (<code>from url</code> tab, click   <code>detect</code> after entering the url).</li> </ul> <p></p>"},{"location":"cookbook/hale-studio/#define-harmonization-rules","title":"Define harmonization rules","text":"<p>We'll go through some cases to highlight some of the features, we'll not produce a full mapping.</p>"},{"location":"cookbook/hale-studio/#join-tables","title":"Join tables","text":"<p>In order to link the geometries from the shapefile to the SOTER data, we'll use a table join. On the left column, select the shapefile as well as the <code>terrain</code>, <code>soils</code> and <code>soilscomponent</code> tables (ctrl-click). On the right colum select the SoilBody type. Now click the blue arrow in the middle and select the <code>join</code> method.</p> <p></p>"},{"location":"cookbook/hale-studio/#link-or-embed-and-identification","title":"Link or embed and identification?","text":"<p>XML allows to embed a property or to reference the value of the property elsewhere. An example; both snippets below have the same meaning, the first is easier to read, the second is easier to handle by software (prevent duplication).</p> <pre><code>&lt;person role=\"student\" name=\"Peter\"&gt;\n    &lt;memberOf&gt;\n        &lt;class name=\"2B\"&gt;\n            &lt;hasMember&gt;\n                &lt;person role=\"teacher\" name=\"Cynthia\"&gt;\n            &lt;/hasMember&gt;\n        &lt;/class&gt;\n    &lt;memberOf&gt;\n&lt;/person&gt;\n</code></pre> <p>And</p> <pre><code>&lt;person role=\"student\" name=\"Peter\" gml:id=\"#student-peter\"&gt;\n    &lt;memberOf xlink:href=\"#class-2b\"&gt;\n&lt;/person&gt;\n&lt;person role=\"teacher\" name=\"Cynthia\" gml:id=\"#teacher-cynthia\"&gt;\n    &lt;memberOf xlink:href=\"#class-2b\"&gt;\n&lt;/person&gt;\n&lt;class name=\"2B\" gml:id=\"#class-2b\"/&gt;\n</code></pre> <p>A good practice is to add reverse links to the second snippet:</p> <pre><code>&lt;class name=\"2B\" gml:id=\"#class-2b\"&gt;\n    &lt;hasMember xlink:href=\"#teacher-cynthia\"/ &gt;\n    &lt;hasMember xlink:href=\"#student-peter\" /&gt;\n&lt;/class&gt;\n</code></pre> <p>Both approaches are supported and can be combined in Hale Studio, but you have to consider upfront which approach to use when. The first approach becomes quite complex if the levels of nesting increase.</p> <p>A suggestion from our side; define Plot, Profile, OM_Observation and Laboratory as root types and embed other types.</p>"},{"location":"cookbook/hale-studio/#codelist-mappings","title":"Codelist Mappings","text":"<p>A common challenge in harmonization is the adoption and extension of common codelists. Hale Studio facilitates the codelist mapping with the possibility to import codelists from the INSPIRE registry and the possibility to define a mapping file to map a local code to a common code. </p> <p>First let's import a codelist from the INSPIRE registry.</p> <ul> <li>In file &gt; import &gt; Codelist, select <code>Import from INSPIRE registry</code></li> <li>Import the SoilProfileParameterNameValue codelist</li> </ul> <p></p> <p>Hale studio supports the INSPIRE registry xml format as well as codelists based on the SKOS ontology. However SKOS codelists should be encoded as RDF/XML. In order to load the GLOSIS codelists, which are encoded as turtle, you can use a python snippet below to convert it to RDF/XML first. </p> <pre><code>from rdflib import Graph\ng = Graph()\ng.parse(\"https://raw.githubusercontent.com/rapw3k/glosis/master/glosis_procedure.ttl\",format='ttl')\ng.serialize(destination='output.xml', format='xml')\n</code></pre> <p>Or use an online conversion tool such as https://issemantic.net/rdf-converter.</p> <p>Now assign a mapping from local values to this codelist.</p> <ul> <li>Select the observed property (bulkdens, organic matter, ...) on the source model</li> <li>On target model select the <code>href</code> attribute of the <code>observedProperty</code> of the <code>OM_Observation</code></li> <li>Click the blue button and select the <code>classification</code> item</li> <li>Proceed to second screen and click the second button <code>Attempt to fill source...</code>, the unique values of the source data are displayed</li> <li>Double click the empty value next to the first value, on the panel select the button to select a codelist</li> <li>Select the codelist, notice the pull down is now populated, select a value from the pulldown</li> <li>Continue the mapping for each of the values</li> </ul> <p></p> <ul> <li>In case you can't find a relevant target value for a source value, then have a look at the extending codelist recipe.</li> </ul>"},{"location":"cookbook/hale-studio/#anytype-in-xsd","title":"Anytype in XSD","text":"<p>XSD allows to leave the type of a property as <code>any</code>. From a standardisation perspective, this is not optimal, because every developer may implement a different type for that field. In the INSPRE Soil theme this challenge is very obvious because the type of the result property of an observation is defined as <code>any</code>. Hale Studio is not able to process <code>anytype</code> fields by default. Instead you have to add below snippet to the <code>eu.esdihumboldt.hale.io.schema.read.target</code> resource of the  <code>project.halex</code> file, to map the any field to a CharacterString. </p> <pre><code> &lt;complex-setting name=\"customTypeContent\"&gt;\n    &lt;xsd:typeContentConfig xmlns:xsd=\"http://www.esdi-humboldt.eu/hale/io/xsd\"&gt;\n        &lt;core:list xmlns:core=\"http://www.esdi-humboldt.eu/hale/core\"&gt;\n            &lt;core:entry&gt;\n                &lt;xsd:association&gt;\n                    &lt;xsd:property&gt;\n                        &lt;core:list&gt;\n                            &lt;core:entry&gt;\n                                &lt;core:name namespace=\"http://www.opengis.net/om/2.0\"&gt;OM_ObservationType&lt;/core:name&gt;\n                            &lt;/core:entry&gt;\n                            &lt;core:entry&gt;\n                                &lt;core:name namespace=\"http://www.opengis.net/om/2.0\"&gt;result&lt;/core:name&gt;\n                            &lt;/core:entry&gt;\n                        &lt;/core:list&gt;\n                    &lt;/xsd:property&gt;\n                    &lt;xsd:config&gt;\n                        &lt;xsd:typeContent mode=\"elements\"&gt;\n                            &lt;xsd:elements&gt;\n                                &lt;core:list&gt;\n                                    &lt;core:entry&gt;\n                                        &lt;core:name namespace=\"http://www.isotc211.org/2005/gco\"&gt;CharacterString&lt;/core:name&gt;\n                                    &lt;/core:entry&gt;\n                                &lt;/core:list&gt;\n                            &lt;/xsd:elements&gt;\n                        &lt;/xsd:typeContent&gt;\n                    &lt;/xsd:config&gt;\n                &lt;/xsd:association&gt;\n            &lt;/core:entry&gt;\n        &lt;/core:list&gt;\n    &lt;/xsd:typeContentConfig&gt;\n&lt;/complex-setting&gt;\n</code></pre>"},{"location":"cookbook/hale-studio/#export-gml","title":"Export GML","text":"<ul> <li>On the File menu, select <code>Export</code> &gt; <code>Transformed data</code>. A panel opens.</li> <li>Select the <code>GML (FeatureCollection)</code> format.</li> <li>Use one of projections suggested by INSPIRE, EPSG:4258 if you're not sure which. Enable the EPSG prefix. </li> <li>Finish the wizard, a GML file will be generated.</li> </ul>"},{"location":"cookbook/hale-studio/#export-geopackage","title":"Export GeoPackage","text":"<p>Hale Studio also facilitates an export to GeoPackage. Hale Studio is able to auto generate a relation data model based on the XSD schema. See also the recipe on GeoPackage.</p> <ul> <li>From the format selection, select the GeoPackage format.</li> <li>Select the relevant projection and finish.</li> <li>Use DBeaver or some other tool to view the outputs. DBeaver has a usefull ER-diagram vizualisation option.</li> </ul>"},{"location":"cookbook/hale-studio/#validate-gml","title":"Validate GML","text":"<p>You can test the generated GML in the INSPIRE Validator.  Select Dataset, and from <code>Annex III</code>, the Soil theme. Solve the <code>numeric riddle</code>, upload the GML file, add a label and start the test.</p>"},{"location":"cookbook/inspire-geoportal/","title":"INSPIRE Geoportal","text":"<p>Status: in progress</p> <p>The INSPIRE Geoportal is the central European access point to the data provided by EU Member States and several EFTA countries under the INSPIRE Directive.</p> <ul> <li>Website: https://inspire-geoportal.ec.europa.eu/</li> <li>GitHub: https://github.com/INSPIRE-MIF/helpdesk-geoportal</li> </ul>"},{"location":"cookbook/inspire-geoportal/#harvesting-metadata","title":"Harvesting metadata","text":"<p><code>Harvest</code> is the process of copying metadata from a remote source (catalogue) to the european GeoPortal. </p> <p>For each memberstate a contact point is assigned, which is responsible for registering the national endpoint(s) to be harvested by the geoportal. The national contact point also triggers the harvest and validates the result before publishing it. At the harvest status page. In case you're interested to have a resource harvested into the INSPIRE GeoPortal you best contact the national contact point to understand the INSPIRE practices in your country. A list of national contact points is available at https://inspire.ec.europa.eu/contact-points/57734. A quick look into the contents of the portal and the harvest status page indicates that countries have quite varying practices on accepting datasets in the geoportal.</p>"},{"location":"cookbook/inspire-geoportal/#metadata-guidelines","title":"Metadata Guidelines","text":"<p>The TG Metadata indicates that INSPIRE datasets and services are described using ISO19139:2007. Discussion is ongoing if the allowed encodings should be extended to include for example DCAT (used by European data portal) or DataCite (used by Zenodo, dataverse, etc). </p>"},{"location":"cookbook/inspire-geoportal/#describe-datasets-using-iso191392007","title":"Describe datasets using iso19139:2007","text":"<p>Various tools exist to support describing datasets using iso19139:2007.</p> <ul> <li>GeoNetwork is a webbased application presenting forms and tools to assist in describing datasets.</li> <li>ArcMap and QGIS include an embedded metadata editor, which is able to export to iso19139:2007 </li> <li>pygeometa is a python library which exports ISO19139:2007 (and other metadata encodings) from a YML encoded format called <code>metadata control file</code> (mcf)</li> </ul>"},{"location":"cookbook/inspire-geoportal/#validate-records-in-the-inspire-validator","title":"Validate records in the INSPIRE validator","text":""},{"location":"cookbook/inspire-geoportal/#validate-links-in-the-link-checker","title":"Validate links in the Link Checker","text":"<p>Operational links are an important aspect in metadata. It determines for example if a user is able to view or download a file after having discovered its metadata. The INSPIRE Geoportal applies some link checks while harvesting the metadata and adds the link check result as tags to the metadata. To optimize your links beforehand the GeoPortal offers the option to run the link checker on an arbitrary metadata record. The Link checker is available at https://inspire-geoportal.ec.europa.eu/linkagechecker.html.</p>"},{"location":"cookbook/inspire-geoportal/#multilingual-metadata","title":"Multilingual metadata","text":"<p>metadata harvested from a national catalogue is usually available in a local language only. To offer users a better user experience, the INSPIRE geoportal translates some key elements of the metadata to english. An automated service is used, which in some cases gives a unexpected translation result. For this reason we encourage you to use the multilingual options of ISO19139:2007 to provide the metadata at least in a local language and english.</p>"},{"location":"cookbook/jmeter/","title":"Capacity testing with jmeter","text":"<p>status: in progress </p> <p>jmeter is a utility which can run a series of performance and capacity tests on a webservice.</p>"},{"location":"cookbook/jmeter/#installation","title":"Installation","text":"<p>Jmeter is a java program, which can run on most platforms (if java is installed). Download the latest version from the apache website. Unzip the archive and run <code>jmeter.bat</code> from bin directory.</p> <p>Jmeter may be quite overwhelming at first, the number of options is high. Follow the tutorials to get introduced to the basic aspects.</p>"},{"location":"cookbook/jmeter/#web-test-plan","title":"Web test plan","text":"<p>Jmeter is typically used to test the performance and capacity of a website or webservice. </p> <ul> <li>Start Jmeter and follow the build web test plan tutorial. </li> <li>Create a web testing plan in jmeter and add a list of sample requests (WMS/WFS/WCS, getcapabilities/getmap/getfeatere etc). </li> </ul> <p></p> <ul> <li>Run the test against the webservice </li> <li>Run the test with multiple users, notice the performance decrease of the service. </li> </ul> <p>Note</p> <p>Do not perform a load test against a production url, it wil severely impact the performance of that service. </p>"},{"location":"cookbook/jmeter/#samples-of-requests","title":"Samples of requests:","text":"<ul> <li>http://localhost:8080/geoserver/wms?request=getcapabilities</li> <li>http://localhost:8080/geoserver/ows?request=getcapabilities&amp;version=2.0.0&amp;service=wfs</li> <li>http://localhost:8080/geoserver/wcs?request=GetCapabilities&amp;version=1.1.1&amp;service=wcs</li> <li>http://localhost:8080/geoserver/gwc/service/wmts?REQUEST=getcapabilities</li> <li>http://localhost:8080/geonetwork/srv/eng/csw?REQUEST=GetCapabilities&amp;version=2.0.1&amp;service=CSW</li> </ul>"},{"location":"cookbook/jmeter/#read-more","title":"Read more","text":"<ul> <li>Website</li> <li>Getting started with a web test plan</li> <li>Docker</li> <li>Jmeter test module included in GeoNetwork</li> </ul>"},{"location":"cookbook/ldproxy/","title":"OGC API Features from a proxied WFS using ldproxy","text":"<p>Status: ready</p> <p>ldproxy is an open source product by interactive instruments. The team has an important role in the development of the suite of new OGC API's and the implementation of ldproxy is an important aspect of that process.</p>"},{"location":"cookbook/ldproxy/#quick-start","title":"Quick start","text":"<ul> <li>Run the image</li> </ul> <p><code>docker run -p7080:7080 -v${PWD}:/ldproxy/data iide/ldproxy</code></p> <ul> <li>Navigate to https://localhost:7080/manager/</li> <li>Login as usr:admin pwd:admin (set new password)</li> <li>You arrive in the 'services' page, create a new service with <code>plus</code> button top right</li> <li>We're setting up LDProxy to act as a proxy to provide OGC API Features over an existing WFS. Select type <code>WFS</code></li> <li>Provide a name and a WFS url (for example https://maps.isric.org/mapserv?map=/map/wosis_latest.map&amp;request=getcapabilities&amp;service=wfs)</li> <li>Click <code>ADD</code>. You return to the list of services, select the one you've just created and click on the <code>home</code> button top right to open it.</li> <li>Click <code>Access the data</code>, select a <code>collection</code> to visualise the items of the collection.</li> </ul> <p>With a tunnel, you can test the local service using inspire OGC API Features validator.</p>"},{"location":"cookbook/ldproxy/#read-more","title":"Read more","text":"<ul> <li>Github</li> <li>Docker</li> <li>Documentation</li> <li>Report testbed Spatial data on the web</li> </ul>"},{"location":"cookbook/link-checker/","title":"INSPIRE Link Checker","text":"<p>Status: in progress</p> <p>A tool to verify links in metadata</p> <ul> <li>Website: https://inspire-geoportal.ec.europa.eu/linkagechecker.html</li> </ul>"},{"location":"cookbook/mapserver/","title":"MapServer","text":"<p>Status: ready</p> <p>MapServer, originally <code>UMN MapServer</code>, is an open source server component which provides OWS services on a variety of data sources. MapServer is commonly used to set up INSPIRE View Services. A detailed guidance on how to use MapServer to set up INSPIRE View Services is available at https://mapserver.org/ogc/inspire.html.</p> <p>MapServer supports WFS and WCS as data exchange mechanisms. MapServer is not able to publish datasets having a hierarchical structure, as common in many INSPIRE datasets, which makes MapServer less suitable to provide INSPIRE download Services using WFS. Stored queries are supported. MapServer can be used to set up a WCS Download service.</p> <p>MapServer runs as a CGI executable. The progream will start up as soon as a request arrives at the server. This makes mapserver very suitable for situations where many datasets are incidentally queried and scales out very well.</p> <p>MapServer is configured using map files. These mapfiles contain metadata for each layer, connection details to the datasource and styling rules for the vizualisation. Various tools exist which create mapfiles automatically, from for example a QGIS layer with GeoCat Bridge. Or by using python script, for example with the mappyfile library. The View services relevant for INSPIRE Soil are described in INSPIRE Data Specification on Soil \u2013 Technical Guidelines in chapter 11. 3 types of layers can be distinguished:</p> <ul> <li>Soil body, Soil profile and Soil Site are vector datasets indicating the location of research area's.</li> <li>Soil properties as vector provide a map view of soil observations on soil profiles or the distribution of a soil property in soil bodies, derived from observations in the area and/or expert judgement.</li> <li>Soil properties as coverage, coverage (grid) is a common output of statistical models which calculate the distribution of a soil property.</li> </ul>"},{"location":"cookbook/mapserver/#the-mapserver-mapfile","title":"The MapServer Mapfile","text":"<p>For this recipe we'll prepare a WMS view service on a Soil Body dataset. For each Soil Body some derived soil properties of the top soil are available.</p> <p>MapServer is configured using map files. These mapfiles contain metadata for each layer, connection details to the datasource and styling rules for the vizualisation. In a typical configuration a user 'calls' the mapserver executable via the web, while indicating the relevant mapfile. For example:</p> <pre><code>https://example.org/mapserv.cgi?map=/data/soilbody.map&amp;service=WMS&amp;request=GetCapabilities\n</code></pre> <p>Various tools exist which create mapfiles automatically, from for example a QGIS layer. See for example the GeoCat Bridge software.</p> <p>In this recipe we'll assemble the mapfile in a text editor. For some of the more advanced text editors, such as Visual Studio Code, mapfile editing plugins are available, which provide validation and syntax highlighting.</p> <p>A generic mapfile Quick Start is provided at https://live.osgeo.org/en/quickstart/mapserver_quickstart.html. The quickstart is based on OSGeo Live, a virtual DVD, which offers a preinstalled mapserver and has data from Natural Earth.</p> <p>Within the mapfile, created in the Quickstart, let's replace some metadata and update the natural earth layer to point to our soil body dataset.</p> <pre><code>MAP\n  NAME \"SOILBODY_QUICKSTART\"\n  EXTENT -180 -90 180 90\n  UNITS DD\n  SHAPEPATH \"/home/user/data/\"\n  SIZE 800 600\n\n  IMAGETYPE PNG24\n\n  PROJECTION\n    \"init=epsg:4326\"\n  END\n\n  WEB\n    METADATA\n      ows_title \"Soil Body Quickstart\"\n      ows_enable_request \"*\"\n      ows_srs \"EPSG:4326 EPSG:25832 EPSG:25833\"\n    END\n  END\n\n  LAYER\n    NAME \"Soilbody\"\n    STATUS ON\n    TYPE POLYGON\n    DATA \"soilbody\"\n    CLASS\n      STYLE\n        COLOR 246 241 223\n        OUTLINECOLOR 0 0 0\n      END\n    END\n  END\n\nEND\n</code></pre>"},{"location":"cookbook/mapserver/#mapserver-via-docker","title":"MapServer via Docker","text":"<p>MapServer requires a number of dependencies, which may be hard to install on some systems, that's why this recipe suggests to work with Docker containers which are prepared to run mapserver.</p> <p>The camp2camp mapserver image is a commonly used mapserver container image. While starting the container we provide a number of parameters so the container is able to locate the mapfile and the data files.</p> <pre><code>docker run -p=80:80 \\\n    -v=$(PWD)/soilbody.map:/etc/mapserver/wms.map \\\n    -v=$(PWD)/data:/home/user/data/ \\\n    camptocamp/mapserver\n</code></pre> <p>For a local installation of mapserver, options vary by platform</p> <ul> <li>Windows; install ms4w </li> <li>Apple; brew install mapserver</li> <li>Debian </li> <li>Ubuntu </li> </ul>"},{"location":"cookbook/mapserver/#testing-a-mapfile","title":"Testing a mapfile","text":"<p>Sometimes mapserver reports an error while generating a map response. If the logs of the container do not provide enough information, mapserver provides an interesting utility map2img to validate a mapfile. To run the utility, you have to open bash on the container: </p> <pre><code>docker exec -it &lt;container&gt; /bin/bash \n</code></pre> <p>and then run the utily:</p> <pre><code>map2img -m local.map -o test2.png\n</code></pre> <ul> <li><code>-m</code> references the mapfile</li> <li><code>-o</code> references an output file to be generated</li> </ul>"},{"location":"cookbook/mapserver/#wrb-layer-with-geocat-bridge","title":"WRB layer with GeoCat Bridge","text":"<p>The technical guidance provides quite detailed instructions on how to style the relavant soil layers. For example a SO.SoilBody.WRB is described with dedicated colors for each WRB Soil type.</p> WRB RSG Code WRB RSG Name Colour RGB code Colour HEX code AC Acrisol (247, 152, 4) #F79804 AB Albeluvisol (254, 194, 194) #FEC2C2 AL Alisol (255, 255, 190) #FFFFBE .. .. .. .. <p>If many style rules are involved (or if your project already has styling) a tool like GeoCat Bridge is helpfull. Read more about GeoCat Bridge in the recipe Bridge and GeoServer. On QGIS, with the GeoCat Bridge plugin installed, load a SoilBody dataset and assign some of the colors. In the web &gt; bridge menu, activate the Style viewer panel. Notice the various tabs in the panel which represent the layer style in various encodings. The first tab contains Styled Layer Descriptor (SLD), a standardised styling format, used for example in GeoServer. The second tab presents the mapfile syntax, you can copy the value into your mapfile (or let Bridge generate a full mapfile).</p> <p></p> <p>An alternative option for creating mapfiles is the geostyler library. A NodeJS application which is able to read and write various styling formats.</p> <p>A web application mapserver studio is in preparation which will be able to create a mapfile in a web environment.</p>"},{"location":"cookbook/mapserver/#mapserver-for-wfs-ogc-api-features-wcs","title":"MapServer for WFS, OGC API Features &amp; WCS","text":"<p>MapServer includes support for WFS, OGC API Features (8+) and WCS. Note that WFS does not support the hierarchal data models as required for INSPIRE, only <code>flat</code> tables are supported.</p>"},{"location":"cookbook/mapserver/#mapserver-and-wmts","title":"MapServer and WMTS","text":"<p>MapServer does not provide tile services (WMTS) itself, but is often combined with a separate tool, mapcache, which provides tile service on top of a MapServer instance. Tile services are generally a safer option with respect to Quality of Service, but less dynamic in update and styling options. </p>"},{"location":"cookbook/mapserver/#validate-a-mapserver-wms-as-inspire-view-service","title":"Validate a Mapserver WMS as INSPIRE View service","text":"<p>You need to set up a tunnel so the INSPIRE validator will be able to assess your local service. </p>"},{"location":"cookbook/mapserver/#read-more","title":"Read more:","text":"<p>At masterclass edition 2023 Seth G presented MapServer.</p> <ul> <li>Website</li> <li>GitHub</li> <li>Docker</li> <li>OSGeo</li> </ul>"},{"location":"cookbook/postgraphile/","title":"GraphQL with Postgraphile","text":"<p>Status: in progress</p> <p>GraphQL is a de-facto standard for self describing API's on hierarchical data. Graphql provides via its API specification capabilities to query datasets using filters, but also indicate which properties to be returned alongside with proper pagination. The GraphQL website provides a quick start on Graphql based on NodeJS.</p> <p>GraphQL is currently not endorsed as an INSPIRE Good Practice, but it fits many aspects of an INSPIRE download service. GraphQL is a good fit to disseminate measurement and observation data from soil profiles.</p> <p>At ISRIC we use GraphQL (along WMS/WFS) to disseminate the WOSIS soil profile database.</p>"},{"location":"cookbook/postgraphile/#postgraphile","title":"Postgraphile","text":"<p>Postgraphile is a NodeJS server application which creates a GraphQL API on any postgres database. Besides being  totally compatible with PostgreSQL it also supports PostGIS and therefore it can easily work as a Spatial GraphQL API.</p>"},{"location":"cookbook/postgraphile/#load-some-data-into-the-database","title":"Load some data into the database","text":""},{"location":"cookbook/postgraphile/#configure-the-service","title":"Configure the service","text":""},{"location":"cookbook/postgraphile/#query-a-graphql-endpoint","title":"Query a GraphQL endpoint","text":""},{"location":"cookbook/postgraphile/#read-more","title":"Read more","text":"<ul> <li>postgraphile</li> <li>workshop</li> </ul>"},{"location":"cookbook/pycsw/","title":"pycsw","text":"<p>Status: in progress</p> <p>An open source python catalogue implementation, used within CKAN-spatial and GeoNode.</p> <p>Supports CSW (v2 and v3) and OGC API Records. Uses a fixed metadata model and has various metadata model output formats. Capability to harvest metadata from remote sources (CSW, WMS, WFS, SOS, etc)</p>"},{"location":"cookbook/pycsw/#quick-start","title":"Quick start","text":"<pre><code>docker run -d -name pycsw -p 8000:8000 geopython/pycsw\n</code></pre> <ul> <li>Visit https://localhost:8000</li> <li>Prepare a folder of iso19139 files, mount it on the container and load it with pycsw-admin.py</li> </ul>"},{"location":"cookbook/pycsw/#read-more","title":"Read more","text":"<p>At masterclass edition 2023 Tom Kralidis presented the geopython ecosystem, including pycsw.</p> <ul> <li>Website</li> <li>Documentation</li> <li>GitHub</li> <li>Docker</li> <li>Demo server</li> <li>OSGeo</li> </ul>"},{"location":"cookbook/pygeoapi/","title":"OGC API Features with pygeoapi","text":"<p>Status: in progress</p> <p>In this recipe we'll set up an instance of pygeoapi with some soil data. A good practice is available on how to provide an INSPIRE download service based on OGC API Features. pygeoapi is an open source python server implementation of OGCAPI Features, Tiles, Maps, Coverages, Records and Processes.</p>"},{"location":"cookbook/pygeoapi/#quick-start","title":"Quick start","text":"<p>The recipe is based on Docker. New to docker? Read more in the Docker recipe.</p> <ul> <li>With commandline in a new folder, run this command:</li> </ul> <pre><code>docker run -p 5000:80 geopython/pygeoapi:latest\n</code></pre> <ul> <li>Navigate with your browser to http://localhost:5000 </li> </ul> <p>If all went fine, you now see the default pygeoapi installation with sample data. In the next step we'll publish a new soil dataset. pygeoapi's configuration is stored in a config file. The config file is encoded as YAML. The first part configures the main settings of the service, in the second part individual datasets are configured.</p> <ul> <li>Download the Dutch INSPIRE dataset 'soil drills' from https://service.pdok.nl/bzk/brobhrpvolledigeset/atom/v1_1/downloads/brobhrpvolledigeset.zip</li> <li>Unzip the file to the work folder</li> <li>Create a local config file in the folder, download it from here</li> <li>Remove all the datasets from the config folder, and replace it for the following:</li> </ul> <pre><code>resources:\n    bro:\n        type: collection\n        title: BRO\n        description: Bro soil drills\n        keywords:\n            - soil\n        links:\n            - type: application/geopackage+sqlite\n              rel: canonical\n              title: source data\n              href: https://service.pdok.nl/bzk/brobhrpvolledigeset/atom/v1_1/downloads/brobhrpvolledigeset.zip\n              hreflang: en-US\n        extents:\n            spatial:\n                bbox: [-180,-90,180,90]\n                crs: http://www.opengis.net/def/crs/OGC/1.3/CRS84\n        providers:\n            -   type: feature\n                name: OGR\n                data:\n                    source_type: GPKG\n                    source: /pygeoapi/brobhrpvolledigeset.gpkg\n                    gdal_ogr_options:\n                        SHPT: POINT\n                id_field: bro_id\n                layer: borehole_research\n</code></pre> <ul> <li>Mount config file in containter</li> </ul> <pre><code>docker run -p 5000:80 -v ${PWD}/pygeoapi-config.yml:/pygeoapi/local.config.yml -v ${PWD}/brobhrpvolledigeset.gpkg:/pygeoapi/brobhrpvolledigeset.gpkg  geopython/pygeoapi:latest\n</code></pre>"},{"location":"cookbook/pygeoapi/#validation","title":"Validation","text":"<p>JRC recently extended the INSPIRE validator. It can now also validate an OGC API Features service. Because docker runs locally, you need to set up a tunnel for the validator to access the local service. Read the tunnel recipe to see how to do that.</p>"},{"location":"cookbook/pygeoapi/#read-more","title":"Read more","text":"<p>At masterclass edition 2023 Tom Kralidis presented the geopython ecosystem, including pygeoapi.</p> <p>The geopython community has prepared a workshop on getting started with pygeoapi.</p> <ul> <li>Website</li> <li>GitHub</li> <li>Docker</li> <li>Demo server</li> <li>Documentation</li> <li>OSGeo </li> </ul>"},{"location":"cookbook/pygeometa/","title":"A Pythonic metadata workflow","text":"<p>status: in progress</p> <p>This recipe presents a minimalistic, however integrated and standardised approach to metadata management. Each data file on a file system will be accompagnied by a minimal YML metadata file. Crawler scripts will pick up these metadata files and publish them as iso19139 (or alternative models) on a catalogue. iso19139 is the metadata model currently mandated by INSPIRE and very common in the GeoSpatial domain. Other communities tend to use different standards, such as STAC (Earth Observation), DCAT (Open Data), DataCite (Academia), etc.</p> <p>The recipe introduces you to a pythonic metadata workflow step by step.</p>"},{"location":"cookbook/pygeometa/#initial","title":"Initial","text":"<p>The inital step assumes a folder of data files on a network drive, sharepoint or git repository. Datasets stored on a database will not be considered for now, but can follow a similar workflow.</p> <p>For each data file in the folder we will create a <code>metadata control file</code> (MCF). MCF is a convention from the pygeometa community. It is a YAML encoded subset of iso19139:2007. YAML is easy to read by humans and an optimal for content versioning (in git).</p> <p>Consider to set up a virtual environment for the workshop:</p> <pre><code>virtualenv pygeometa &amp;&amp; cd pygeometa &amp;&amp; . bin/activate\n</code></pre> <p>Then install the pygeometa library.</p> <pre><code>pip install pygeometa\n</code></pre>"},{"location":"cookbook/pygeometa/#create-an-mcf","title":"Create an MCF","text":"<p>A minimal example of MCF is (see also a more extended version):</p> <pre><code>mcf:\n    version: 1.0\n\nmetadata:\n    identifier: 3f342f64-9348-11df-ba6a-0014c2c00eab\n    language: en\n    hierarchylevel: dataset\n    datestamp: 2023-01-01\n\nspatial:\n    datatype: grid\n\nidentification:\n    language: eng\n    title: Soilgrids sample Dataset\n    abstract: This is a sample dataset for the EJP Soil Dataset Assimilation Masterclass\n    dates:\n        creation: 2023-01-01\n    keywords:\n        default:\n            keywords: [\"sample\"]\n    topiccategory:\n        - geoscientificInformation\n    extents:\n        spatial:\n            - bbox: [2,50,4,52]\n              crs: 4326\n    fees: None\n    accessconstraints: otherRestrictions\n    rights: CC-BY\n\ncontact:\n    pointOfContact: \n        organization: ISRIC - World Soil Information\n        url: https://www.isric.org\n        city: Wageningen\n        country: The Netherlands\n        email: info@isric.org\n\ncontent_info:\n    type: image\n    dimensions:\n        - name: N\n          units: g/m3\n          min: 10\n          max: 75\n        - name: P\n          units: mg/m3\n          min: 30\n          max: 75\n        - name: K\n          units: mg/m3\n          min: 0.5\n          max: 1.2\n\ndistribution:\n    wms:\n        url: https://maps.isric.org\n        type: OGC:WMS\n        rel: service\n        name: soilgrids\n</code></pre> <ul> <li>Use the above template to create a metadata file for a dataset. The file should have the same name as the dataset, but with an extension <code>.yml</code> or <code>.mcf</code>.</li> </ul>"},{"location":"cookbook/pygeometa/#import-existing-metadata","title":"Import existing metadata","text":"<ul> <li>If the data file already has a metadata document (for example with a shapefile, if it contains a file with extension .shp.xml), you can try to import it using pygeometa. pygeometa requires to indicate the metadata schema in advance.</li> </ul> <p>For iso19139:2007 use:</p> <pre><code>pygeometa metadata import path/to/file.xml --schema=iso19139\n</code></pre> <p>For fgdc (typically used with shapefiles) use: </p> <pre><code>pygeometa metadata import path/to/file.xml --schema=fgdc\n</code></pre>"},{"location":"cookbook/pygeometa/#generate-iso191392007","title":"Generate iso19139:2007","text":"<p>As soon as you have a folder of MCF's, you can use <code>pygeometa generate</code> to convert them to iso19139:2007.</p> <pre><code>pygeometa metadata generate path/to/file.yml --schema=iso19139 --output=some_file.xml\n</code></pre> <p>Or for a folder of files:</p> <pre><code>FILES=\"/path/to/*.yml\"\nfor f in $FILES\ndo\n  echo \"Processing $f file...\"\n  pygeometa metadata generate $f --schema=iso19139 --output=$f.xml\ndone\n</code></pre> <p>Notice that you can also create your own template for the iso19139 generation. By using a customised template you're able to optimise the generated iso19139 records to facilitate for example better INSPIRE complience.</p> <pre><code>pygeometa metadata generate path/to/file.yml --schema_local=/path/to/my-schema --output=some_file.xml\n</code></pre>"},{"location":"cookbook/pygeometa/#import-generated-metadata-to-pycsw","title":"Import generated metadata to pycsw","text":"<p>pycsw is a python based OGC reference implementation of Catalog Service for the Web and an early adaptor of OGC API Records and STAC Catalog. We'll use pycsw via a docker image to publish the metadata records in a search service. We run it in <code>detach</code> mode so we can interact with the running container, type <code>docker stop pycsw</code> to stop the container.</p> <pre><code>docker run -d --rm --name pycsw -p 8000:8000 geopython/pycsw\n</code></pre> <p>We now have a running pycsw at http://localhost:8000/collections with some sample data. We will now remove the sample data and insert our metadata. For that reason we <code>mount</code> our current folder with xml files into the container</p> <pre><code>docker stop pycsw\ndocker run -d --rm --name pycsw -v ${PWD}:/metadata -p 8000:8000 geopython/pycsw\n</code></pre> <p>Now we can trigger pycsw admin to remove the default records and import our metadata. As part of the calls we reference the config file, which contains the connection details to the database.</p> <pre><code>docker exec -ti pycsw pycsw-admin.py delete-records -c /etc/pycsw/pycsw.cfg\ndocker exec -ti pycsw pycsw-admin.py load-records -c /etc/pycsw/pycsw.cfg -p /metadata -r\n</code></pre> <p>Check out the new content at http://localhost:8000/collections. Note that if you restart the container, all records are removed, because the database is currently not persisted on a volume.</p> <p>Try to mount also a customised configuration file into the container, so you can optimise the configuration of the catalogue. Also have a look at the INSPIRE extension for pycsw.</p>"},{"location":"cookbook/pygeometa/#evaluate-metadata-and-discovery-service","title":"Evaluate Metadata and Discovery Service","text":"<p>You can evaluate individual iso19139 records in the INSPIRE reference validator. Also you can evaluate the discovery service. If a service is running on localhost, use the tunnel approach to evaluate it.</p>"},{"location":"cookbook/pygeometa/#access-the-service-from-qgis","title":"Access the service from QGIS","text":"<p>QGIS contains a default plugin called MetaSearch which enables catalogue searches from within QGIS. You can find the plugin in the <code>web</code> menu or on the toolbar as a set of binoculars. Open the plugin. First you need to set up a new service connection. On the services tab, click new, choose a name and add the url http://localhost:8080/csw. Click the <code>serviceinfo</code> button to view the metadata of the service. Now return to the <code>Search</code> tab and perform a search. Notice that if you select a search result, it highlights on the map and may trigger the <code>Add data</code> button in the footer (this depends on if QGIS recognises the protocol mentioned in the metadata). </p>"},{"location":"cookbook/pygeometa/#read-more","title":"Read more","text":"<p>At masterclass edition 2023 Tom Kralidis presented the geopython ecosystem, including pycsw.</p> <ul> <li>github the geopython community welcomes your questions and contributions.</li> <li>pygeometa</li> <li>pycsw</li> <li>pyGeoDataCrawler is a set of scripts to manage MCF's. It supports importing MCF from a CSV, MCF inheritence, generate MCF from a data file, etc.</li> <li>Model Driven Metadata Editor A web based GUI for populating MCF's.</li> </ul>"},{"location":"cookbook/qgis/","title":"View services with QGIS server","text":"<p>Status: done</p> <p>QGIS started as a GIS desktop application. In recent years the community prepared a server edition of QGIS. The server provides WMS, WFS and since recently OGC API Features. </p> <p>The advantage of using QGIS both as a desktop and server component is that the maps generated on the server will display exactly the same as those prepared on a desktop client.</p>"},{"location":"cookbook/qgis/#prepare-a-view-service-in-qgis-desktop","title":"Prepare a view service in QGIS Desktop","text":"<p>Use an existing QGIS project, or create a new project with some sample data. If you include any data, place it in a folder within the current folder, so docker can access it as a docker mounted volume.</p> <p>Name your project <code>project.qgs</code>.</p> <p>On project settings, open the WMS properties to add relevant metadata.</p>"},{"location":"cookbook/qgis/#publish-a-view-service","title":"Publish a view service","text":"<p>Navigate with a console application to the folder which contains <code>project.qgs</code>.</p> <p>We're running QGIS server as a docker container based on https://hub.docker.com/r/camptocamp/qgis-server. Run the QGIS container:</p> <pre><code>docker run -p 8080:80 --volume=$PWD:/etc/qgisserver camptocamp/qgis-server\n</code></pre> <p>Try the service via:</p> <pre><code>http://localhost:8080/?SERVICE=WMS&amp;REQUEST=GetCapabilities\n</code></pre>"},{"location":"cookbook/qgis/#validate-the-view-service","title":"Validate the view service","text":"<p>In order for the INSPIRE validator to access your local service, you need to set up a tunnel.</p>"},{"location":"cookbook/qgis/#qgis-desktop-as-a-client-for-inspire-data","title":"QGIS Desktop as a client for INSPIRE data","text":"<p>QGIS can act as a client for WMS, WMTS, WCS, CSW, Sensorthings API, OGC API Records and OGC API Features.</p> <p>The QGIS data model is based on data layers and has minimalistic support for joins. QGIS is therefore less optimal for rich GML data as used in INSPIRE. There have been initiatives to bring hierarchical data to QGIS, such as GMLAS but adoption has been limited. The Application Schema functionality has been introduced in GDAL/OGR, the QGIS plugin builds on top of it. OGR converts the contents of the GML to a relational database. Individual tables from that database are loaded in the QGIS interface.</p>"},{"location":"cookbook/rasdaman/","title":"Coverages with rasdaman","text":"<p>Coverage data is the digital representation of some spatio-temporal phenomenon. Usually in the form of a grid of cells having a certain resolution. Grid cells can provide a value for multiple phenomena. The OGC Web Coverage Service and the upcoming OGC API Coverages provide a standardised mechanism to query coverage data over the web. Web Coverage Service is the main option to provide a Download service on INSPIRE coverage data. An alternative option is INSPIRE Atom.</p> <p>Rasdaman is software to set up a Web Coverage Service. The Rasdaman team has prepared a tutorial for setting up an INSPIRE Coverage service at Good practice on coverage data. The tutorial is set up using Jupyter. New to Jupyter? Read the Jupyter recipe.</p>"},{"location":"cookbook/rasdaman/#read-more","title":"Read more:","text":"<p>At masterclass edition 2023 Kathi Schleidt presented INSPIRE CSW using Rasdaman.</p> <ul> <li>Website</li> <li>Docker</li> <li>Sources</li> <li>Tickets</li> <li>OsGeo</li> <li>Rasdaman &amp; INSPIRE</li> </ul>"},{"location":"cookbook/re3gistry/","title":"Re3gistry","text":"<p>Status: contribution needed</p> <p>Re3gistry is a tool to manage and share reference codes.  It provides a central access point that allows labels and descriptions for reference codes to be easily looked up by humans or retrieved by machines. It supports organisations in managing and updating reference codes consistently in a well governed manner so that all versions of a code remain traceable and adequately documented over time.</p> <p>The Re3gistry development started under the ISA Are3na action. It continues under the ISA2 ELISE action. The main user of the software is the INSPIRE Registry. Although the software has been implemented in various regional instances.</p> <p>When you need to manage and share reference codes within and between organisations, the Re3gistry solution can support you. These reference codes uniquely define sets of permissible values for a data field or provide context for the exchanged data. Examples of reference codes are simple enumerations and flat lists to complex controlled vocabularies, taxonomies, thesauri.</p>"},{"location":"cookbook/re3gistry/#registry-in-relation-to-other-software","title":"Registry in relation to other software","text":"<p>Tools such as GeoNetwork and Hale studio are able to import code lists from Re3gistry software. Which are then used within the software to populate pull down menus.</p>"},{"location":"cookbook/re3gistry/#registry-and-the-semantic-web","title":"Registry and the semantic web","text":"<p>The background of Re3gistry is in the geospatial domain (iso19135). Hoewever there are efforts to integrate with the semantic web. Each codelist can for example be exported as a SKOS RDF, for example in an rdf/xml format</p>"},{"location":"cookbook/re3gistry/#read-more","title":"Read more","text":"<ul> <li>Community at GitHub</li> <li>INSPIRE registry</li> <li>Solution at ISA2</li> <li>User manual</li> </ul>"},{"location":"cookbook/rml/","title":"RML.io","text":"<p>RML.io is a toolset for the generation of knowledge graphs. They automate the creation of RDF from diverse data sources, primarily unstructured tabular data. </p> <p>RML.io has programmes to be used on-line and to be installed on computer systems (Linux, MacIntosh and Windows platforms are supported). The former are useful for prototyping, whereas the latter are meant for actual transformations of large datasets.</p>"},{"location":"cookbook/rml/#the-yarrrml-syntax","title":"The YARRRML syntax","text":"<p>The RML tools apply data transformations according to a set of rules recorded in a YAML file. This file must respect a specific syntax, named YARRRML. This specification defines a number of sections (or environments) in the YAML file that lay out the structure of the resulting triples.</p> <p>The first of these sections is named <code>prefixes</code> and provides the space for the definition of URI abbreviations, in all similar to the Turtle syntax. Each abbreviation is encoded as a list item and can be used in the reminder of the YARRRML as it would be in a Turtle knowledge graph.</p> <pre><code>prefixes:\n rdf: http://www.w3.org/1999/02/22-rdf-syntax-ns#\n xsd: http://www.w3.org/2001/XMLSchema#\n geo: http://www.opengis.net/ont/geosparql#\n</code></pre> <p>Next comes the <code>mappings</code> section, where the actual transformations are encoded. This section is to be populated with sub-sections, one for each individual subject class (or type) necessary in the output RDF. For instance, if the transformation must produce triples for profiles and layers, then a sub-suction for each is necessary. The name of these subject sub-sections is arbitrarily chosen by the user.</p> <pre><code>mappings:\n  profile:\n\n  layer:\n</code></pre> <p>For each subject class sub-section at least one data source needs to be specified in the <code>sources</code> section. The source can be declared within square brackets (i.e. a YAML collection), providing a path to a file followed by a tilde and then a type. The sources section can be more intricate, as YARRRML supports a wide range of different data sources, including flat tables, databases and Web APIs. </p> <pre><code>mappings:\n  profile:\n    sources:\n      - ['SoilData.csv~csv']\n</code></pre> <p>The following sub-section of the class declares the subject and has the simple name of <code>s</code>. Its purpose is to define the URI structure for the instances of the class. In principle this is also the first element that makes reference to the contents of the source file. In the case of CSV, as in this example, the column names are used. They are invoked using the dollar character (<code>$</code>), with the column name within parenthesis. The practical result is the generation of an individual element (subject in this case) for each distinct value found in the source column. </p> <pre><code>  profile:\n    sources:\n      - ['SoilData.csv~csv']\n    s: http://my.soil.org#$(profile_id)\n</code></pre> <p>With the subject defined, triples can be completed with predicates and objects in sub-section <code>po</code>. This section is itself composed by a list, whose items comprise a pair: predicate (item <code>p</code>) and object (item <code>o</code>). The predicate is encoded as a URI in a similar way to the subject, using abbreviations if necessary. As for the object it can be decomposed further into a <code>value</code> and a <code>datatype</code> to accommodate literals.   </p> <p>The example below creates triples for the layer class subject, using the <code>layer_id</code> column in the source to generate subject URIs. The source column <code>layer_order</code> is used to complete triples declaring the order of a layer within a profile.</p> <pre><code>prefixes:\n xsd: http://www.w3.org/2001/XMLSchema#\n iso28258: http://w3id.org/glosis/model/iso28258/2013#\n\nmappings:\n  layer:\n    sources:\n      - ['SoilData.csv~csv']\n    s: http://my.soil.org#$(layer_id)\n    po:\n      - p: iso28258:ProfileElement.order\n        o:\n           value: \"$(layer_order)\"\n           datatype: xsd:integer\n</code></pre> <p>The encoding of the predicates and objects list can be shortened with collections. Instead of discriminating value and datatype, they can be expressed as elements of a collection. This formulation is useful when the object is itself a URI. Note how in the example below (for the layer class) the tilde is used again, to indicate the object type. </p> <pre><code>    po:\n      - [iso28258:Profile.elementOfProfile, http://my.soil.org#$(layer_id)~iri]\n</code></pre> <p>This was just a brief introduction to the YARRRML syntax. It goes far deeper, even allowing for some functional programming. While the guidelines in this document make enough of a start to automated RDF generation, the documentation is indispensable to take full advantage of the RML tool set.</p>"},{"location":"cookbook/rml/#matey","title":"Matey","text":"<p>The simplest way to start using RML.io is through the Matey online user interface. It is an excellent prototyping tool and will help you getting acquainted with the YARRRML syntax.</p> <p>The standard view of Matey has 4 sections:</p> <ul> <li>a section for input data;</li> <li>a section to define YARRRML rules;</li> <li>a section to display RDF output;</li> <li>a section to visualise exported RML.io rules from YARRRML.</li> </ul> <p>There are various examples available to guide you through the basics of YARRRML and RML. Take some time to experiment with these examples, try modifying the output, or even to create further transformation rules.  </p> <p>Eventually you will find the limitations of Matey, while convenient for prototyping, it does not scale for large datasets or to process a large number of source files. For that you need to use the command line interface.</p>"},{"location":"cookbook/rml/#install","title":"Install","text":"<p>Using RML.io in your system requires two programmes, a parser for the YARRRML syntax (<code>yarrrml-parser</code>) and a transformer that converts tabular data to RDF (<code>rmlmapper</code>).</p> <p>The first of these programmes is installed with <code>npm</code>:</p> <pre><code>npm i -g @rmlio/yarrrml-parser\n</code></pre> <p><code>rmlmapper</code> is a Java programme, that can be downloaded directly from the project GitHub page. For instance:</p> <pre><code>wget https://github.com/RMLio/rmlmapper-java/releases/download/v6.1.3/rmlmapper-6.1.3-r367-all.jar\n</code></pre> <p>It can then be run with the Java Runtime Environment:</p> <pre><code>java -jar rmlmapper-6.1.3-r367-all.jar\n</code></pre> <p>At this stage it might be useful to create a shortcut to call the programme with a simple command like <code>rmlmapper</code>. How to do this depends on your system and is beyond the scope of this document.</p>"},{"location":"cookbook/rml/#how-to-use","title":"How to use","text":"<p>Note: before starting a data transformation into RDF you must devise a URI policy for your data. Please refer to the URI Policy document for details.</p> <p>The file SoilData.csv contains a simple set of hypothetical measurements referring to three soil profiles collected in two different sites. The goal is to transform this dataset into GloSIS compliant RDF.</p> <pre><code>site_id,lat,lon,profile_id,layer_id,upper_depth,lower_depth,pH,SOC,\n1,49.43,8.31,1,11,0,15,7.4,6,\n1,49.43,8.31,1,12,15,40,7.2,4,\n1,49.43,8.31,2,21,0,10,8,3,\n1,49.43,8.31,2,22,10,30,8.1,2,\n2,46.82,11.45,3,31,0,15,6.8,1,\n2,46.82,11.45,3,32,15,30,6.7,1,\n2,46.82,11.45,3,33,30,60,6.7,0,\n</code></pre>"},{"location":"cookbook/rml/#profiles","title":"Profiles","text":"<p>The simplest place to start is with the profiles. There are three essential elements to generate for each profile:   - A new URI for the profile;   - The declaration of the new profile as an instance of the class <code>GL_Profile</code>;   - The association with the respective site. </p> <p>Below are the contents of the file profile.yarrrml that encodes this transformation. Note how the URIs of both the profile and the site are created using the prefixes.</p> <pre><code>prefixes:\n wosis_prf: http://wosis.isric.org/profile#  \n wosis_sit: http://wosis.isric.org/site#\n glosis_pr: http://w3id.org/glosis/model/profile# \n iso28258: http://w3id.org/glosis/model/iso28258/2013#\n\nmappings:\n  profile:\n    sources:\n      - ['SoilData.csv~csv']\n    s: wosis_prf:$(profile_id)\n    po:\n      - [a, glosis_pr:GL_Profile]\n      - [iso28258:Profile.profileSite, wosis_sit:$(site_id)~iri]\n</code></pre> <p>To perform the actual transformation you must first apply <code>yarrrml-parser</code> to create the RML transformation file and then use <code>rmlmapper</code> to obtain the actual knowledge graph. By default <code>rmlmapper</code> creates a Turtle file that is printed to the standard output (STDOUT). You can use the parameters <code>-o</code> to redirect output to a text file and <code>-s</code> to select an alternative serialisation syntax.</p> <pre><code>yarrrml-parser -i profile.yarrrml -o profile.rml.ttl\nrmlmapper -s turtle -m profile.rml.ttl\n</code></pre>"},{"location":"cookbook/rml/#sites","title":"Sites","text":"<p>Sites are the spatial features in the knowledge graph, therefore they require the creation of appropriate GeoSPARQL instances. Three new elements must be addressed in this transformation:   - Declaration of the site as an instance of the class <code>geo:Feature</code>;   - Creation of a <code>geo:Geometry</code> instance to host the actual geo-spatial     information;   - A literal of the type <code>geo:wktLiteral</code> or <code>geo:gmlLiteral</code> to encode the geometry.</p> <p>The file site.yarrrml achieves this transformation. Its contents are reproduced below:</p> <pre><code>prefixes:\n geo: http://www.opengis.net/ont/geosparql#\n glosis_sp: http://w3id.org/glosis/model/siteplot# \n wosis_sit: http://wosis.isric.org/site#\n wosis_geo: http://wosis.isric.org/geometry#\n\nmappings:\n  site:\n    sources:\n      - ['SoilData.csv~csv']\n    s: wosis_sit:$(site_id)\n    po:\n      - [a, glosis_sp:GL_Site]\n      - [a, geo:Feature]\n      - [geo:hasGeometry, wosis_geo:$(site_id)~iri]\n\n  geometry:\n    sources:\n      - ['SoilData.csv~csv']\n    s: wosis_geo:$(site_id)\n    po:\n      - [a, geo:Point]\n      - p: geo:asWKT\n        o:\n           value: \"POINT($(lon) $(lat))\"\n           datatype: geo:wktLiteral\n</code></pre> <p>This example also shows the inclusion of two different classes in the same transformation. Note how the Feature is associated with the geometry using the <code>geo:hasGeometry</code> object property. Also important is the creation of the WKT literal, as it requires a verbose declaration of the object to make the type explicit.</p>"},{"location":"cookbook/rml/#layers","title":"Layers","text":"<p>Having created a transformation for the sites, one for the layers is not much of a challenge. Download the file layer.yarrrml and try it yourself. </p> <p>Look carefully at the transformation file, note how the object properties from the ISO28258 module are used to declare the layers depths.</p> <p>Question: what would be different if in the source dataset horizons were identified instead of layers? </p>"},{"location":"cookbook/rml/#measurements","title":"Measurements","text":"<p>An example transformation for the measurements in the original dataset is available in the file measurements.yarrrml. The extra elements to address in this transformation are:</p> <ul> <li>Instance of the respective Observation class;</li> <li>Instance of the respective Result class;</li> <li>Relation between Observation and Result;</li> <li>Numerical literal with the measurement result.</li> </ul> <p>Question: Identify in the Layer Horizon module of GloSIS which are the units of measurement associated with the Result instances used in this example. </p> <p>Exercise I: Create a new <code>yarrrml</code> file including all the transformations given above, creating all necessary triples for sites, profiles, layers and measurements. Make sure it is correctly parsed by <code>yarrrml-parser</code> and generate a new, complete, knowledge graph.</p> <p>Exercise II: Modify the transformation you obtained in the previous exercise so that it declares all pH measurements as resulting from a H2O procedure (water solution).</p>"},{"location":"cookbook/rml/#more","title":"More","text":"<p>Now that you obtained a RDF knowledge graph you can publish it to the internet. Follow the guide on Virtuoso to learn how.</p> <p>In alternative to RML, you may transform tabular data into RDF with SPARQL queries using tarql. Follow that guide for the details.</p>"},{"location":"cookbook/sql/","title":"SQL &amp; python","text":"<p>Status: in progress</p> <p>This recipe implements the GeoPackage Good Practice for Soil Data by converting a sample soil database to GeoPackage based on the INSPIRE Soil Model using SQL statements from a Python environment.</p> <p>REQUIREMENTS - basic understanding of python and sql concepts - running environment python, pip, virtualenv (or alternative)</p> <p>Setup a python environment</p> <pre><code>virtualenv soildata\ncd soildata\n. bin/activate\n</code></pre> <p>Clone the workshop environment and install python requirements</p> <pre><code>git clone \npip install -r requirements.txt\n</code></pre> <p>As a soil database we use the SOTER database of Cuba which is available as Microsoft Access mdb as well as SQLite. </p>"},{"location":"cookbook/tarql/","title":"tarql","text":"<p>The basic idea behind <code>tarql</code> is to develop a <code>CONSTRUCT</code> SPARQL query that instead of executing against a triple store is executed against a CSV file. The result is a knowledge graph that may itself be deployed to a triple store or outright published on the web.</p>"},{"location":"cookbook/tarql/#install","title":"Install","text":"<p>Start by accessing the releases page and download a compressed file with the latest version. Then uncompress the file, it will create a new folder in your system with the release number appended, for instance <code>tarql-1.2</code>.</p> <p>The sub-folder <code>bin</code> contains executables for both Linux and Windows. You may run the executable directly or install it for wider system use. On Linux it is common practice to copy the programme folder to <code>/opt</code> and then create a symbolic link in <code>/usr/local/bin</code>.</p> <pre><code>$ unzip tarql-1.2.zip\n$ sudo mv tarql-1.2 /opt\n$ sudo ln -s /opt/tarql-1.2/bin/tarql /usr/local/bin/tarql\n</code></pre> <p>Finally try invoking the executable to make sure it is functioning.</p> <pre><code>$ tarql --help\n</code></pre>"},{"location":"cookbook/tarql/#use","title":"Use","text":"<p>Note: before starting a data transformation into RDF you must devise a URI policy for your data. Please refer to the URI Policy document for details.</p> <p>The file SoilData.csv contains a simple set of hypothetical measurements referring to three soil profiles collected in two different sites. The goal is to transform this dataset into GloSIS compliant RDF.</p> <pre><code>site_id,lat,lon,profile_id,layer_id,upper_depth,lower_depth,pH,SOC,\n1,49.43,8.31,1,11,0,15,7.4,6,\n1,49.43,8.31,1,12,15,40,7.2,4,\n1,49.43,8.31,2,21,0,10,8,3,\n1,49.43,8.31,2,22,10,30,8.1,2,\n2,46.82,11.45,3,31,0,15,6.8,1,\n2,46.82,11.45,3,32,15,30,6.7,1,\n2,46.82,11.45,3,33,30,60,6.7,0,\n</code></pre>"},{"location":"cookbook/tarql/#profile","title":"Profile","text":"<p>The Profile class is the most simple in GloSIS, making it a good place to start. It only requires the declaration of a new instance and its association with the respective site, in essence just two triples. To do so two URIs must be created, one for the profile and another for the site. </p> <p>The Listing below shows a complete example creating URIs according to the policy used for the World Soil Information Service (WoSIS). The <code>CONSTRUCT</code> clause in the query is pretty vanilla, whereas in the <code>WHERE</code> clause the URIs are created with the <code>BIND</code> and <code>URI</code> functions. The important thing to note in this query is the use of column names in the CSV file as variables, <code>?site_id</code> and <code>?profile_id</code>. <code>tarql</code> matches every variable with the CSV columns, replacing them with the corresponding values. </p> <pre><code>PREFIX rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt;\nPREFIX glosis_pr: &lt;http://w3id.org/glosis/model/profile#&gt;\nPREFIX iso28258: &lt;http://w3id.org/glosis/model/iso28258/2013#&gt;\n\nCONSTRUCT { \n    ?uri_profile rdf:type glosis_pr:GL_Profile ; \n               iso28258:Profile.profileSite ?uri_site .\n}\nWHERE {\n    BIND (URI(CONCAT('http://wosis.isric.org/site#', ?site_id)) AS ?uri_site)\n    BIND (URI(CONCAT('http://wosis.isric.org/profile#', ?profile_id)) AS ?uri_profile)\n}\n</code></pre> <p>Exercise: try running this example using the <code>profile.sparql</code> file in the <code>tarql</code> folder against the <code>SoilData.csv</code> file. You should issue a command like:</p> <pre><code>$ tarql tarql/profile.sparql data/SoilData.csv\n</code></pre>"},{"location":"cookbook/tarql/#remove-duplicates","title":"Remove duplicates","text":"<p>By default <code>tarql</code> generates a triple for each line in the CSV file. Most likely the data in the CSV is not normalised, and thus many duplicates result. You can observe this with the example for the Profile class above. </p> <p>The tool provides a specific argument to deal with duplicates: <code>--dedup</code>. It suppresses all duplicate triples up to a given line in the output. In general you will want to use this argument with a large enough number to cover all the triples produced.</p> <pre><code>$ tarql --dedup 1000 tarql/profile.sparql data/SoilData.csv\n</code></pre> <p>If your only intention is to load <code>tarql</code>'s output to a triple store, you might not need to worry about duplicate triples. Most likely the software automatically discards the duplicates on load.</p>"},{"location":"cookbook/tarql/#site","title":"Site","text":"<p>The next example deals with the Site class, whose instances are spatial features. Therefore an additional instance of the GeoSPARQL Point class must be created, with the associated geometry literal. The <code>BIND</code> and <code>URI</code> are again at your service to create the URIs. The <code>STRDT</code> function is used to create the geometry instance with the appropriate type. The listing below is available in the file <code>site.sparql</code>.</p> <pre><code>PREFIX rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt;\nPREFIX geo: &lt;http://www.opengis.net/ont/geosparql#&gt;\nPREFIX glosis_sp: &lt;http://w3id.org/glosis/model/siteplot#&gt;\n\nCONSTRUCT { \n    ?uri_site rdf:type glosis_sp:GL_Site ;\n              rdf:type geo:Feature ;\n              geo:hasGeometry ?uri_geo .\n\n    ?uri_geo rdf:type geo:Point ; \n             geo:asWKT ?geom .\n}\nWHERE {\n    BIND (URI(CONCAT(\"http://wosis.isric.org/site#\", ?site_id)) AS ?uri_site)\n    BIND (URI(CONCAT(\"http://wosis.isric.org/geometry#\", ?site_id)) AS ?uri_geo)\n    BIND (STRDT(CONCAT(\"POINT(\", $lon, \", \", $lat, \")\"), geo:wktLiteral) AS ?geom)\n}\n</code></pre> <p>Exercise: try modifying the query above so that it produces a GML literal instead of WKT. Which of the literals do you prefer?</p>"},{"location":"cookbook/tarql/#layer","title":"Layer","text":"<p>Instances for the layer should not be a challenge, after succeeding with sites and profiles. The list below provides and example, creating triples relating the layer to the respective profile and declaring its extent with ISO-28258 predicates. </p> <pre><code>PREFIX rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt;\nPREFIX glosis_lh: &lt;http://w3id.org/glosis/model/layerhorizon#&gt;\nPREFIX iso28258: &lt;http://w3id.org/glosis/model/iso28258/2013#&gt;\n\nCONSTRUCT { \n    ?uri_layer rdf:type glosis_lh:GL_Layer ; \n        iso28258:ProfileElement.elementOfProfile ?uri_profile ;\n        iso28258:ProfileElement.upperDepth ?Top ;\n        iso28258:ProfileElement.lowerDepth ?Bottom .\n}\nWHERE {\n    BIND (URI(CONCAT('https://example.org/layer#', ?Layer)) AS ?uri_layer)\n    BIND (URI(CONCAT('https://example.org/profile#', ?Profile)) AS ?uri_profile)\n}\n</code></pre> <p>Exercise: save the query below in a file and use <code>tarql</code> to run it against the SoilData.csv file. </p> <p>Advanced exercise: hop on to the WoSIS RDF pilot and verify how a GloSIS Observation instance is formed. Create a new query to generate the approapriate instances for the pH measurements in the SoilData.csv file. Don't forget to create the associated GloSIS Result instance(s).</p>"},{"location":"cookbook/tarql/#more","title":"More","text":"<p>Now that you obtained a RDF knowledge graph you can publish it to the internet. Follow the guide on Virtuoso to learn how.</p> <p>In alternative to <code>tarql</code>, you may instead transform tabular data into RDF with RML.io, a transformation tool-set based on YAML configuration files.</p>"},{"location":"cookbook/uri/","title":"URI policy","text":""},{"location":"cookbook/uri/#the-role-of-uris","title":"The role of URIs","text":"<p>The Unified Resource Identifier is a standard set by The Internet Society, that has been in development for more that 30 years. It is a unique sequence of characters that identifies a logical or physical resource used by web technologies. </p>"},{"location":"cookbook/uri/#types-of-uris","title":"Types of URIs","text":"<p>There are two types of URIs that are important to identify: Unified Resource Names (URNs) and Unified Resource Locators (URLs). </p>"},{"location":"cookbook/uri/#urn","title":"URN","text":"<p>URNs are logical identifiers, simply providing a name for a resource. A URN does not indicate where to find or access the resource, only gives it a name. URNs start with the characters <code>urn:</code> to which follows a namespace and then a succession of other strings that identify sub-categories or paths down to the particular resource name.</p> <p>The <code>lex:</code> namespace is used in some countries to identify legislation documents. As an example, a URN to a European Directive: <code>urn:lex:eu:council:directive:2010-03-09;2010-19-UE</code></p> <p>URNs are used to identify books, media works, trade items, shipping vessels and many other resources. In the context of data provision over the internet that are less used, as they do not provide location. However, they can be useful to identify other resources somehow associated with the data in question.</p>"},{"location":"cookbook/uri/#url","title":"URL","text":"<p>In its turn, the URL not only identifies a resource, it also provides a location and the mechanism to interact with it. These are the most commonly used URIs in the Semantic Web and in digital data exchange in general.</p> <p>A URL is composed by three broad components:</p> <ul> <li> <p>Protocol: identifies a specific data exchange mechanism, for instance   Hyper Text Transfer Protocol (HTTP) or File Transfer Protocol (FTP). It composes the first segment of the URL and is followed by the characters <code>://</code>. Examples: <code>http://</code>, <code>file://</code>.</p> </li> <li> <p>Authority: an individual or institution responsible for the resource and   usually the infrastructure that may provide it on the internet. E.g. <code>isric.org</code>.</p> </li> <li> <p>Path: the relative path to the resource within the institutional   infrastructure. E.g. <code>/locations/id/123</code>.</p> </li> </ul> <p>Although meant to locate resource, URLs may not always do so and be simply used as identifiers. They remain useful as identifiers as they express an authority over the resource. URLs that do not locate a resource are termed as \"non dereferenceable\", meaning that when a computer programme attempts to use it no result is obtained.</p>"},{"location":"cookbook/uri/#selecting-a-uri-structure","title":"Selecting a URI structure","text":"<p>Before making any data available on the internet, say to publish the result of a transformation into RDF, you must devise an appropriate URI structure. This is particularly important with RDF, as every non-literal element in a knowledge graph must correspond to a URI. But even outside the Semantic Web, URIs are very useful, as they provide unique identifiers on the World Wide Web (WWW) to your datasets, even to each individual datum.</p> <p>A simple approach is to construct your URIs with three building blocks: </p> <ol> <li> <p>Use a sub-domain of your institutional domain to identify a single project or dataset. E.g. <code>soil.my-institute.org</code>.</p> </li> <li> <p>Add a path that starts with a name or identifier of the class to which the data instance belongs. This can be a database table, or a OWL or UML class. E.g. <code>/profile</code>.</p> </li> <li> <p>Complete the path with a number or string that unequivocally identifies the    data instance within the class. If you work with relational databases this may be the table primary key. An example: <code>#prof1</code>. </p> </li> </ol> <p>The complete template for this approach looks like:</p> <pre><code>http://project.institution.org/class#identifier\n</code></pre>"},{"location":"cookbook/uri/#the-wosis-example","title":"The WoSIS example","text":"<p>ISRIC is currently working on a pilot service providing data from the World Soil Information Service (WoSIS) as RDF compliant with the GloSIS Web Ontology. You may browse these data at virtuoso.isric.org.</p> <p>In this example the service is identified in the authority segment of the the URI (<code>wosis.isric.org</code>). Then short identifiers for GloSIS classes make the prefix of the URI path, (<code>site</code>, <code>profile</code>, <code>layer</code>, etc). The path suffix is composed by an hash followed by the WoSIS primary key. </p> <p>Some examples currently used with WoSIS data:</p> <ul> <li><code>http://wosis.isric.org/site#72007</code></li> <li><code>http://wosis.isric.org/profile#72007</code></li> <li><code>http://wosis.isric.org/layer#64448</code></li> <li><code>http://wosis.isric.org/observation#4357805</code></li> <li><code>http://wosis.isric.org/result#4357805</code></li> </ul>"},{"location":"cookbook/uri/#questions","title":"Questions","text":"<ul> <li> <p>Does your institution already has a URI policy?</p> </li> <li> <p>Is it applied to data provision?</p> </li> <li> <p>If not, can you elaborate a proposal?</p> </li> </ul>"},{"location":"cookbook/uri/#further-reading","title":"Further reading","text":"<ul> <li> <p>Best Practices URI Construction</p> </li> <li> <p>URIs, URLs, and URNs: Clarifications and Recommendations 1.0</p> </li> <li> <p>On Linking Alternative Representations To Enable Discovery And Publishing</p> </li> </ul>"},{"location":"cookbook/virtuoso/","title":"Virtuoso &amp; Skosmos","text":"<p>Status: in progress</p> <p>This recipe presents steps to publish a code-lists using the SKOS ontology in Virtuoso and Skosmos. Virtuoso is an open source Triple store providing a SPARQL endpoint.  Skosmos is an open source web application providing a human friendly browse interface for skos thesauri stored in a triple store.</p> <p>This is a follow up recipe of the extending code-lists recipe.</p> <p>In this recipe we're going to reproduce the publication of the <code>GLOSIS - Procedures</code> Codelist, maintained at github, initiated by the former SieuSoil project with contributions from the EJP Soil project.  </p>"},{"location":"cookbook/virtuoso/#load-skos-rdf-to-virtuoso","title":"Load SKOS RDF to virtuoso","text":"<p>We're using a docker compose orchestration to deploy virtuoso and skosmos locally. Copy the contents of the virtuoso folder into an empty folder. Navigate to the folder with command line and run:</p> <pre><code>docker compose up\n</code></pre> <p>(ctrl-c to stop the containers)</p> <ul> <li>Open http://localhost:8890/conductor and login using user: dba, password: dba (the password is configured as part of the docker compose)</li> </ul> <p></p> <ul> <li>On the <code>linked data</code> tab, select <code>Quad store upload</code>.</li> <li>Select <code>Resource URL</code> and paste the url of the ttl file from github (or upload a local file)</li> <li>Select <code>create graph</code> and enter as graph uri <code>http://w3id.org/glosis/model/procedure#</code>, click <code>upload</code>.</li> </ul> <p></p> <ul> <li>Navigate to http://localhost:8890/sparql/ and run a SPARQL query like the one below, the glosis concepts should be returned.</li> </ul> <pre><code>SELECT DISTINCT ?Concept \nFROM &lt;http://w3id.org/glosis/model/procedure#&gt;\nWHERE {[] a ?Concept} \nLIMIT 20\n</code></pre>"},{"location":"cookbook/virtuoso/#setup-skosmos","title":"Setup Skosmos","text":"<p>The file config-docker.ttl contains the configuration of SKOSMOS. The file is pre-configured for the procedures codelist, but you have to update it if you want to include alternative code lists. </p> <p>After an update of the config file, you need to restart the docker compose.</p> <ul> <li>Open http://localhost:8080 and evaluate if the codelist is properly loaded. </li> </ul> <p>In case of errors, logging of skosmos occurs in the <code>browser log panel</code>, click anywhere in the page and select <code>inspect</code> from the context menu. Then open the <code>console</code> tab and refresh the page.</p> <p>In case you are insterested to update the look and feel of the skosmos instance, notice the <code>skosmos:customCss</code> property on the config file. This property can link to a css file having custom css. Include the css file via a volume mount in the docker compose.</p>"},{"location":"cookbook/virtuoso/#read-more","title":"Read more","text":"<p>Virtuoso</p> <ul> <li>Website: virtuoso</li> <li>GitHub: github</li> <li>Docker: docker</li> <li>Virtuoso at ISRIC: isric</li> </ul> <p>Skosmos</p> <ul> <li>Website: skosmos</li> <li>GitHub: github</li> <li>Docker: docker</li> <li>Skosmos examples: glosis, agrovoc, agclass</li> </ul>"},{"location":"cookbook/webdav/","title":"WebDav &amp; Atom","text":"<p>Status: ready</p> <p>In the technical Guidelines download services <code>INSPIRE atom services</code> are described to provide a light weight alternative to WFS and WCS, while fitting with all the aspects of a <code>webservice</code> as described in the implementing rules. </p> <p>This recipe describes a minimal approach which is based on placing a number of Atom-xml files along side the downloadable resources in a web accessible folder or webdav. Consider that WebDav is used as an example, any online file system would suffice. A plain apache webserver, Zenodo or even sharepoint or dropbox.</p>"},{"location":"cookbook/webdav/#wsgidav","title":"WSGIDAV","text":"<p>Various webdav implmentations exist; apache webdav, NGINX DAV, SFTPGO, wsgidav. For this recipe we'll use wsgidav, but others will work in a similar way.</p> <p>WsgiDAV provides a docker image, with below statement you advertise the current folder via WebDAV.</p> <pre><code>docker run --rm -it -p 8080:8080 -v ${PWD}:/var/wsgidav-root mar10/wsgidav\n</code></pre> <p>Open http://localhost:8080 in your browser to see the file contents.</p>"},{"location":"cookbook/webdav/#atom-files","title":"ATOM files","text":"<p>Stop the container (ctrl-C). Create a new folder and copy your dataset(s) of choice into it. Create a file <code>service.atom.xml</code> and for each dataset a new text file with the same name, but the atom extension.</p> <p>service.atom.xml is the <code>service feed</code> (comparable to the capabilities operation in OWS). The service feed will contain details about the service and link to each of the dataset feeds. Populate the service feed with (replace relevant sections):</p> <pre><code>&lt;feed xmlns=\"http://www.w3.org/2005/Atom\"\n  xmlns:georss=\"http://www.georss.org/georss\" \n  xmlns:inspire_dls=\"http://inspire.ec.europa.eu/schemas/inspire_dls/1.0\" \n  xml:lang=\"en\"&gt;\n &lt;!-- feed title --&gt;\n &lt;title&gt;XYZ Example INSPIRE Download Service&lt;/title&gt;\n &lt;!-- feed subtitle --&gt;\n &lt;subtitle&gt;INSPIRE Download Service of Soil Properties data in Sahel region&lt;/subtitle&gt;\n &lt;!-- self-referencing link to this feed --&gt;\n &lt;link href=\"http://localhost:8080/service.atom.xml\" rel=\"self\" type=\"application/atom+xml\"  hreflang=\"en\" title=\"This document\"/&gt;\n &lt;!-- link to Open Search definition file for this servicen (not implemented) \n&lt;link rel=\"search\" href=\"https://example.org/search/opensearchdescription.xml\" type=\"application/opensearchdescription+xml\" title=\"Open Search Description for XYZ download service\"/&gt; --&gt;\n &lt;!-- identifier --&gt;\n &lt;id&gt;http://localhost:8080/service.atom.xml&lt;/id&gt;\n &lt;!-- rights, access restrictions --&gt;\n &lt;rights&gt;Copyright (c) 2021, XYZ; all rights reserved&lt;/rights&gt;\n &lt;!-- date/time this feed was last updated --&gt;\n &lt;updated&gt;2021-03-31T13:45:03Z&lt;/updated&gt;\n &lt;!-- author contact information --&gt;\n &lt;author&gt;&lt;name&gt;John Doe&lt;/name&gt;&lt;email&gt;doe@example.org&lt;/email&gt;&lt;/author&gt;\n &lt;category term=\"http://inspire.ec.europa.eu/metadata-codelist/SpatialDataServiceCategory/infoFeatureAccessService\" scheme=\"http://inspire.ec.europa.eu/metadata-codelist/SpatialDataServiceCategory\"/&gt;\n &lt;!-- entry for a \"Dataset Feed\" for a pre-defined dataset --&gt;\n &lt;entry&gt;\n    &lt;!-- title for \"Dataset Feed\" for pre-defined dataset --&gt;\n    &lt;title&gt;soil properties ABC Dataset Feed&lt;/title&gt;\n    &lt;!-- Spatial Dataset Unique Resource Identifier for this dataset--&gt;\n    &lt;inspire_dls:spatial_dataset_identifier_code&gt;wn_id1&lt;/inspire_dls:spatial_dataset_identifier_code&gt; \n    &lt;inspire_dls:spatial_dataset_identifier_namespace&gt;https://example.org/&lt;/inspire_dls:spatial_dataset_identifier_namespace&gt;\n    &lt;!-- link to dataset metadata record --&gt;\n    &lt;link href=\"https://example.org/metadata/abcISO19139.xml\" rel=\"describedby\" type=\"application/xml\"/&gt;\n    &lt;!-- link to \"Dataset Feed\" for pre-defined dataset --&gt;\n    &lt;link rel=\"alternate\" href=\"http://localhost:8080/soilproperties.atom.xml\" type=\"application/atom+xml\"  hreflang=\"en\" title=\"Feed containing the soil properties data\"/&gt;\n    &lt;!-- identifier for \"Dataset Feed\" for pre-defined dataset --&gt;\n    &lt;id&gt;http://localhost:8080/soilproperties.atom.xml&lt;/id&gt;\n    &lt;!-- rights, access info for pre-defined dataset --&gt;\n    &lt;rights&gt;Copyright (c) 2002-2021, XYZ; all rights reserved&lt;/rights&gt;\n    &lt;!-- last date/time this entry was updated --&gt;\n    &lt;updated&gt;2012-03-31T13:45:03Z&lt;/updated&gt;\n    &lt;!-- summary --&gt;\n    &lt;summary&gt;This is the entry for soil properties ABC Dataset&lt;/summary&gt;\n    &lt;!-- optional GeoRSS-Simple polygon outlining the bounding box of the pre-defined dataset described by the entry. Must be lat lon --&gt;\n    &lt;georss:polygon&gt;47.202 5.755 55.183 5.755 55.183 15.253 47.202 15.253 47.202 5.755&lt;/georss:polygon&gt;\n    &lt;!-- CRSs in which the pre-defined Dataset is available --&gt;\n    &lt;category term=\"http://www.opengis.net/def/crs/EPSG/0/4258\" label=\"ETRS89\"/&gt;\n &lt;/entry&gt;\n&lt;/feed&gt;\n</code></pre> <p>Notice that we're not implementing the actual opensearch search functionality yet. You can leave the <code>opensearchdescription</code> line empty for now. There are some external options to provide the opensearch, the Technical Guidance document actually includes a PHP script to facilitate opensearch.</p> <p>Then for each dataset add a file <code>soilproperties.atom.xml</code>:</p> <pre><code>&lt;feed xmlns=\"http://www.w3.org/2005/Atom\"\n xmlns:georss=\"http://www.georss.org/georss\" xml:lang=\"en\"&gt;\n    &lt;!-- feed title --&gt;\n    &lt;title&gt;INSPIRE Dataset Soil properties Download&lt;/title&gt;\n    &lt;!-- feed subtitle --&gt;\n    &lt;subtitle&gt;INSPIRE Download Service, of organisation XYZ providing dataset Soil Properties&lt;/subtitle&gt;\n    &lt;!-- links to INSPIRE Spatial Object Type definitions for this predefined dataset --&gt;\n    &lt;link href=\"https://inspire.ec.europa.eu/featureconcept/SoilProfile\" rel=\"describedby\" type=\"text/html\"/&gt;\n    &lt;!-- self-referencing link to this feed --&gt;\n    &lt;link href=\"http://localhost:8080/soilproperties.atom.xml\" rel=\"self\" \n    type=\"application/atom+xml\"\n    hreflang=\"en\" title=\"This document\"/&gt;\n    &lt;!-- upward link to the corresponding download service feed --&gt;\n    &lt;link href=\"http://localhost:8080/service.atom.xml\" rel=\"up\" type=\"application/atom+xml\" hreflang=\"en\" title=\"The parent service feed document\"/&gt;\n    &lt;!-- identifier --&gt;\n    &lt;id&gt;http://localhost:8080/soilproperties.atom.xml&lt;/id&gt;\n    &lt;!-- rights, access restrictions --&gt;\n    &lt;rights&gt;Copyright (c) 2021, XYZ; all rights reserved&lt;/rights&gt;\n    &lt;!-- date/time this feed was last updated --&gt;\n    &lt;updated&gt;2021-03-31T13:45:03Z&lt;/updated&gt;\n    &lt;!-- author contact information --&gt;\n    &lt;author&gt;&lt;name&gt;John Doe&lt;/name&gt;&lt;email&gt;doe@xyz.org&lt;/email&gt;&lt;/author&gt;\n    &lt;!-- download the pre-defined dataset in GML format in CRS EPSG:25832 --&gt; \n    &lt;entry&gt;\n        &lt;title&gt;soil properties in CRS EPSG:25832 (GML)&lt;/title&gt;\n        &lt;link rel=\"alternate\" href=\"http://localhost:8080/soilproperties_25832.gml\" type=\"application/gml+xml;version=3.2\" hreflang=\"en\" length=\"34987\" \n          title=\"soil properties dataset encoded as a GML 3.2 document in ETRS89 UTM zone 32N (http://www.opengis.net/def/crs/EPSG/0/25832)\"/&gt;\n        &lt;id&gt;http://localhost:8080/soilproperties_25832.gml&lt;/id&gt;\n        &lt;updated&gt;2021-06-15T11:12:34Z&lt;/updated&gt;\n        &lt;category term=\"http://www.opengis.net/def/crs/EPSG/0/25832\" label=\"ETRS89 / UTM zone 32N\"/&gt;\n    &lt;/entry&gt;\n    &lt;!-- download the same pre-defined dataset in GML format in CRS EPSG:4258--&gt;\n    &lt;entry&gt;\n        &lt;title&gt;soil properties in CRS EPSG:4258 (GML)&lt;/title&gt;\n        &lt;!--file download link--&gt;\n        &lt;link rel=\"alternate\" href=\"http://localhost:8080/soilproperties_WGS84.gml\" type=\"application/gml+xml;version=3.2\" hreflang=\"en\" length=\"37762\" \n          title=\"soil properties encoded as a GML 3.2 document in WGS84 geographic coordinates (http://www.opengis.net/def/crs/OGC/1.3/CRS84)\"/&gt;\n        &lt;id&gt;http://localhost:8080/soilproperties_WGS84.gml&lt;/id&gt;\n        &lt;updated&gt;2021-06-14T12:22:09Z&lt;/updated&gt;\n        &lt;category term=\"http://www.opengis.net/def/crs/EPSG/0/4258\" label=\"ETRS89\"/&gt;\n    &lt;/entry&gt;\n    &lt;!-- download the same pre-defined dataset in ShapeFile format in CRS EPSG:25832, ShapeFile is in a single zip archive.--&gt;\n    &lt;entry&gt;\n        &lt;title&gt;soil properties in CRS EPSG:25832 (ShapeFile)&lt;/title&gt;\n        &lt;link rel=\"alternate\" href=\"http://localhost:8080/soilproperties_25832.zip\" type=\"application/xshapefile\" hreflang=\"en\" length=\"89274\" \n        title=\"soil properties dataset encoded as a ShapeFile in ETRS89 UTM zone 32N (http://www.opengis.net/def/crs/EPSG/0/25832)\"/&gt;\n        &lt;id&gt;http://localhost:8080/soilproperties_25832.zip&lt;/id&gt;\n        &lt;updated&gt;2021-06-15T11:12:34Z&lt;/updated&gt;\n        &lt;category term=\"http://www.opengis.net/def/crs/EPSG/0/25832\" \n        label=\"ETRS89 / UTM zone 32N\"/&gt;\n    &lt;/entry&gt;\n&lt;/feed&gt;\n</code></pre> <p>Notice that you can provide multiple distributions for the same dataset (in various projections, translations or formats) to facilitate users.</p> <p>Notice that the examples above incorperate ongoing work as described in https://github.com/INSPIRE-MIF/gp-data-service-linking-simplification/issues/63.</p> <p>The docker container runs locally, so it can not be tested by the INSPIRE Validator.  In Local Tunnel an approach is suggested to temporarily host a local service online, so you can run the validation.</p> <p>Note that you have to update the self-references in atom files to use the tunneled web address. Then trigger the Atom validation:</p> <p></p>"},{"location":"cookbook/webdav/#read-more","title":"Read more","text":"<p>An alternative Atom implementation exists in GeoNetwork. The approach is described at https://geonetwork-opensource.org/manuals/trunk/en/tutorials/inspire/download-atom.html and https://geonetwork-opensource.org/manuals/3.10.x/en/api/opensearch.html. GeoNetwork provides an internal and external mode, the external mode provides opensearch on a set of remote atom feeds. The internal mode generates the atom feeds from metadata records.</p> <p>Hale Studio provides an option to generate a Atom feed while exporting a dataset to GML. The Hale Connect platform offers a Atom based service endpoint for every dataset published.</p> <p>The QGIS INSPIRE Atom plugin provides access to Atom services through QGIS.</p>"},{"location":"cookbook/xtraserver/","title":"Xtra server","text":"<p>Status: contribution required</p> <p>A proprietary java implementation of WFS, WMS. The package is also distributed as ArcGIS for INSPIRE Classic server extension. The team at interactive-instruments is and has been heavily involved in the definition of GML and the INSPIRE download specifications. They developed ShapeChange which is commonly used to export UML diagrams to XSD. Another product is LDProxy, an open source java implementation of the OGC API suite of standards.</p> <ul> <li>Website: https://www.interactive-instruments.de/en/xtraserver/</li> </ul>"},{"location":"cookbook/zenodo/","title":"Zenodo","text":"<p>This recipe proposes an approach to use Zenodo to publish a dataset in such a way, that many of the INSPIRE practices are implemented.  Zenodo is a generic repository for academic work part of the European OpenAIRE program and operated by CERN.  Zenodo resources are clustered in communities. You can join an existing community or start one. Resources from a community can be accessed as a collection. Community moderators (dis)allow contributions to a community.</p>"},{"location":"cookbook/zenodo/#datacite-inspire","title":"DataCite &amp; INSPIRE","text":"<p>Zenodo adopted aspects of the DataCite metadata schema. The procedure below is also relevant for other DataCite oriented repositories, such as Dataverse. Since Zenodo is DataCite oriented, it does not directly match the requirement of INSPIRE to provide iso19139 metadata. However by following the suggested procedure, the metadata is almost semantically identical. Two routes of resolving this challenge are possible:</p> <ul> <li>Engage with the INSPIRE community to enable the DataCite encoding as an additional format to provide INSPIRE metadata</li> <li>Introduce a technical component, such as pygeometa, which could provide a transformation from DataCite to iso19139</li> </ul> <p>Clement Lattelais (INRAE) is preparing a comparison tool on how the metadata schemes of various repositories (DataVerse, Zenodo, ..) relate to the INSPIRE Metadata Guidelines. This tool is relevant to evaluate if (and how) a certain repository can be used for INSPIRE. But also to assess any non listed repositories.</p>"},{"location":"cookbook/zenodo/#upload-your-data-to-zenodo","title":"Upload your data to Zenodo","text":"<p>After (registering and) logging in you can select the upload resource option.</p> <p></p> <p>In the next step a metadata form opens starting with the obvious fields, title, abstract, keywords, publication date. Notice that an existing DOI can be provided or a new one be generated by the platform.</p> <p></p> <p>The list of contact roles is very detailed in Zenodo. Verify that you have at least 1 contact which has the role of <code>Contact point</code>, and provide at least a valid email address.</p> <p>INSPIRE metadata guidelines mandate a number of fields which are not available on the Zenodo metadata form. There are 2 options to still provide this metadata. </p> <ul> <li>Provide a iso19139 document as an additional attachement to the record.</li> <li>Use the subject field to provide additional metadata</li> </ul> <p>For option 2, the relevant element is the subject area at the bottom of the metadata form. Each subject is defined by a label and a URI. These URI's can reference concepts which provide the required context to the data. The table below suggests a number of subjects to be added, related to specific metadata requirements</p> Requirement Thesaurus Example Topic category TopicCategory geoscientific Information Conditions applying to access and use ConditionsApplyingToAccessAndUse no Conditions Apply Geographic bounding box GeoNames Italy INSPIRE Theme Theme register Soil Priority dataset Priority dataset Directive 2008/56/EC Degree of conformity Degree of conformity Not evaluated IACS Data IACS Data lpis Spatial Data Service Type Spatial Data Service Type View service Spatial Scope Spatial Scope Regional <p></p>"},{"location":"cookbook/zenodo/#an-atom-approach-to-access-the-data-on-zenodo","title":"An Atom approach to access the data on Zenodo","text":"<p>This approach is based on the recipe on WebDav and Atom services. The suggestion is to upload 2 additional Atom.xml files which describe the resources in an Atom format. The first file describes the Atom Download Service. The second Atom describes the distributions of the dataset. The zenodo approach will not cover all the aspects of a INSPIRE ATOM Download service, the OpenSearch option is missing. </p>"},{"location":"cookbook/data/bhr-p/readme/","title":"BHR-P","text":"<p>This is an extract of BRO - Bodemkundig booronderzoek (BHR-P), the Dutch National Database of Soil Profile analyses data (Licensed: CC-0).</p> <p>Bulk density and organic matter are relevant indicators of Carbon Stock.</p> <p>Downloaded at 17-01-2023.</p> <ul> <li>Bulk density has been extracted via:</li> </ul> <pre><code>SELECT borehole_research_pk, bro_id, research_report_date, litter_layer_investigated, standardized_location, begin_depth, end_depth, analysis_report_date,  dry_bulk_density_determination.*\nfrom borehole_research, dry_bulk_density_determination, investigated_interval, borehole_sample_analysis\nwhere investigated_interval_fk=investigated_interval_pk\nand borehole_sample_analysis_pk=borehole_sample_analysis_fk\nand borehole_research_pk = borehole_research_fk \nand dry_bulk_density is not null;\n</code></pre> <ul> <li>Organic matter has been extracted via:</li> </ul> <pre><code>SELECT borehole_research_pk, bro_id, research_report_date, litter_layer_investigated, standardized_location, begin_depth, end_depth, analysis_report_date,  organic_matter_content_determination.*\nfrom borehole_research, organic_matter_content_determination, investigated_interval, borehole_sample_analysis\nwhere investigated_interval_fk=investigated_interval_pk\nand borehole_sample_analysis_pk=borehole_sample_analysis_fk\nand borehole_research_pk = borehole_research_fk \nand organic_matter_content is not null;\n</code></pre>"},{"location":"cookbook/data/bhr-p/readme/#description-of-the-dataset","title":"Description of the dataset","text":"Attribute Description borehole_research_pk identifier of the research bro_id alternative identifier of the research research_report_date Date of the fieldwork litter_layer_investigated Indication if the litter layer was considered geometry lLocation of the research begin_depth Top of the layer end_depth Bottom of the layer analysis_report_date Date of the lab analyses dry_bulk_density_determination_pk Identifier of the analyses investigated_interval_fk Identifier of the layer determination_procedure Identifer of the procedure determination_method Identifier of the method ring_diameter Parameter of the specimen ring_height Parameter of the specimen drying_temperature Parameter of the analyses method volume_water_saturated Property of the sample dry_bulk_density Observed value material_irregularity Property of the sample"},{"location":"masterclass/","title":"Masterclass on Soil Data Assimilation","text":"<p>At 24-26 january 2023 Wageningen Environmental Research and ISRIC - World Soil Information has organised a masterclass on Soil Data Assimilation. This masterclass was a follow up of  the Training on INSPIRE Good Practices around Soil Data held in april 2022.</p>"},{"location":"masterclass/#program-times-in-cet","title":"Program (times in CET)","text":""},{"location":"masterclass/#24th-of-january","title":"24th of january","text":"Building Gaia; Meeting room 2 Video 10.00 Introduction to data assimilation (harmonization; extension; codelists) - Paul van Genuchten, Jandirk Bulens video 11:00 INSPIRE Tools - Paul van Genuchten video 12:00 Skosmos (code lists) - Paul van Genuchten video 14:30 Soil model in Geopackage - Stefania Morrone video"},{"location":"masterclass/#25th-of-january","title":"25th of january","text":"Lumen 2 meeting room Video 9.00 Introduction to View and download services video 10.00 Sensorthings API - Kathi Schleidt, Hylke van der Schaaf video 13:45 WCS - Kathi Schleidt, Peter Baumann video 14:30 geopython (pygeoapi, pygeometa, pycsw) - Tom Kralidis, Luis de Sousa video"},{"location":"masterclass/#26th-of-january","title":"26th of january","text":"Gaia 2 meeting room Video 9.00 Introduction to Metadata and discovery vido 10:00 deegree webservices intro and Q&amp;A - Torsten Friebe video 10:30 MapServer intro and Q&amp;A - Seth G video 11:00 Extending Codelists - Luis de Sousa video 14.00 Hale Studio - Kate Lyndegaard"},{"location":"masterclass/edition-2022/","title":"Training on INSPIRE Good Practices around Soil Data","text":"<p>In april 2022 WENR and ISRIC organised a first iteration of a training series on Good Practices around Soil Data.  The presentations of that training are still available and provide a good introduction to the edition 2023 masterclass.</p> <p></p> Video Speaker The reasoning behind INSPIRE why do we need a directive? Joeri Robbrecht, European Commission Why do we need to understand INSPIRE and share data? Jandrik Bulens, WENR Experiences of Implementing SOIL in INSPIRE. Maria Fantappi\u00e8 (CREA), Florian Hoedt (Th\u00fcnen) and Dries Luts (Department of the Environment and Spatial Development, Flemish Government) INSPIRE Conceptual Framework Luis de Sousa, ISRIC Data discovery Paul van Genuchten, ISRIC Interoperability; O&amp;M, Sensorthings API, Web Coverage Services Kathi Schleidt, Datacove Extending INSPIRE for the Air Quality directive Kathi Schleidt, Datacove INSPIRE Soil: an overview and relations with other standards, the conceptual model of the soil theme as a common base Kathi Schleidt, Datacove Harmonize, map, transform: what does it mean? Paul van Genuchten, ISRIC Code lists in INSPIRE Paul van Genuchten, ISRIC Implementation, operation, reporting. How do you keep track on progress Paul van Genuchten, ISRIC Technical aspects of view (WMS)-, download (WFS, Atom) services and data harmonization Paul van Genuchten, ISRIC Adapting to evolved developments specifically WCS and SensorThings Kathi Schleidt, Datacove Zooming in on INSPIRE and GloSIS mapping. What about tools and software to be used Luis de Sousa, ISRIC Emerging data exchange technologies: OGC API; RDF/SPARQL, Gaia-x. Why, what and how? Paul van Genuchten, ISRIC <p>For a full report see also the EJP website.</p>"},{"location":"utils/docker/","title":"Overview Docker","text":"<p>Docker is a virtualisation technology, slightly more efficient then running a virtual machine. With docker you run a full virtual environment (container) within your PC. Most containers run a flavour of Linux and you access them as if you access a remote server. Containers are based on a <code>docker-image</code>, a prepared set of operating system and software. Docker images are build locally from a <code>Dockerfile</code> (recipe) or downloaded from a repository such as dockerhub. Learn more about docker in the Docker Overview.</p> <p>In this recipe we run most examples using Docker, because in this way there is no need to install any software on your computer, which may either not be allowed, give errors due to missing dependencies or in a worse case scenario can corrupt an existing configuration. </p> <p>On Windows and Apple we recommend to install Docker Desktop. Docker Desktop provides an additional panel to manage running containers. On Linux you can install docker engine. Start docker engine from the start menu, if it is not already running. A general check can be triggered by typing <code>docker ps</code> in a console, this will provide a notification of docker availability and running containers.</p> <p>In this recipe we will use the following Docker concepts:</p> <ul> <li>Docker container; an image deployed as a virtual environment. Most containers have an assigned port, so you can access the service of the container via the browser (for example, http://localhost:5000). You can also interact with containers from your commandline (docker ps, docker logs xxx, docker run geopython/pygeoapi)</li> <li>Docker volume; a folder on the host system which you mount as a folder into the container. Containers are destroyed when stopped, any file stored in the container file system is lost. By mounting a host folder into the container, you can persist items between runs of the container.</li> <li>Docker compose; with a compose file you orchestrate a cluster of containers into a functional system. One container runs a database, another container runs a webserver and the third container runs the web application. The compose file arranges that the containers can connect, on which port they run and which volumes they load.</li> <li>Docker build is the command to build a docker image from a <code>Dockerfile</code></li> <li>With Docker pull you can pull an image from a repository </li> <li>Docker run starts a container, press ctrl-c to stop is again (unless you run it with -d (detach) option then stop a container with <code>docker stop &lt;id&gt;</code>) </li> </ul> <p>Instead of running containers for permanent server applications, you can also start a container to run a short process, similar to running a command line utility. The container will stop when the process is finished. </p>"},{"location":"utils/docker/#docker-exercise","title":"Docker exercise","text":"<ul> <li>Install Docker Desktop, verify it is running, else start it from the start menu.</li> <li>On commandline run this command</li> </ul> <p><code>docker run -p 80:80 --name test uzyexe/tetris:latest</code></p> <ul> <li>Open browser at http://localhost</li> <li>Open a shell within the container</li> </ul> <pre><code>docker exec -it test /bin/bash\n</code></pre> <ul> <li>type <code>exit</code> to return to your pc</li> <li>ctrl-c to stop the container</li> </ul> <p>(if other processes are running on port 80, Docker will throw an error, select another port, e.g. -p 81:80, and open http://localhost:81)</p>"},{"location":"utils/docker/#cleaning-up","title":"Cleaning up","text":"<p>The virtual environments take quite some memory and CPU, you will notice this on older computers. Make sure to stop all containers after you finish using them. You can easily start them again later. <code>docker ps</code> (or the docker desktop window) indicate which containers are still running.</p> <p>After your experiments you will notice the size of your harddisk has considerably been reduced. Images, containers, volumes all use quite some space. There is a single command to clean up everything</p> <pre><code>docker system prune -a\n</code></pre> <p>Only use it if you are sure you don't want to keep any docker resources.</p>"},{"location":"utils/gdal/","title":"GDAL","text":"<p>OGR/GDAL is a swiss army knife for spatial data. It can read u multitude of grid and vector formats and interact with OGC services. The tool includes 2 clusters of scripts: </p> <ul> <li><code>GDAL utilities</code> interact with grids </li> <li><code>OGR utilities</code> interact with vector formats</li> </ul>"},{"location":"utils/gdal/#installation","title":"Installation","text":"<ul> <li>On Windows, most easy install is using Conda. Other option is via ms4w. Note that QGIS (C:\\Program Files\\QGIS 3.xx\\bin\\gdalinfo.exe) and ArcMap are also packaged with GDAL.</li> <li>For Apple, use brew install gdal</li> <li>On Ubuntu, run add-apt-repository ppa:ubuntugis/ppa &amp;&amp; apt update &amp;&amp; apt install gdal-bin</li> <li>On Debian, use Conda</li> <li>For Docker, use this container</li> </ul>"},{"location":"utils/gdal/#gdal-exersize","title":"GDAL exersize","text":"<ul> <li>Get details of a gridded dataset</li> </ul> <pre><code>gdalinfo https://files.isric.org/soilgrids/latest/data/bdod/bdod_0-5cm_mean.vrt\n</code></pre> <ul> <li>Get details of a geojson file</li> </ul> <pre><code>ogrinfo https://github.com/gregoiredavid/france-geojson/raw/master/cantons-avec-outre-mer.geojson\n</code></pre> <ul> <li>Convert geojson to shapefile</li> </ul> <pre><code>ogr2ogr cantons.shp https://github.com/gregoiredavid/france-geojson/raw/master/cantons-avec-outre-mer.geojson\n</code></pre>"},{"location":"utils/gdal/#gml-application-schema","title":"GML Application Schema","text":"<p>In 2017 a large work on GDAL introduced support for GML Application Schema. GDAL will read the schema of the xml and generate a relational database schema required to store the data. Verify that the build of GDAL includes GMLAS, it needs a special library XERCES</p> <ul> <li>Convert a gml file to geopackage</li> </ul> <pre><code>ogr2ogr -f GPKG test.gpkg GMLAS:/path/to/the.gml\n</code></pre> <ul> <li>Use DBeaver or similar to evaluate the contents of the GeoPackage</li> </ul>"},{"location":"utils/gdal/#gdal-and-python","title":"GDAL and Python","text":"<p>GDAL includes python bindings, but many prefer the Fiona and Rasterio library, which do the same in a more <code>pythonic</code> way.</p>"},{"location":"utils/gdal/#gdal-and-r","title":"GDAL and R","text":"<p>RGDAL and sf provide bindings to GDAL within R.</p>"},{"location":"utils/gdal/#read-more","title":"Read more","text":"<ul> <li>Website</li> <li>Docker</li> <li>GMLAS</li> <li>Presentation on FOSS4G Europe 2017</li> </ul>"},{"location":"utils/git/","title":"Overview Git","text":"<p>Git is a distributed version management system of mainly text files to facilitate shared development of software and/or  documentation. Git is one of the creations of Linus Torvalds. In Git everybody <code>checks out</code> the full repository with all  its history, you make changes to a version locally and <code>push</code> them back to the server. In the process incidental <code>conflicts</code>  may occur, if someone else has made a change in the same version and line as you. The server will reject the push until  you resolved the conflict.</p> <p>Git is mainly operated from the command line, but a lot of client software is available to facilitate participation.  An example is SmartGit, which provides an easy to use interface to manage even  complex git tasks such as fixing conflicts. Git GUI is the graphical user  interface included with the Git software suite.</p> <p>GitHub.com is a well known provider of Git services. They offer a lot of additional functionality on top  of the Git version management, such as a web interface, issue management, forking &amp; pull requests, wiki, actions, etc.</p> <p>Gitlab is a popular open source software stack offering a similar set of functionalities as GitHub,  you can use it as SAAS or install it on a server.</p> <p>In 2020 various code forges based in the United States arbitrary blocked access to users from countries deemed hostile. This action affected foreign students and scientists working in Europe or collaborating with European institutions. That same year a community of European software developers registered a non-for-profit in Germany to support an independent code forge hosted in Europe. The result is Codeberg, a free, open source, Git-based code forge accessible to everyone in the world.</p> <p>An interesting quick start to Git is written by Roger Dudler (multiple translations).  Or if you prefer a video.</p> <p>The main reference for this software is the Git Book.</p>"},{"location":"utils/git/#git-use-cases","title":"Git Use Cases","text":"<ul> <li>A software project with contributors from around the globe. Including issue management, software releases and automated validation of unit and integration tests.</li> <li>This wiki is maintained in a Git repository. With every new push a new version is build and it replaces the previous version.</li> <li>A roll out of an improved composition of Docker containers on a cloud platform like Kubernetes. The helm charts (configuration files) are stored on a Git repository, every change to the helm chart results in an update of the development environment to reflect the latest changes.</li> <li>Git is an important tool for Dev Ops staff. Manual interventions are minimized. Every action is scripted, stored in a Git repository and started from a Git action. This improves tracebility and reproducability in software maintenance.</li> </ul>"},{"location":"utils/git/#exersize","title":"Exersize","text":"<ul> <li>Install git (or smartgit, which has git included) on your machine </li> <li>Clone the repository of this wiki from the command line:</li> </ul> <pre><code>git clone https://github.com/ejpsoil/soildata-assimilation-guidance.git\n</code></pre> <ul> <li>open the folder and log all events that happened on the repository </li> </ul> <pre><code>cd soildata-assimilation-guidance\ngit log --oneline\n</code></pre>"},{"location":"utils/git/#forking-and-pull-requests","title":"Forking and Pull requests","text":"<p>GitHub added an extra interactivity on top of Git, the option to <code>Fork</code> a repository to a personal space. And from the personal space provide an option to propose a change to the <code>upstream</code> repository, this is called a <code>pull request</code>. Now users were able to propose changes to repositories of which they were not even a member yet. The fork and pull mechanism has now been adopted by other git platforms, such as Gitlab, but because it is not part of the Git specification itself, it may work slightly different on other platforms. Let's try and improve this wiki via a pull request:</p> <ul> <li>If you do not have a github account yet, we invite you to set up one now at https://github.com/signup.</li> <li>When logged in, open https://github.com/ejpsoil/soildata-assimilation-guidance, and click the fork button in the top right. This will clone the repo to your personal space.</li> <li>On https://github.com/YOU/soildata-assimilation-guidance, edit a file (maybe you found a typo somewhere, or would like to comment/extend something) via the github web interface by opening the file and clicking on the pencil button.</li> <li>Below the text you notice a commit message and button. Every commit in Git requires a usefull message, so others understand what you did. Select the option \"Create a new branch for this commit ...\".</li> <li>Then create the branch, but not the pull request in the next step, because this pull request would arrive in your own repository</li> <li>Now navigate back to https://github.com/ejpsoil/soildata-assimilation-guidance. A banner shows indicating you can <code>compare and pull request</code> your recent change. Click this button and create the pull request.</li> </ul> <p>The process of a pull request is quite overwhelming. But it is an important aspect in collaborative development these days. Many repositories only allow code changes via pull requests, because it is a guarantee that at least 2 persons have looked at it.</p>"},{"location":"utils/jupyter/","title":"Jupyter notebooks","text":"<p>Status: in progress</p> <p>Jupyter notebooks are a combination of text and code (JUlia, PYThon and R), the code can directly be run within the notebook. Jupyter notebooks are mainly used in training or documentation settings in data science. Some argue that jupyter notebooks will replace articles in scientific magazines to communicate research results.</p> <p>Some examples of Jupyter notebooks of interest:</p> <ul> <li>https://inspire.rasdaman.org/apps/jupyter-notebook/index.html</li> <li>https://github.com/geopython/geopython-workshop</li> </ul> <p>A Quick Start on using Jupyter is written by Antonino Ingargiola.</p>"},{"location":"utils/jupyter/#read-more","title":"Read more","text":"<ul> <li>Website</li> <li>Documentation</li> <li>Wikipedia</li> </ul>"},{"location":"utils/localtunnel/","title":"Overview Local Tunnel","text":"<p>This recipe describes an approach to temporarily host a local webservice as an online service. A utility opens a tunnel to a service provicer, the service provider routes all traffic for a specific domain via the tunnel to your machine. The tunnel stops if you quit the utility (ctrl-c).</p> <p>This technology is for example relevant if you want to test a local service with the INSPIRE validator (which requires a service to be online).</p> <p>Various (free) service providers exist offering this service:</p> <ul> <li>localtunnel requires nodejs </li> <li>ngrok web based, but requires registration </li> <li>localhost.run requires SSH to be installed</li> </ul>"},{"location":"utils/localtunnel/#exercise","title":"Exercise","text":"<ul> <li>Verify a docker image is running, for example: </li> </ul> <p><code>docker run -p 80:80 -d uzyexe/tetris:latest</code></p> <ul> <li>Enter this command:</li> </ul> <p><code>ssh -R 80:localhost:80 nokey@localhost.run</code></p> <ul> <li>The utility will display a url on which the service will be available, try this url in your browser (and phone, to make sure it works also outside your computer)</li> </ul>"},{"location":"utils/python/","title":"Python","text":"<p>A programming language commonly used in data science. From scripts to increase the capabilities or automate processes in  ArcMAP or QGIS, maintanance and deployment tasks in DevOps, up to full programs such as GeoNode and pygeoapi.</p>"},{"location":"utils/python/#pip","title":"PIP","text":"<p>PIP is the package manager of python. Every script or program installs a series of packages from pip as part of its installation.</p>"},{"location":"utils/python/#virtual-environments","title":"Virtual environments","text":"<p>To prevent collision of dependencies between projects the use of virtual environment is common. Each virtual environment has its own python executable and libraries. Popular virtual environments are virtualenv, Conda, poetry. If you're new to python I would suggest to start with Conda.</p> <p>Note that some prefer to work even more isolated and use docker to set up a development container including the required libraries.</p>"},{"location":"utils/python/#jupyter-notebooks","title":"Jupyter notebooks","text":"<p>Jupyter notebooks combine text and executable (python) code into a single document. Optimal for training, documentation and reporting about research.</p>"},{"location":"utils/python/#geopython-community","title":"GeoPython Community","text":"<p>There is a significat community dedicated to the use of python in the geospatial domain. Interesting libraries used in that community are: </p> <ul> <li>RasterIO</li> <li>Shapely</li> <li>Fiona</li> <li>OWSLib</li> <li>GeoPandas</li> <li>PyGRASS</li> <li>pygeometa</li> <li>pyproj, etc.</li> </ul>"},{"location":"utils/r/","title":"R","text":"<p>r is a statistics utility commonly used in the soil statistics. R is typically extended with a series of libraries, like GDAL and grassgis.  Due to the amount of plugins required, some with a difficult installation procedure and potential high use of resources, the use of virtualisation, such as Docker, is common in soil data statistics.</p>"},{"location":"utils/r/#soil-mapping-spring-school","title":"Soil Mapping spring school","text":"<p>R is used to generate SoilGrids from the global Soil Profile database WoSIS.</p> <p>At intervals ISRIC organises a spring school on soil mapping using R in case you're interested in this topic.</p>"},{"location":"utils/r/#accessing-sql-data-from-r","title":"Accessing SQL data from R.","text":"<p>Since many soil observation data is stored in relational databases, but processing in R requires a flat data view, it is relevant to have reproducable conventions to access sql data in R. David Rositter described an approach to load soil profile data into R from sqlite in 2017. For PostGreSQL a similar approach should be possible using RPostres.  </p>"},{"location":"utils/r/#read-more","title":"Read more","text":"<ul> <li>Website</li> <li>Docker</li> </ul>"},{"location":"utils/visualstudiocode/","title":"Overview Text Editors","text":"<p>When working with a variety of text files (HTML, CSV, YAML, XML, GeoJSON, JS, Python, Markdown), you need a replacement for the basic Notepad. A magnitude of options exists, such as Vim, Notepad++, PyCharm, Eclipse, Sublime. The last years my personal favourite has become Visual Studio Code. But let's try to stay neutral and list what features we expect from a daily used text editor:</p> <ul> <li><code>Find in files</code> (and replace) is an important feature when looking for a certain pattern in a folder of files. </li> <li>Syntax highlighting for xml, json, python and yml facilitate readability of the file</li> <li>Code formatting/validation. In Python and YAML indenting is essential, that's when code formatting is extra important.</li> <li>A tree view of the project structure, so you can easily open files from the project </li> <li>Code completion/suggestions when you start typing</li> </ul> <p>Optional features:</p> <ul> <li>Preview HTML &amp; Markdown</li> <li>XML schema validation</li> <li>Git operations</li> <li>Content comparison, compare 2 (or more) files.</li> </ul> <p>Many of the text editor communities have a range of plugins available to extend the functionality of the editor. Various text editors have for example a MapServer Mapfile or SLD plugin.</p>"}]}